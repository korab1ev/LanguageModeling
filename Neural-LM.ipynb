{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHIDS-pkIoUT"
      },
      "source": [
        "### Homework: going neural (6 pts)\n",
        "\n",
        "We've checked out statistical approaches to language models in the last notebook. Now let's go find out what deep learning has to offer.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/expanding_mind_lm_kn_3.png' width=300px>\n",
        "\n",
        "We're gonna use the same dataset as before, except this time we build a language model that's character-level, not word level. Before you go:\n",
        "* If you haven't done seminar already, use `seminar.ipynb` to download the data.\n",
        "* This homework uses Pytorch v1.x: this is [how you install it](https://pytorch.org/get-started/locally/); and that's [how you use it](https://github.com/yandexdataschool/Practical_RL/tree/9f89e98d7df7ad47f5d6c85a70a38283e06be16a/week04_%5Brecap%5D_deep_learning)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EM_DT0x0IoUf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative manual download link: https://yadi.sk/d/_nGyU2IajjR9-w\n",
        "!wget \"https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\" -O arxivData.json.tar.gz\n",
        "!tar -xvzf arxivData.json.tar.gz\n",
        "data = pd.read_json(\"./arxivData.json\")\n",
        "data.sample(n=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "id": "N2AUQGW1JDB3",
        "outputId": "7d43d843-6e14-4ed5-e9b9-1f23cb968c18"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-20 09:42:23--  https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.6.18, 2620:100:601c:18::a27d:612\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.6.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/dl/99az9n1b57qkd9j/arxivData.json.tar.gz [following]\n",
            "--2022-09-20 09:42:25--  https://www.dropbox.com/s/dl/99az9n1b57qkd9j/arxivData.json.tar.gz\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc339e7031ddafac044cbfabf154.dl.dropboxusercontent.com/cd/0/get/BtRGGjBonwLMQBYe9aDQ7PgSvgfnJdh-6j3NZwrFPf8byYGAwLRfX4ilwye53dMhhOSYmgIinGWnhz739E19kb_XPBH7Nu5FdkLzZkrZ1XKQ-DHGmWkLu85WjdKkNAPh5O8RXc4vje8CtT2sD5G6c-Iap1JIhYsEjphXx1NVwhhU4Q/file?dl=1# [following]\n",
            "--2022-09-20 09:42:26--  https://uc339e7031ddafac044cbfabf154.dl.dropboxusercontent.com/cd/0/get/BtRGGjBonwLMQBYe9aDQ7PgSvgfnJdh-6j3NZwrFPf8byYGAwLRfX4ilwye53dMhhOSYmgIinGWnhz739E19kb_XPBH7Nu5FdkLzZkrZ1XKQ-DHGmWkLu85WjdKkNAPh5O8RXc4vje8CtT2sD5G6c-Iap1JIhYsEjphXx1NVwhhU4Q/file?dl=1\n",
            "Resolving uc339e7031ddafac044cbfabf154.dl.dropboxusercontent.com (uc339e7031ddafac044cbfabf154.dl.dropboxusercontent.com)... 162.125.6.15, 2620:100:6019:15::a27d:40f\n",
            "Connecting to uc339e7031ddafac044cbfabf154.dl.dropboxusercontent.com (uc339e7031ddafac044cbfabf154.dl.dropboxusercontent.com)|162.125.6.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18933283 (18M) [application/binary]\n",
            "Saving to: ‘arxivData.json.tar.gz’\n",
            "\n",
            "arxivData.json.tar. 100%[===================>]  18.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2022-09-20 09:42:26 (279 MB/s) - ‘arxivData.json.tar.gz’ saved [18933283/18933283]\n",
            "\n",
            "arxivData.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  author  day            id  \\\n",
              "31859  [{'name': 'Rémi Flamary'}, {'name': 'Benjamin ...   17   1106.3396v1   \n",
              "1029   [{'name': 'Hanie Sedghi'}, {'name': 'Anima Ana...    3  1603.00954v5   \n",
              "40868                       [{'name': 'Matthias Boehm'}]   22  1503.06384v1   \n",
              "6173   [{'name': 'Rémi Flamary'}, {'name': 'Marco Cut...   29  1608.08063v1   \n",
              "22418  [{'name': 'Gaël Varoquaux'}, {'name': 'Pradeep...   16  1606.05201v2   \n",
              "\n",
              "                                                    link  month  \\\n",
              "31859  [{'rel': 'related', 'href': 'http://dx.doi.org...      6   \n",
              "1029   [{'rel': 'alternate', 'href': 'http://arxiv.or...      3   \n",
              "40868  [{'rel': 'alternate', 'href': 'http://arxiv.or...      3   \n",
              "6173   [{'rel': 'alternate', 'href': 'http://arxiv.or...      8   \n",
              "22418  [{'rel': 'related', 'href': 'http://dx.doi.org...      6   \n",
              "\n",
              "                                                 summary  \\\n",
              "31859  Signal Sequence Labeling consists in predictin...   \n",
              "1029   We consider the problem of training input-outp...   \n",
              "40868  Declarative large-scale machine learning (ML) ...   \n",
              "6173   Wasserstein Discriminant Analysis (WDA) is a n...   \n",
              "22418  Decoding, ie prediction from brain images or s...   \n",
              "\n",
              "                                                     tag  \\\n",
              "31859  [{'term': 'cs.LG', 'scheme': 'http://arxiv.org...   \n",
              "1029   [{'term': 'cs.LG', 'scheme': 'http://arxiv.org...   \n",
              "40868  [{'term': 'cs.DC', 'scheme': 'http://arxiv.org...   \n",
              "6173   [{'term': 'stat.ML', 'scheme': 'http://arxiv.o...   \n",
              "22418  [{'term': 'stat.ML', 'scheme': 'http://arxiv.o...   \n",
              "\n",
              "                                                   title  year  \n",
              "31859  Large margin filtering for signal sequence lab...  2011  \n",
              "1029   Training Input-Output Recurrent Neural Network...  2016  \n",
              "40868  Costing Generated Runtime Execution Plans for ...  2015  \n",
              "6173                   Wasserstein Discriminant Analysis  2016  \n",
              "22418  Assessing and tuning brain decoders: cross-val...  2016  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-830d49a0-d664-4d6b-bfe8-a8e0aeb37e53\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>day</th>\n",
              "      <th>id</th>\n",
              "      <th>link</th>\n",
              "      <th>month</th>\n",
              "      <th>summary</th>\n",
              "      <th>tag</th>\n",
              "      <th>title</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31859</th>\n",
              "      <td>[{'name': 'Rémi Flamary'}, {'name': 'Benjamin ...</td>\n",
              "      <td>17</td>\n",
              "      <td>1106.3396v1</td>\n",
              "      <td>[{'rel': 'related', 'href': 'http://dx.doi.org...</td>\n",
              "      <td>6</td>\n",
              "      <td>Signal Sequence Labeling consists in predictin...</td>\n",
              "      <td>[{'term': 'cs.LG', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Large margin filtering for signal sequence lab...</td>\n",
              "      <td>2011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1029</th>\n",
              "      <td>[{'name': 'Hanie Sedghi'}, {'name': 'Anima Ana...</td>\n",
              "      <td>3</td>\n",
              "      <td>1603.00954v5</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>3</td>\n",
              "      <td>We consider the problem of training input-outp...</td>\n",
              "      <td>[{'term': 'cs.LG', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Training Input-Output Recurrent Neural Network...</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40868</th>\n",
              "      <td>[{'name': 'Matthias Boehm'}]</td>\n",
              "      <td>22</td>\n",
              "      <td>1503.06384v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>3</td>\n",
              "      <td>Declarative large-scale machine learning (ML) ...</td>\n",
              "      <td>[{'term': 'cs.DC', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Costing Generated Runtime Execution Plans for ...</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6173</th>\n",
              "      <td>[{'name': 'Rémi Flamary'}, {'name': 'Marco Cut...</td>\n",
              "      <td>29</td>\n",
              "      <td>1608.08063v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>8</td>\n",
              "      <td>Wasserstein Discriminant Analysis (WDA) is a n...</td>\n",
              "      <td>[{'term': 'stat.ML', 'scheme': 'http://arxiv.o...</td>\n",
              "      <td>Wasserstein Discriminant Analysis</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22418</th>\n",
              "      <td>[{'name': 'Gaël Varoquaux'}, {'name': 'Pradeep...</td>\n",
              "      <td>16</td>\n",
              "      <td>1606.05201v2</td>\n",
              "      <td>[{'rel': 'related', 'href': 'http://dx.doi.org...</td>\n",
              "      <td>6</td>\n",
              "      <td>Decoding, ie prediction from brain images or s...</td>\n",
              "      <td>[{'term': 'stat.ML', 'scheme': 'http://arxiv.o...</td>\n",
              "      <td>Assessing and tuning brain decoders: cross-val...</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-830d49a0-d664-4d6b-bfe8-a8e0aeb37e53')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-830d49a0-d664-4d6b-bfe8-a8e0aeb37e53 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-830d49a0-d664-4d6b-bfe8-a8e0aeb37e53');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1j3_BBoFIoUi"
      },
      "source": [
        "Working on character level means that we don't need to deal with large vocabulary or missing words. Heck, we can even keep uppercase words in text! The downside, however, is that all our sequences just got a lot longer.\n",
        "\n",
        "However, we still need special tokens:\n",
        "* Begin Of Sequence  (__BOS__) - this token is at the start of each sequence. We use it so that we always have non-empty input to our neural network. $P(x_t) = P(x_1 | BOS)$\n",
        "* End Of Sequence (__EOS__) - you guess it... this token is at the end of each sequence. The catch is that it should __not__ occur anywhere else except at the very end. If our model produces this token, the sequence is over.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "F_WXfz1fIoUj"
      },
      "outputs": [],
      "source": [
        "BOS, EOS = ' ', '\\n'\n",
        "\n",
        "lines = data.apply(lambda row: (row['title'] + ' ; ' + row['summary'])[:512], axis=1) \\\n",
        "            .apply(lambda line: BOS + line.replace(EOS, ' ') + EOS) \\\n",
        "            .tolist()\n",
        "\n",
        "# if you missed the seminar, download data here - https://yadi.sk/d/_nGyU2IajjR9-w"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "lsprIqJsJp7S",
        "outputId": "cd1e5312-d293-49f8-cc8d-0c953cfc66eb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Dual Recurrent Attention Units for Visual Question Answering ; We propose an architecture for VQA which utilizes recurrent layers to generate visual and textual attention. The memory characteristic of the proposed recurrent attention units offers a rich joint embedding of visual and textual features and enables the model to reason relations between several parts of the image and question. Our single model outperforms the first place winner on the VQA 1.0 dataset, performs within margin to the current state-\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOYsLiXMIoUk"
      },
      "source": [
        "Our next step is __building char-level vocabulary__. Put simply, you need to assemble a list of all unique tokens in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVqTxTPQIoUl",
        "outputId": "f29f28f0-3195-48ef-da90-a11401dba941"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_tokens =  136\n"
          ]
        }
      ],
      "source": [
        "# get all unique characters from lines (including capital letters and symbols)\n",
        "\n",
        "# tokens = set(''.join(line for line in lines))\n",
        "\n",
        "tokens = set()\n",
        "for line in lines:\n",
        "    tokens.update(set(line))\n",
        "\n",
        "tokens = sorted(tokens)\n",
        "n_tokens = len(tokens)\n",
        "print ('n_tokens = ',n_tokens)\n",
        "assert 100 < n_tokens < 150\n",
        "assert BOS in tokens, EOS in tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJWacuxZIoUl"
      },
      "source": [
        "We can now assign each character with it's index in tokens list. This way we can encode a string into a torch-friendly integer vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QpvVX_svIoUm"
      },
      "outputs": [],
      "source": [
        "# dictionary of character -> its identifier (index in tokens list)\n",
        "token_to_id = {char : id for (char,id)  in zip(tokens, range(n_tokens)) }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mquHo2wIoUn",
        "outputId": "1693130a-9a83-4d4a-f00c-c9a70ee4ff15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seems alright!\n"
          ]
        }
      ],
      "source": [
        "assert len(tokens) == len(token_to_id), \"dictionaries must have same size\"\n",
        "for i in range(n_tokens):\n",
        "    assert token_to_id[tokens[i]] == i, \"token identifier must be it's position in tokens list\"\n",
        "\n",
        "print(\"Seems alright!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fALM9NgaIoUn"
      },
      "source": [
        "Our final step is to assemble several strings in a integet matrix `[batch_size, text_length]`. \n",
        "\n",
        "The only problem is that each sequence has a different length. We can work around that by padding short sequences with extra _EOS_ or cropping long sequences. Here's how it works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Vyx6YB05IoUo"
      },
      "outputs": [],
      "source": [
        "def to_matrix(lines, max_len=None, pad=token_to_id[EOS], dtype=np.int64):\n",
        "    \"\"\"Casts a list of lines into torch-digestable matrix\"\"\"\n",
        "    max_len = max_len or max(map(len, lines))\n",
        "    lines_ix = np.full([len(lines), max_len], pad, dtype=dtype)\n",
        "    for i in range(len(lines)):\n",
        "        line_ix = list(map(token_to_id.get, lines[i][:max_len]))\n",
        "        lines_ix[i, :len(line_ix)] = line_ix\n",
        "    return lines_ix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiFn3iEfIoUp",
        "outputId": "ca2c4ea0-b63e-4a5b-ca5f-4f4ddb95939f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1 66 67 68  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 1 66 67 66 68 66 67 66  0  0  0  0  0  0  0]\n",
            " [ 1 66 67 68 18 19 20 21 22 23 24 25 26 17  0]]\n"
          ]
        }
      ],
      "source": [
        "#Example: cast 3 random lines to matrices, pad with zeros\n",
        "dummy_lines = [\n",
        "    ' abc\\n',\n",
        "    ' abacaba\\n',\n",
        "    ' abc1234567890\\n',\n",
        "]\n",
        "print(to_matrix(dummy_lines))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWzKDi2dIoUr"
      },
      "source": [
        "### Neural Language Model (2 points including training)\n",
        "\n",
        "Just like for N-gram LMs, we want to estimate probability of text as a joint probability of tokens (symbols this time).\n",
        "\n",
        "$$P(X) = \\prod_t P(x_t \\mid x_0, \\dots, x_{t-1}).$$ \n",
        "\n",
        "Instead of counting all possible statistics, we want to train a neural network with parameters $\\theta$ that estimates the conditional probabilities:\n",
        "\n",
        "$$ P(x_t \\mid x_0, \\dots, x_{t-1}) \\approx p(x_t \\mid x_0, \\dots, x_{t-1}, \\theta) $$\n",
        "\n",
        "\n",
        "But before we optimize, we need to define our neural network. Let's start with a fixed-window (aka convolutional) architecture:\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/fixed_window_lm.jpg' width=400px>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2lf3uSBQIoUs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7SMGTcCpKlm",
        "outputId": "805f9c58-aebb-45f1-8bec-8c80e666c5b9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "02hVEWHLIoUt"
      },
      "outputs": [],
      "source": [
        "class FixedWindowLanguageModel(nn.Module):\n",
        "    def __init__(self, device, n_tokens=n_tokens, emb_size=16, hid_size=64, kernel_size=5):\n",
        "        \"\"\" \n",
        "        A fixed window model that looks on at least 5 previous symbols (kernel_size).\n",
        "        \n",
        "        Note: fixed window LM is effectively performing a convolution over a sequence of words.\n",
        "        This convolution only looks on current and previous words.\n",
        "        Such convolution can be represented as a sequence of 2 operations:\n",
        "        - pad input vectors by {strides * (filter_size - 1)} zero vectors on the \"left\", do not pad right\n",
        "        - perform regular convolution with {filter_size} and {strides}\n",
        "        \n",
        "        - If you're absolutely lost, here's a hint: use nn.ZeroPad2d((NUM_LEADING_ZEROS, 0, 0, 0))\n",
        "          followed by a nn.Conv1d(..., padding=0). And yes, its okay that padding is technically \"2d\".\n",
        "        \"\"\"\n",
        "        super().__init__() # initialize base class to track sub-layers, trainable variables, etc.\n",
        "        \n",
        "        self.device = device\n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        self.emb = nn.Embedding(n_tokens, emb_size)\n",
        "        self.conv1d = nn.Conv1d(in_channels=emb_size,\n",
        "                                out_channels=hid_size,\n",
        "                                kernel_size=kernel_size)\n",
        "        self.fc1 = nn.Linear(hid_size, hid_size)\n",
        "        self.fc2 = nn.Linear(hid_size, n_tokens)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def __call__(self, input_ix):\n",
        "        \"\"\"\n",
        "        compute language model logits given input tokens\n",
        "        :param input_ix: batch of sequences with token indices, tensor: int32[batch_size, sequence_length]\n",
        "        :returns: pre-softmax linear outputs of language model [batch_size, sequence_length, n_tokens]\n",
        "            these outputs will be used as logits to compute P(x_t | x_0, ..., x_{t - 1})\n",
        "            \n",
        "        :note: that convolutions operate with tensors of shape [batch, channels, length], while linear layers\n",
        "         and *embeddings* use [batch, length, channels] tensors. Use tensor.permute(...) to adjust shapes.\n",
        "\n",
        "        \"\"\"\n",
        "        inp_emb = self.emb(input_ix) # [batch_size, sequence_length, emb_dim]\n",
        "        inp_emb = inp_emb.permute((0, 2, 1)) # [batch_size, emb_dim, sequence_length]        \n",
        "        \n",
        "        # apply padding to keep tensor size\n",
        "        # pad (with zeros) last dim by kernel_size - 1 on the left and 0 on the right\n",
        "        inp_emb = F.pad(inp_emb, pad=(self.kernel_size - 1, 0)) \n",
        "        #print(inp_emb[0][0])\n",
        "        '''\n",
        "        tensor([ 0.0000,  0.0000,  0.0000,  0.0000, -0.0653, -0.4699,  1.1353, -0.8066,\n",
        "         0.4302,  0.4302,  0.4302,  0.4302,  0.4302,  0.4302,  0.4302,  0.4302,\n",
        "         0.4302,  0.4302,  0.4302], grad_fn=<SelectBackward0>)\n",
        "        '''\n",
        "        inp_emb = self.conv1d(inp_emb) # [batch_size, hid_size, sequence_length]\n",
        "        #print(inp_emb.size())\n",
        "        # if we haven't used the padding we would have gotten out_shape = [batch_size, hid_size, sequence_length - 4]\n",
        "        \n",
        "        inp_emb = inp_emb.permute((0, 2, 1)) # [batch_size, sequence_length, hid_size]       \n",
        "\n",
        "        inp_emb = self.fc1(inp_emb) # [batch_size, sequence_length, hid_size]  \n",
        "        inp_emb = self.relu(inp_emb)  \n",
        "        inp_emb = self.fc2(inp_emb) # [batch_size, sequence_length, n_tokens]  \n",
        "        \n",
        "        return inp_emb # [batch_size, sequence_length, n_tokens]\n",
        "    \n",
        "    def get_possible_next_tokens(self, prefix=BOS, temperature=1.0, max_len=100):\n",
        "        \"\"\" :returns: probabilities of next token, dict {token : prob} for all tokens \"\"\"\n",
        "        prefix_ix = torch.as_tensor(to_matrix([prefix]), dtype=torch.int64).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            probs = torch.softmax(self(prefix_ix)[0, -1], dim=-1).cpu().numpy()  # shape: [n_tokens]\n",
        "        return dict(zip(tokens, probs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2aR5C9cIoUu",
        "outputId": "1fd05d13-62c3-41c4-d50b-d800ee46bca9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: ('emb.weight', 'conv1d.weight', 'conv1d.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias')\n"
          ]
        }
      ],
      "source": [
        "dummy_model = FixedWindowLanguageModel(device=device)\n",
        "\n",
        "dummy_input_ix = torch.as_tensor(to_matrix(dummy_lines))\n",
        "dummy_logits = dummy_model(dummy_input_ix)\n",
        "\n",
        "print('Weights:', tuple(name for name, w in dummy_model.named_parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8oeS6DYFIoUv"
      },
      "outputs": [],
      "source": [
        "assert isinstance(dummy_logits, torch.Tensor)\n",
        "assert dummy_logits.shape == (len(dummy_lines), max(map(len, dummy_lines)), n_tokens), \"please check output shape\"\n",
        "assert np.all(np.isfinite(dummy_logits.data.cpu().numpy())), \"inf/nan encountered\"\n",
        "assert not np.allclose(dummy_logits.data.cpu().numpy().sum(-1), 1), \"please predict linear outputs, don't use softmax (maybe you've just got unlucky)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "avB9WHl1IoUw"
      },
      "outputs": [],
      "source": [
        "# test for lookahead\n",
        "dummy_input_ix_2 = torch.as_tensor(to_matrix([line[:3] + 'e' * (len(line) - 3) for line in dummy_lines]))\n",
        "dummy_logits_2 = dummy_model(dummy_input_ix_2)\n",
        "\n",
        "assert torch.allclose(dummy_logits[:, :3], dummy_logits_2[:, :3]), \"your model's predictions depend on FUTURE tokens. \" \\\n",
        "    \" Make sure you don't allow any layers to look ahead of current token.\" \\\n",
        "    \" You can also get this error if your model is not deterministic (e.g. dropout). Disable it for this test.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V51YcEXIoUw"
      },
      "source": [
        "We can now tune our network's parameters to minimize categorical crossentropy over training dataset $D$:\n",
        "\n",
        "$$ L = {\\frac1{|D|}} \\sum_{X \\in D} \\sum_{x_i \\in X} - \\log p(x_t \\mid x_1, \\dots, x_{t-1}, \\theta) $$\n",
        "\n",
        "As usual with with neural nets, this optimization is performed via stochastic gradient descent with backprop.  One can also note that minimizing crossentropy is equivalent to minimizing model __perplexity__, KL-divergence or maximizng log-likelihood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5vlP6_SIoUx",
        "outputId": "329e3310-9769-4f63-bc2d-4e1c628a08b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "matrix:\n",
            " [[ 1 66 67 68  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 1 66 67 66 68 66 67 66  0  0  0  0  0  0  0]\n",
            " [ 1 66 67 68 18 19 20 21 22 23 24 25 26 17  0]]\n",
            "mask: [[1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 1 1 1 1 1 1 1 1 0 0 0 0 0 0]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
            "lengths: [ 5  9 15]\n"
          ]
        }
      ],
      "source": [
        "def compute_mask(input_ix, eos_ix=token_to_id[EOS]):\n",
        "    \"\"\" compute a boolean mask that equals \"1\" until first EOS (including that EOS) \"\"\"\n",
        "    return F.pad(torch.cumsum(input_ix == eos_ix, dim=-1)[..., :-1] < 1, pad=(1, 0, 0, 0), value=True)\n",
        "\n",
        "print('matrix:\\n', dummy_input_ix.numpy())\n",
        "print('mask:', compute_mask(dummy_input_ix).to(torch.int32).cpu().numpy())\n",
        "print('lengths:', compute_mask(dummy_input_ix).sum(-1).cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "30SksDA1IoUx"
      },
      "outputs": [],
      "source": [
        "def compute_loss(model, input_ix):\n",
        "    \"\"\"\n",
        "    :param model: language model that can compute next token logits given token indices\n",
        "    :param input ix: int32 matrix of tokens, shape: [batch_size, length]; padded with eos_ix\n",
        "    :returns: scalar loss function, mean crossentropy over non-eos tokens\n",
        "    \"\"\"\n",
        "    input_ix = torch.as_tensor(input_ix, dtype=torch.int64)\n",
        "    '''\n",
        "    tensor([[ 1, 66, 67, 68,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "        [ 1, 66, 67, 66, 68, 66, 67, 66,  0,  0,  0,  0,  0,  0,  0],\n",
        "        [ 1, 66, 67, 68, 18, 19, 20, 21, 22, 23, 24, 25, 26, 17,  0]])\n",
        "    '''\n",
        "    targets = input_ix[:, 1:]\n",
        "    '''\n",
        "    tensor([[66, 67, 68,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "        [66, 67, 66, 68, 66, 67, 66,  0,  0,  0,  0,  0,  0,  0],\n",
        "        [66, 67, 68, 18, 19, 20, 21, 22, 23, 24, 25, 26, 17,  0]])\n",
        "    '''\n",
        "    mask = compute_mask(targets)\n",
        "    targets_1hot = F.one_hot(targets, n_tokens).to(torch.float32)\n",
        "\n",
        "    logits = model(input_ix[:, :-1])\n",
        "    logits = torch.log_softmax(logits, dim=-1)\n",
        "\n",
        "    # log-probabilities of correct outputs, [batch_size, n_tokens]\n",
        "    logp_out = (logits * targets_1hot).sum(dim=-1)  \n",
        "\n",
        "    return -logp_out[mask].mean() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "IrmuyUdnIoUy"
      },
      "outputs": [],
      "source": [
        "loss_1 = compute_loss(dummy_model, to_matrix(dummy_lines, max_len=15))\n",
        "loss_2 = compute_loss(dummy_model, to_matrix(dummy_lines, max_len=16))\n",
        "assert (np.ndim(loss_1) == 0) and (0 < loss_1 < 100), \"loss must be a positive scalar\"\n",
        "assert torch.allclose(loss_1, loss_2), 'do not include  AFTER first EOS into loss. '\\\n",
        "    'Hint: use compute_mask. Beware +/-1 errors. And be careful when averaging!' "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLTKBrJsIoUz"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "You will need two functions: one to compute test loss and another to generate samples. For your convenience, we implemented them both in your stead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "wzqN6my8IoUz"
      },
      "outputs": [],
      "source": [
        "def score_lines(model, dev_lines, batch_size):\n",
        "    \"\"\" computes average loss over the entire dataset \"\"\"\n",
        "    dev_loss_num, dev_loss_len = 0., 0.\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(dev_lines), batch_size):\n",
        "            batch_ix = torch.as_tensor(to_matrix(dev_lines[i: i + batch_size])).to(device)\n",
        "            dev_loss_num += compute_loss(model, batch_ix).item() * len(batch_ix)\n",
        "            dev_loss_len += len(batch_ix)\n",
        "    return dev_loss_num / dev_loss_len\n",
        "\n",
        "def generate(model, prefix=BOS, temperature=1.0, max_len=100):\n",
        "    \"\"\"\n",
        "    Samples output sequence from probability distribution obtained by model\n",
        "    :param temperature: samples proportionally to model probabilities ^ temperature\n",
        "        if temperature == 0, always takes most likely token. Break ties arbitrarily.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        while True:\n",
        "            token_probs = model.get_possible_next_tokens(prefix)\n",
        "            tokens, probs = zip(*token_probs.items())\n",
        "            if temperature == 0:\n",
        "                next_token = tokens[np.argmax(probs)]\n",
        "            else:\n",
        "                probs = np.array([p ** (1. / temperature) for p in probs])\n",
        "                probs /= sum(probs)\n",
        "                next_token = np.random.choice(tokens, p=probs)\n",
        "\n",
        "            prefix += next_token\n",
        "            if next_token == EOS or len(prefix) > max_len: break\n",
        "    return prefix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vI9QmixFIoU0"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "Finally, let's train our model on minibatches of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VKvkF_mIoU0",
        "outputId": "1f23ef0a-d0d4-4a3c-dacf-5d6a57d23312"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev history =  [(0, 4.9113205302168685)]\n",
            "Sample before training: Bridgingôy4`cX17öeb`=ô%õ@λX1ç \n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_lines, dev_lines = train_test_split(lines, test_size=0.25, random_state=42)\n",
        "\n",
        "batch_size = 256\n",
        "score_dev_every = 250\n",
        "train_history, dev_history = [], []\n",
        "model = FixedWindowLanguageModel(device=device)\n",
        "opt = torch.optim.Adam(model.parameters())\n",
        "model.to(device)\n",
        "# hint: if you ever wanted to switch to cuda, do it now.\n",
        "\n",
        "# score untrained model\n",
        "dev_history.append((0, score_lines(model, dev_lines, batch_size)))\n",
        "print(\"Dev history = \", dev_history)\n",
        "print(\"Sample before training:\", generate(model, 'Bridging'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "pSyG4GuxIoU1",
        "outputId": "b3b9bf4b-e307-4b99-ba64-1f0f647c70dc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated examples (tau=0.5):\n",
            " A difference of the fired based on the design ; Words image and approachere a complex a methodel com\n",
            " 1 Refine the network and the part a regrodal enework in the propose a neural networks ; On a semanti\n",
            " date deters fir datch of the propose as a sens of the to as extrictures for and Genever, and computa\n",
            "Scoring dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [03:14<00:00, 25.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#4999 Dev loss: 1.649\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "from random import sample\n",
        "from tqdm import trange\n",
        "\n",
        "for i in trange(5000):\n",
        "    batch = torch.as_tensor(to_matrix(sample(train_lines, batch_size))).to(device)\n",
        "    \n",
        "    loss_i = compute_loss(model, batch)\n",
        "    \n",
        "    opt.zero_grad()\n",
        "    loss_i.backward()\n",
        "    opt.step()\n",
        "        \n",
        "    train_history.append((i, loss_i.item()))\n",
        "    \n",
        "    if (i + 1) % 50 == 0:\n",
        "        clear_output(True)\n",
        "        plt.scatter(*zip(*train_history), alpha=0.1, label='train_loss')\n",
        "        if len(dev_history):\n",
        "            plt.plot(*zip(*dev_history), color='red', label='dev_loss')\n",
        "        plt.legend(); plt.grid(); plt.show()\n",
        "        print(\"Generated examples (tau=0.5):\")\n",
        "        for _ in range(3):\n",
        "            print(generate(model, temperature=0.5, max_len=100))\n",
        "    \n",
        "    if (i + 1) % score_dev_every == 0:\n",
        "        print(\"Scoring dev...\")\n",
        "        dev_history.append((i, score_lines(model, dev_lines, batch_size)))\n",
        "        print('#%i Dev loss: %.3f' % dev_history[-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "uYe7gJc4IoU2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a406d92-3962-465f-e20a-9a8e325ba9a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final dev loss: 1.6488661629281394\n",
            " Distribution of the networks has are conce it the problem for learning the applicition of and the and such a struction and learning to problem is in a model comporess of the frametric many a complexity algorithms propose a no in a recognitional are Recogration the sim canows and facts of contexting \n",
            " Model strases to processent the extrate the and canducies with of exprical the comples are use model to the system, which learning and the propossems of the and to end networks from and as a sently complaned are on problem in the fully the givery comporess of network and on suple of the the stent me\n",
            " Graining a propose and recomerems answing to a desens of experst comportain the propose in the to semplexister relent constral network an in the prove is a propose part is an in a decomplexity and model the aise mation of the partame of the optimization of the for Nolutional to multi-based the compu\n",
            " Seliment of the ele starch to a number eveowst the use of the pardent Spariation of a generalization, that in proposed to such as a function ; Segning the requires a sented and and as in the been a selsentiming proposed on shalle tech as the may comple conside the propose sence camicular of a model \n",
            " Propose rearning to sermal argorieval-serainal correction and and Visional to and evailing correction ; We and the model comparistically proposes all been the proposed to the extroded to in the propose convolution of a recognition of the and extrol solution invery and the segment present of the cons\n",
            " Prove we problem a been from the such as in a selling and are sening the trainal methion models have the deep arrowst of the the deperts is dopans to can be conducient is a similar   Thetic many deders and the propose acquences in the application of sigies are stata is resents of the for interate be\n",
            " And that in grading a provide this that detection propose a partexy and recogned and conversal compresent a seriction for informance for the proposy and provide and and the solution of station of the and classifications ; We presentations is a generation for the waye a now the exproarating the sumpl\n",
            " Sorest an and tenformation and between of and in a nover subsed. In this in distriction the ansition the presented to rearning ophine to propose meary conducing of model of the has are from a simular interation of the station ; In task of relation of the the algorithms of the porters effication of w\n",
            " Contrical best of the contration of the are the transification ; We process ; The art relle station in the inferent ; We provised in a numption of the propose a nove been of the detection of the that a new bank semantic inter of computer treas in a set of that is resent a not the proposed arto of th\n",
            " Fase and the computation of a complexity is the deneral networks and of Partical and has that controduce of the domputer of convergets of the propose the with recogenerated on the sentent and seders the and and a specification of such as a recoment in a sumpore a generated model of and sumational in\n"
          ]
        }
      ],
      "source": [
        "assert np.mean(train_history[:10], axis=0)[1] > np.mean(train_history[-10:], axis=0)[1], \"The model didn't converge.\"\n",
        "print(\"Final dev loss:\", dev_history[-1][-1])\n",
        "\n",
        "for i in range(10):\n",
        "    print(generate(model, temperature=0.5, max_len=300))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    print(generate(model, temperature=.4, max_len=300))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NgDINYFSu8J",
        "outputId": "45c5ce57-bf24-4912-cefe-d4507bb329b4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A distribution and the sence of the proposed on the constributed on the and and as a crases of the and defers the propose a novel of a noverading the are matral and evers is a model computates are contral network for the learning a semant a new a problem a non-based results and and and the parametri\n",
            " A logical be the as a complex the high sample and nonding are can be agent and sonvideation of the learning algorithm we of the sech as a marial problem a selity of the learning the despers in the problem a nonsing and a novel and sumples that the explication for loistriction and to expore siment so\n",
            " A serned to sequention of the extribution of the propers in the algorithms to in the model controdur visual information of the propose to constraction and and of the partical betwer of the to explication of the propose a constral and method to consignification of the are semant a model problem in a \n",
            " To in the approximate and and to semples: the problem in a novel variable requires and and the the numptimation of the propose a new end problem in proce model to recogence station and problem the problem a sersing and structure senformation and the partic problem a not on the parametric proposed an\n",
            " A sumportant as problem in a describlem in a for this paper the we the propose image struction and to explication ; We problem a servolutions are converal networks for consigned to a seriation of the converge and a novel problem and in the problems ; We problem a servation the application of a sumpl\n",
            " Proced by a model classic setection of the propose a novel and seterning a sime the decoment a seural and to propose a neural network scalized convexial of the learning a novel and model consing in a sermal wind to the prosed to the starch as and and to convolution of the maching the partical method\n",
            " Some the learning to the from the problem of the propose a not of the the such a strection of the represent algorithm the algorithm the presentation of ent sime the complexity in a propose a novel computation of the in the proposed unsoress of the station and in the learning of imples afsimation of \n",
            " A computer system is objective and a neural networks the propose convolution of probability and the for the proposed and for approach to the propose and in the stempersing a similation and generaction of the station and resent the can algorithm and to relation of the are complex are conver   logic p\n",
            " In this paper a seural networks and as a model vision the station of the proposed to in a nomm in patter state of the the matration and relass and and converage to the learning application of the arroved can be image amage that in the problem sparsing the resultive and the mine a sermal sparname is \n",
            " Method provide considers the proposed of the computional computation and compution and models for deep decompare dependentify represent and a new effect to settensistems ; The and for the and as a comportex of the as the problem a special model computation of the the lase the the models proving of t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9NzDDoNIoU3"
      },
      "source": [
        "### RNN Language Models (3 points including training)\n",
        "\n",
        "Fixed-size architectures are reasonably good when capturing short-term dependencies, but their design prevents them from capturing any signal outside their window. We can mitigate this problem by using a __recurrent neural network__:\n",
        "\n",
        "$$ h_0 = \\vec 0 ; \\quad h_{t+1} = RNN(x_t, h_t) $$\n",
        "\n",
        "$$ p(x_t \\mid x_0, \\dots, x_{t-1}, \\theta) = dense_{softmax}(h_{t-1}) $$\n",
        "\n",
        "Such model processes one token at a time, left to right, and maintains a hidden state vector between them. Theoretically, it can learn arbitrarily long temporal dependencies given large enough hidden size.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/rnn_lm.jpg' width=480px>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHmhGo_lIoU4"
      },
      "outputs": [],
      "source": [
        "class RNNLanguageModel(nn.Module):\n",
        "    def __init__(self, device, n_tokens=n_tokens, emb_size=16, hid_size=256, num_layers=2, dropout=0.2, bid=False):\n",
        "        \"\"\" \n",
        "        Build a recurrent language model.\n",
        "        You are free to choose anything you want, but the recommended architecture is\n",
        "        - token embeddings\n",
        "        - one or more LSTM/GRU layers with hid size\n",
        "        - linear layer to predict logits\n",
        "        \n",
        "        :note: if you use nn.RNN/GRU/LSTM, make sure you specify batch_first=True\n",
        "         With batch_first, your model operates with tensors of shape [batch_size, sequence_length, num_units]\n",
        "         Also, please read the docs carefully: they don't just return what you want them to return :)\n",
        "        \"\"\"\n",
        "        super().__init__() # initialize base class to track sub-layers, trainable variables, etc.\n",
        "        self.device = device\n",
        "        \n",
        "        self.emb = nn.Embedding(n_tokens, emb_size) \n",
        "\n",
        "        self.lstm = nn.LSTM(emb_size, hid_size, \n",
        "                            num_layers, batch_first=True, \n",
        "                            bidirectional=bid) # bidirectional sucks for character-level approach\n",
        "                                               # so don't use it\n",
        "        self.fc1 = nn.Linear(hid_size + hid_size*bid, hid_size + hid_size*bid)\n",
        "        self.fc2 = nn.Linear(hid_size + hid_size*bid, n_tokens)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "    def __call__(self, input_ix):\n",
        "        \"\"\"\n",
        "        compute language model logits given input tokens\n",
        "        :param input_ix: batch of sequences with token indices, tensor: int32[batch_size, sequence_length]\n",
        "        :returns: pre-softmax linear outputs of language model [batch_size, sequence_length, n_tokens]\n",
        "            these outputs will be used as logits to compute P(x_t | x_0, ..., x_{t - 1})\n",
        "        \"\"\"\n",
        "        inp_emb = self.emb(input_ix) # [batch_size, sequence_length, emb_dim]\n",
        "        inp_emb = self.dropout(inp_emb)\n",
        "\n",
        "        enc_seq, last_state_but_not_really = self.lstm(inp_emb)\n",
        "        # enc_seq: [batch, time, hid_size], last_state: [batch, hid_size]\n",
        "        # enc_seq -> contains the output features (h_t) from the last layer of the LSTM, for each t\n",
        "        # last_state -> last state h_t of encoder (h_0 for decoder)\n",
        "        enc_seq = self.fc1(enc_seq)\n",
        "        enc_seq = self.relu(enc_seq)\n",
        "        \n",
        "        next_logits = self.fc2(enc_seq)\n",
        "        next_logp = F.log_softmax(next_logits, dim=-1)\n",
        "\n",
        "        return next_logp # [batch_size, sequence_length, n_tokens]\n",
        "\n",
        "\n",
        "    def get_possible_next_tokens(self, prefix=BOS, temperature=1.0, max_len=100):\n",
        "        \"\"\" :returns: probabilities of next token, dict {token : prob} for all tokens \"\"\"\n",
        "        prefix_ix = torch.as_tensor(to_matrix([prefix]), dtype=torch.int64).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            probs = torch.softmax(self(prefix_ix)[0, -1], dim=-1).cpu().numpy()  # shape: [n_tokens]\n",
        "        return dict(zip(tokens, probs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "554JsYrVIoU5",
        "outputId": "9e81b581-8854-44e2-b625-dbb697f6b24d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: ('emb.weight', 'lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'lstm.weight_ih_l1', 'lstm.weight_hh_l1', 'lstm.bias_ih_l1', 'lstm.bias_hh_l1', 'lstm.weight_ih_l2', 'lstm.weight_hh_l2', 'lstm.bias_ih_l2', 'lstm.bias_hh_l2', 'hid1.weight', 'hid1.bias', 'hid_to_logits.weight', 'hid_to_logits.bias')\n"
          ]
        }
      ],
      "source": [
        "rnn_model = RNNLanguageModel(device=device, dropout=0, num_layers=3, bid=False)\n",
        "\n",
        "dummy_input_ix = torch.as_tensor(to_matrix(dummy_lines))\n",
        "dummy_logits = rnn_model(dummy_input_ix)\n",
        "\n",
        "assert isinstance(dummy_logits, torch.Tensor)\n",
        "assert dummy_logits.shape == (len(dummy_lines), max(map(len, dummy_lines)), n_tokens), \"please check output shape\"\n",
        "assert not np.allclose(dummy_logits.cpu().data.numpy().sum(-1), 1), \"please predict linear outputs, don't use softmax (maybe you've just got unlucky)\"\n",
        "print('Weights:', tuple(name for name, w in rnn_model.named_parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "UfRsmkOQIoU5"
      },
      "outputs": [],
      "source": [
        "# test for lookahead\n",
        "dummy_input_ix_2 = torch.as_tensor(to_matrix([line[:3] + 'e' * (len(line) - 3) for line in dummy_lines]))\n",
        "dummy_logits_2 = rnn_model(dummy_input_ix_2)\n",
        "\n",
        "assert torch.allclose(dummy_logits[:, :3], dummy_logits_2[:, :3]), \"your model's predictions depend on FUTURE tokens. \" \\\n",
        "    \" Make sure you don't allow any layers to look ahead of current token.\" \\\n",
        "    \" You can also get this error if your model is not deterministic (e.g. dropout). Disable it for this test.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL7S0HfmIoU5"
      },
      "source": [
        "### RNN training\n",
        "\n",
        "Our RNN language model should optimize the same loss function as fixed-window model. But there's a catch. Since RNN recurrently multiplies gradients through many time-steps, gradient values may explode, [ruining](https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/nan.jpg) your model.\n",
        "The common solution to that problem is to clip gradients either [individually](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/clip_by_value) or [globally](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/clip_by_global_norm).\n",
        "\n",
        "Your task here is to implement the training code that minimizes the loss function. If you encounter large loss fluctuations during training, please add [gradient clipping](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html) using urls above. But its **not necessary** to use gradient clipping if you don't need it.\n",
        "\n",
        "_Note: gradient clipping is not exclusive to RNNs. Convolutional networks with enough depth often suffer from the same issue._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79ob0PtXIoU6",
        "outputId": "c1142277-5dba-4be8-d69d-60c29306efda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 4.904841408706293)]\n",
            "Sample before training: Bridgingk7ózS.D$v(Zx33Ωä$ŁŁγêbΠ%oó*õ%BDãyIz%2}fwet(I.tm0Gtaσ0mJxÜáô9αóKg;ΩRàKêä`ö+=+vByχ>í87phTAcqρ\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128 # <-- please tune batch size to fit your CPU/GPU configuration\n",
        "score_dev_every = 250\n",
        "train_history, dev_history = [], []\n",
        "\n",
        "# bidirectional sucks with character-level, so don't use it\n",
        "# higher num_layers - slower convergence\n",
        "rnn_model = RNNLanguageModel(device=device, num_layers=2, dropout=0.2, bid=False)\n",
        "opt = torch.optim.Adam(rnn_model.parameters())\n",
        "rnn_model.to(device)\n",
        "\n",
        "# score untrained model\n",
        "dev_history.append((0, score_lines(rnn_model, dev_lines, batch_size)))\n",
        "print(dev_history)\n",
        "print(\"Sample before training:\", generate(rnn_model, 'Bridging'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "ApE01DvlIoU7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "0a13242b-5ff3-48bb-e67e-7226768efcf8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated examples (tau=0.5):\n",
            " Unsupervised Learning and Deep Neural Networks ; We propose a new problem of distributed algorithms \n",
            " Deep Convolutional Neural Networks for Texture Learning ; Convolutional neural networks are set of t\n",
            " Stochastic Models for Multi-Sparse Real-Simulation of Adversarial Networks ; Fuzzy one of the proble\n",
            "Scoring dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [17:09<00:00,  4.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#4999 Dev loss: 1.098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "from random import sample\n",
        "from tqdm import trange\n",
        "\n",
        "for i in trange(5000):\n",
        "    batch = torch.as_tensor(to_matrix(sample(train_lines, batch_size))).to(device)\n",
        "\n",
        "    loss_i = compute_loss(rnn_model, batch)\n",
        "    \n",
        "    opt.zero_grad()\n",
        "    loss_i.backward()\n",
        "    opt.step()\n",
        "\n",
        "    train_history.append((i, float(loss_i)))\n",
        "    \n",
        "    if (i + 1) % 50 == 0:\n",
        "        clear_output(True)\n",
        "        plt.scatter(*zip(*train_history), alpha=0.1, label='train_loss')\n",
        "        if len(dev_history):\n",
        "            plt.plot(*zip(*dev_history), color='red', label='dev_loss')\n",
        "        plt.legend(); plt.grid(); plt.show()\n",
        "        print(\"Generated examples (tau=0.5):\")\n",
        "        for _ in range(3):\n",
        "            print(generate(rnn_model, temperature=0.5))\n",
        "    \n",
        "    if (i + 1) % score_dev_every == 0:\n",
        "        print(\"Scoring dev...\")\n",
        "        dev_history.append((i, score_lines(rnn_model, dev_lines, batch_size)))\n",
        "        print('#%i Dev loss: %.3f' % dev_history[-1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating samples with RNN model"
      ],
      "metadata": {
        "id": "0H5WSkAF06Yr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "aBg7r_2eIoU7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b68f8c17-2471-4007-fbe1-3cac8601207d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final dev loss: 1.0984088114761725\n",
            " A Bayesian Approach for Embedding Algorithms ; In this paper, we propose a set of the first of the statistical approach for exploration in the set of the set of the control control in convolutional neural networks (CNNs) is a structure of the problem of the parameters of the systematic problems. The\n",
            " Method for Semantic Infully Step to Similar Bourt Detection ; We propose a new model and the presence of simulations are inverse and accountered in statistical streaming, and the developing on the problem of all the field of the approach of the discriminative relative supervised methods that there a\n",
            " A new probabilistic topic for context of the device and   interval recognition ; We present a method for emotional contextual spectral constituent of the construction of complexity of search analysis of the articipant and algorithms that enh accurate in the problem of times and a set of the probabil\n",
            " A Robust Complexity of Deep Learning for Deep Learning ; The framework of the factorization of the statistical interaction programming, in particular, we propose to the structure of the sentence of a set of the problem. We consider the statistical veristic setting is a property of the to developing \n",
            " Analyzing a semantic and supervised learning of supervised learning ; We propose a proposed approach to many spans of the neural network in a prior of problems in constructing the comprehension of the sparse sufferences in the large orientation of the most object or the original distribution of the \n",
            " An Expirical Approach to Hule Complexity of Learning Sentences ; This issue is a complete dataset of the internal data setting and sensing the sequence of the computational field of the problem of the same scale in the structure. The popular techniques are becomed to the success of the motion to a s\n",
            " Towards Function Graph Detection for Action Networks ; We propose a new role of the fundamental scenario for the internal optimization algorithm for contextual research in the same convex optimizing the specific scene in the problem by a set of the computer vision computer vision activity to solve t\n",
            " Realing Synthesis of Deep Networks for Long   Data ; We present a traffic to the important of an approach to be proposed and adversarial ambiguous relations in a probability of probability researchers that be allows to the proposed mathematical components for the continuous approaches that allows th\n",
            " A Side Context-Based Deep Structure of Support Regression ; Recent years to support complexity can be containing the content of the computer vision of the new classifiers of the context of the number of distance and automatic object recognition. The world that structure is the definition of computer\n",
            " Deep Convorulal Networks with Word Contributions ; The problem of a probabilistic information that can gene two models in the control of the problem with the prediction of computer vision or applicability and multiple classification on the brain distribution of the problem of the problem. We propose\n"
          ]
        }
      ],
      "source": [
        "assert np.mean(train_history[:10], axis=0)[1] > np.mean(train_history[-10:], axis=0)[1], \"The model didn't converge.\"\n",
        "print(\"Final dev loss:\", dev_history[-1][-1])\n",
        "for i in range(10):\n",
        "    print(generate(rnn_model, temperature=0.5, max_len=300))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert np.mean(train_history[:10], axis=0)[1] > np.mean(train_history[-10:], axis=0)[1], \"The model didn't converge.\"\n",
        "for i in range(10):\n",
        "    print(generate(rnn_model, temperature=0.6, max_len=300))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjPFqZGP9q5s",
        "outputId": "fa862a45-c472-4eb0-8421-fbae2abbdeaf"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Learning and Broad Learning Estimation Cases ; The sub-set of this article on the model for compositionally sparse methods have been lower to perform such as For-Post-Instance Amonomography, which is a simple dependent or an algorithm for a noun of the need for the analysis of the notion of the trai\n",
            " Large-Analysis Of Target Face Retreesing and Semantic Pose ; Auto-entropy deep neural networks that rely one of the sample setmentation for the model approach to accousteric information strategy constructions and remarkable statistical problems, the objective of the theoretical relationships and its\n",
            " A Supervised Learning Approach for Language To word State-Recognition ; This paper presents a novel distribution of the relationship between the automatic problem based on the pattern model that they makes a storether constraint process of population models such data and semantics and problems, such\n",
            " Data Principal Exploration of Revices of Alternative ;ould Detection ; The fine-grained model network to this single-based sentences in the problem of the factorization of the spatial analysis. The main comprehensive and exploration of different design on a constrainting computer vision of the perso\n",
            " Experimental Algorithms for Activation Approach with Computer Transformation ; The context-constraint of a single method for operations or samples of the inference understanding of our relational performance. The means of artificial approaches for machine learning algorithms and compared to a fast t\n",
            " Modeling Acrues and Speech Monte Evaluation ; In this paper, we propose a formal function of the introducion of the memory network to a context statistical problem of the statistical problem, which is the local streve task of the inference of a neural network for computing analysis in the possibilit\n",
            " Term Convex Adeptor For Convergence Reason of Deep   Exploration Technologies ; In this paper, we address the statistical intelligence to accelerate the despite. Atchologies in very conceptual representation and problems are the computational Model for the modeled practical foal optimization and pro\n",
            " Spatially State-of-Graph Control Models for Interval Sparse Continuous   Croupls ; A human variational population of the area of the variational convolutional neural network autoencoders are information is to provide a sketched model and computer visive complexity such as the accuracy of projects, w\n",
            " Deep Convolutional Neural Networks for Relational Complexity ;ut ; This paper investigates the constraint system in detection, management of the setting by meaning-problems, and introduced relavionship between the non-solving deep learning problem in machine learning that are often in why applicatio\n",
            " Training the More Discretation for Explanation of Computing ; In this paper, we present an efficient convolutional neural network algorithm for shallow to the high-graph of the distribution of skin some from points of the substantial interaction sources. The single implementation of interest interes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert np.mean(train_history[:10], axis=0)[1] > np.mean(train_history[-10:], axis=0)[1], \"The model didn't converge.\"\n",
        "for i in range(10):\n",
        "    print(generate(rnn_model, 'Artificial intelligence', temperature=0.6, max_len=300))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Puh-HEHK95oV",
        "outputId": "e910bfaa-b599-4b90-aae5-6755040a1312"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial intelligence models using spatial intelligence and models ; The proposed stnames to practical and algorithm can be developed to our approach for small detection of the screeting or action learning problems. An approximate surface detection system the reconstruction system are contextual to\n",
            "Artificial intelligence of expertation of   programming mapping in the context-of-the-art part of the   the maximilation of the sequences ; The computational components of the target of latent classification problems. In this paper, we explore the problem of the modeling of the novel motion of multip\n",
            "Artificial intelligence surface approaches for problem in the decomposed Representation of Machine Ensembles ; Artificial particular recognition is a feature of the distribution of the simple training steps of the problem of the redularized distributions of the difeasion of a single gradient speech. \n",
            "Artificial intelligence to a polynomial image analysis of a component of   real-world and specific cancer datasets of controlle evolutional approximations of the presentation of an  the application of the task analysis of the complex fractal patterns of fast can be support to compare these stations a\n",
            "Artificial intelligence that miting a constraint of the computation of computer vision languages are the approach of the information of latent contentions consists of the underlying the regissue to the  -GAN methods are generally evaluated to biological and several approximated modeling the problem a\n",
            "Artificial intelligence to the notterial model for relational parameter properties of the non-start performance of extression problems state detection ; Complete or the physical scale interaction is a spectral property of the depths are attreaded to propose a neuromort to train a computational transf\n",
            "Artificial intelligence of the integration of image theory into a multi-label machine learning framework that can be recognized that shortest the problem of denous developments are considered to explore the end-to-end tool for the otherwide in multiple tracking approaches. In this paper, we propose a\n",
            "Artificial intelligence on object to impresent optimization in the problem ; In particular methods for proposed by practical analysis of one of the alternative object tree on the parameter related and semantic scale in the problem of other or some of the support specificity. We show that the training\n",
            "Artificial intelligence and computer vision on the interaction of polynomial variation systems, where the original problem with the use of interest of the recent problem of a counting in this paper proposes a brain and computation of similarity means of deep neural networks. The local optimization pr\n",
            "Artificial intelligence of the input invanions are successfully affected we explore an internal form of the contern of intelligent distribution algorithms for several methods are used to the same semantic features for meaningly detection. In this paper, we propose a novel reported to understand in th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert np.mean(train_history[:10], axis=0)[1] > np.mean(train_history[-10:], axis=0)[1], \"The model didn't converge.\"\n",
        "for i in range(10):\n",
        "    print(generate(rnn_model, 'Natural language', temperature=0.6, max_len=300))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAct7yNW0ShY",
        "outputId": "5e70d3f6-2610-4baa-b3ac-d0e98e8f80e5"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural language of the Development of Recurrent Neural Networks with Monological Models with Event Prior Machine Learning   Based Containing System   for Substantial Transfer Input Sparse Renots for All States of   Digital Pre System Study on Support Genetic  Abappling ; Unlinite theory are provided\n",
            "Natural language and analyses of the context of the paradigm of specifically supervised anomy models understanding the scalable transformation and artificial processing problems are experience of each of structure of a comparison of our work on the problem of extremes and computational by a represent\n",
            "Natural language of practical data is that an approximate classification of the spectral structure of subtle given statistical and introduction of the significant subject of short-based distributions in the problem of the topic resolution problem. In this paper, we propose a single problem of the app\n",
            "Natural language recognition in the grounding classification of the state-of-the-art to selifies a simulation of the recent addresses the loss of information and robust selection and computational relocation to an analyzing the orthogonal distribution of the automatically management, and sample to th\n",
            "Natural language of the sensitivity of a particula the shortcoming of neural networks, and textual renerators to the benefit problem of software classification based in the recent  not recognition and another developing complexity from the problem of solving analysis the research instent of the probl\n",
            "Natural language estimation using steps of image and convolutional neural networks ; We propose an overall and needed optimization of the article image color accuracy of the fact that are training and industries and a supervised learning method for the activity system that can a label of computer vis\n",
            "Natural language methodology and its linearity construction in deep learning algorithms for automatically introducing statistical models in the  of the vector is to selected showing tool and an integerable approach to learn the computer vision ; We propose a novel constraint to the problem of lower t\n",
            "Natural language representation in one of the source of the autononomous model tasks in computer sparse scales are consistent in a component optimization of the target of convex classical selection and wellows that computer therefore such as the problem of components and possibly analysis ; The probl\n",
            "Natural language processing of marme deporting recognition and subspaces with manifold extraction ; We propose a text data is an attribute to the problem. Our key task accurate in particular, we propose an information in the original model with the possible strategy, and the expert or correlation of \n",
            "Natural language of component correspondences are not better instance in the word probabilistic complexity and collecting expension of the controls in  the problem of continuous range of the art of computational  normalization and experiments to the  normalization to the another or interaction and of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert np.mean(train_history[:10], axis=0)[1] > np.mean(train_history[-10:], axis=0)[1], \"The model didn't converge.\"\n",
        "for i in range(10):\n",
        "    print(generate(rnn_model, 'Transformer', temperature=0.6, max_len=300))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJWNYMjN-OKX",
        "outputId": "c67e848a-6e76-4c3a-afd7-e1df82b362a7"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer-construction of computer optimization of a human parameter of interest problems ; The convex optimization and approaches concept to obtain the interaction of a discrete of the contextual content to the feature satisfactory. In this paper we partine the one of the system with speech space \n",
            "Transformer classification models recognition scheme that the problem of described minimization ; We the time algorithm discasted to a contextual processing combination of a seen approximation between a convergence of camera performance. An explain spatially defined and settings of the problem of nat\n",
            "Transformers of the attention of the Treedy Decision for veres of the function of a presence of   recognition ; We consider the results of the classification of a supervised power of an evolutionary processing that the distributional results of these languages or spatial textures in the sequence of c\n",
            "Transformer Variational Supervised Symbol Sorting   Support Description Estimation ; This paper describes the problem of the proposed framework to success the task of interactions from a consistent device on the system can be proposed to the number of computational specifically, we propose a simple c\n",
            "Transformertive Disambiguation:   Analysis of Automatic Programming ; Many optimization is provided the shorthing noisy of the images is a continuous contextual of prediction and interaction in the training splioted study. This paper introduces a probabilistic progress of the training of the state-of\n",
            "Transformer-entropy low-of-the-based context of the problems representation ; In this paper, we propose a specific traphical structure, optimal in the posed control in the enough-based approach to handle the statistical dataset in the manipulation of the problem of analysis of the supervision. In thi\n",
            "Transformered Filtering Sparse Problems ; We present a both activity of the detection of clustering is an important to be used-based corpus deformation to control selections are provided to the same content of the number of a further decomposition. The output involves of artificial neural network and\n",
            "Transformer-Controlled Search for Detection using Bayesian Networks ; In design long to the knowledge neural network (ASs and tensor problems in the linear model of the interaction of expert and much representation. This paper presents a major analysis of model despect learning is to design a combina\n",
            "Transformers and the View of Specifically and   Dynamic Targeting Machine   Processes with Stochastic Learning ; In this paper we propose a novel approach to exploit models for a graph-set of the sequence of source labels with deep convolutional neural networks. This paper introduces a personal frame\n",
            "Transformer: An Hardware Model and Complex Graphs for Combining Accuracy of   Resolution Structure ; The expert system of accurately, it is usually applied to the generalised convolutional neural networks for such mapping can be recommended by scale the people and the devicing the significant subspac\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert np.mean(train_history[:10], axis=0)[1] > np.mean(train_history[-10:], axis=0)[1], \"The model didn't converge.\"\n",
        "for i in range(10):\n",
        "    print(generate(rnn_model, 'Transformer', temperature=0.5, max_len=300))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmpjV7At-bpj",
        "outputId": "08718a23-9d2e-40e7-f43a-8ab9fdbde200"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformertic Image Detection with Sone Search ; Practical and representation of frameworks in many object terms of the performance of the fields of the state-of-the-art of the sequences of such as a composition of the task. However, the generative method for predicting the standard state of images \n",
            "Transformer-scale optimal context of the development of the high-resolution context ; We present a very analysis of the stare feature of the selection problem. We propose a novel term accurate and a classifier that the algorithms in the structured dataset by provide a correlation of a training proble\n",
            "Transformer Face Detection for Constrained Fast Machine Classification ; The experimental optimization of network to the detection of the source to develop an analysis of the computational variational system is introduced by the original approach to learn from a simultaneously research to construct t\n",
            "Transformer-Convolutional Neural Network Models ; We propose that are adversarial attention is a trained feature for the computer vision of the resulting and content of adversarial industries and an interpreting parallel in a single decomposition problem. In this paper, we propose a new model for the\n",
            "Transformer-based Factorization using Word Transformations ; In this paper, we propose a novel approach for the problem of a single form of the content, they are not have become that the computational models in the use of the captured datasets, we propose a novel accuracy of a subsets of the distance\n",
            "Transformers for real-time features for the sequence of sparse methods ; In this paper, we present an intringing support to solve that the system staces and unsupervised generations in a text. The conditioned approach is to consider a widely problem of a context in advanced and real with a large-scal\n",
            "Transformery stored by the State of Resolution of Evolutionary Programming ; The performance of probabilistic interval or computational problems are not only outperformed in a temporal approach to the stochastic problem of the sequence of a significant process of complex complexity. The theoretical d\n",
            "Transformerning structure of an example of manually detection of data models   with a concept of the point of the stabel that can be used ; We present a control of the search algorithm to extract content, and advanced information and the problem of which the system and deconstrained between convergen\n",
            "Transformers of a high-draw for computer vision of the optimal search is to describe a novel towards and algorithm that sometrics the many of the problem of the distribution of the statistical state of all problem in a computational response to provide an important and the developmed of the expert on\n",
            "Transformers of low-rank recognition methods in consistent from present expertation methods ; We propose a simple information of the computer vision problem on the content of the problems without an optimum detection of the analysis, compared to recorded to a single state-of-resolution and the orderi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert np.mean(train_history[:10], axis=0)[1] > np.mean(train_history[-10:], axis=0)[1], \"The model didn't converge.\"\n",
        "for i in range(10):\n",
        "    print(generate(rnn_model, 'Markov chain ', temperature=0.5, max_len=300))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jgd38gMp_CK8",
        "outputId": "cc6f938d-ec58-4298-c4e3-62c11895bb8e"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Markov chain state-of-the-art models ; We present a new information of event and the technique for speech problem in a contextume that the conditional context of a conventional convergence in a target content of the language from the same and estimation for transformations. Etch empirical actions are\n",
            "Markov chain open-task ; The problem of specifically, we propose a novel framework for a set of stochastic and problem using the training of the semantic infular inentifying state of the computer visual scales of variables dataset. The contextual encoder for image synthesis of a single approach to le\n",
            "Markov chain approaches ; Convolutional neural networks have been widely used to a proof of the result of the sequence of the contextural specific and context of an evolutionary tool in dimensionality of $\\mathbb{\\emph{linear) (GOC). This, we propose a novel model which is the development of the stru\n",
            "Markov chain an artificial analysis of the statistical model ; We consider the convolutional neural network that have a simple and alternative term of more analysis and automatic decision problems in a parameter description. In this paper, we propose a novel problem of functional common contexts of t\n",
            "Markov chain rules and computational   structures of a discourse conditional problem ; In this paper, we present a new approach is a set of models and interest in statistical processing techniques for solving the basical segmentation from behavior problems in the possible approaches in the most state\n",
            "Markov chain context of the   the outlier of the prediction of the   partition in the sparse state of   learning and function ; The proposed context of the proportional processing problem is a counterprint of the same policy and algorithm that control or the applications of the state-of-the-resulting\n",
            "Markov chain recognition of the variational color   for seconder framework in the pace of   has been combined on the machine learning   problems ; In this paper we aim to investigate the computational system in a function of probabilistic languages based on the first subspace to the large-scale conte\n",
            "Markov chain a summarient scale   surface of the detection of   extraction of representations for constrained and   approach to the concept of fuzed semantic   reasoning ; Restricted segmentation problems, is the problem of the incorporation of the task of real-world approaches that the contextual pr\n",
            "Markov chain limited settings ; The context-of-the-description of the supervised image segmentation of the most statistical arbitude is the number of an online labeled as a complex model and subjection of the interactions from the shart feature constraints. The discusses of the augmented based on the\n",
            "Markov chain components ; In this paper, we propose a new context oaserver of the original complexity of many researchers to a significant constraint of the matching setting is presented as a single model of researchers and continuous sensitivity. The constraint the construction of model to the prove\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert np.mean(train_history[:10], axis=0)[1] > np.mean(train_history[-10:], axis=0)[1], \"The model didn't converge.\"\n",
        "for i in range(10):\n",
        "    print(generate(rnn_model, temperature=0.2, max_len=500))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAwIaPTzxgs2",
        "outputId": "7f075a4c-950b-4e5f-ed3e-5ccc085b2a3b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A Convolutional Neural Network for Computation ; We propose a new approach to the application of the art of the analysis of the approximation of the problem of the problem of a set of the approach to the algorithm for the state-of-the-art state-of-the-art content of the problem of the problem of the set of the approach to consider the problem of the target structure of the statistical problem. In this paper, we propose a novel method for a set of the computational structure of the set of the pro\n",
            " A Convonnt Spatial Control of the Inference for Convex   Distribution of How Semantic Machine Learning ; In this paper, we propose a new method for the interpretation of the semantic data and a single content of the problem of the structure of the problem of the standard set of the approach to the context of the are and application of the area of the controllers to solve the strong of the art set of the set of the problem of the optimization of the computational system to the transform of the an\n",
            " A Control of Semantic Methods for Exploiting Model ; The problem of interest and the problem of the problem in the optimization of the problem of sensitive information and set of the context of the computational formal interestion by a single set of the state-of-the-art approach to a computational feature of the problem of the different space of the article of the set of the sequence of a simple of the problem of the problem of a set of the set of the state-of-the-art context of the article and \n",
            " A Convolutional Approach for Convolutional Neural Networks ; This paper proposes a novel approach to a simple transformation of the problem of the problem of a set of the system of the particular convolution and a set of the different problem. The problem of the such as a sparse probabilistic approach to the state of the problem of the computation of the context of the sensor setting is a computation of the optimal problem. We propose a novel approach to a signal task of the set of the control i\n",
            " A Convexity of the State-of-the-art Convergence of Expert ; The problem of the approach to as the problem of the search of the search of the set of the problem of a set of the setting of the distribution of the state-of-the-art contribution. In this paper, we propose a non-linear component of the problem is a computational formalism of the set of the set of some constraints of the computational conditions of the problem of a set of the accuracy of the state of the context of the computational mo\n",
            " A Convolutional Neural Network for Machine Learning ; We propose a new approach to a large set of the state-of-the-art and the problem of the set of the art and the constraint of the problem of the context of a simple network to constraint the state-of-the-art detection and accomproving the state of the state-of-the-art and in the problem of a graph context. The problem of the model that are compared to the problem of the context of the approach to a specifical context of the approach to the con\n",
            " A Convolutional Neural Network for Convolutional Neural Networks ; We propose a novel approach to a single semantic component of the matrix of the state of the set of the problem of the adaptation of the semantic structure of the stale of the context of the problem of the state-of-the-art and the content of the semantics of the problem. The restriction of the art of the approach is a convexity of the state-of-the algorithms to recognize the same set of the training of the network to the speech a\n",
            " A Search of Statistical Medical Context of Semi-Supervised   Deep Learning ; The problem of state-of-the-art probabilities is a set of the set of the set of the first state of the model that the operator of the problem of the problem of the artificial method for the experiment of the computational structure of the set of the context of the approach to solve the set of the context of the sensitive problem of the context of the semantic control problem in the state-of-the-art probability and const\n",
            " A Convolutional Neural Network for Large Search Analysis ; In this paper, we propose a novel approach to a computational strategy of the set of the complexity of the problem of the data of the approach to the simple of the set of the algorithm is the subspace of the oengingical and computational signals to the interest of the artificial data are applied to a state-of-the-art and semantic set of the systems. The problem of the problem of the problem of the art controlue to achieve the problem of \n",
            " A Computational Contingous Approach for Semantic Programming ; We propose a novel method for the problem of a set of the set of the problem of the structure of the problem of a set of the semantic problem of the problem of the most recognition and access of the structure of the sensitive and a set of the set of the set of the state-of-the-art classification of the stage of the analysis. The proposed approach to the problem of the state-of-the-art a deep neural network for the concept of the fina\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert np.mean(train_history[:10], axis=0)[1] > np.mean(train_history[-10:], axis=0)[1], \"The model didn't converge.\"\n",
        "for i in range(10):\n",
        "    print(generate(rnn_model, temperature=1, max_len=500))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uGDbZy6y5Ms",
        "outputId": "738a8470-41ba-466f-9f29-80502b9bcc5e"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Optified Location and Lolar Roword Analysis, Randov Quadratory (L Nearest   Geongsts And Intelligent Human Captures ; There categories in the first with non-side, although recognition to the veriete text of drawn and coordinately poperious penaltic wavelets which appropriates and canonused symbetchilated, capture than new method-bounded point intentio\n",
            "\n",
            " Simplears (QST) via Hu\n",
            "\n",
            " Wumps: A Neural Enviring Upon Rule Kernel TextS on Conditional IIT-P/IFDIC ; We refer\n",
            "\n",
            " Measury by Servicipation Slow for Several Problem ; Multi-agent model to free supervised level and monitoring image dense spiking, regademihic interest as the weak dimensionality referenc\n",
            "\n",
            " Troughnes for Combinatorial Localization Account in Texts of 3D ; A mapping of video experment combine of influence conventional and quantitity and bord outlinual analysis. Necroider much anotherior in this problem users, the runtry to vitoha the artions for existing parametrics, and utilizes the popular wirst, weneral applications, which couples, each intrusion stace tasked in the System network stMb is moved to doweriotes researchers with high-approach. They some pattern restence have been oft\n",
            " Braid Learning Element Has and Sentiment Temporalization ; To role similarity precision is a tree-ideal correlation in 2a)y popularity ones. One low-to-ener convolution) throughond underly we train sexment,\n",
            "\n",
            " Guided Actions, and Nonparisons Approximately Including Model-iser-W Speech variational   Sport setting ; Producing most Remotely are explored by a singlex extension that assisfed to probuse a distributed learning performance in most many fitness observable family extracting subver and consisting systems in whethere\n",
            "\n",
            " Class wfich center in presential models ; Good allows these semantic and a dissignal estimation of HAT region) negword (RGB) to different including words and automatically descriptions important ipic typical influence\n",
            "\n",
            " Offleed F-Bayesian networks for search tasks of   convolutional networks with learning filting emlibies however, hynamical intervals ; This data using the model, esemal specified architectures sized with extent models feasers there developed the last small statistical experiment. This paper  We are effectively distributed with utilities: We present a local search to a data of the do nould instances, with the madeity of linguistic, avaltitates law in donal finite. There is a variables continuous \n",
            " Evolutionary Quality Vector Processing in Recent Architecture ; Not deland, becomes a will large quaction writing approach from meaningfut statement of interact of the bandit assumption and computate statistical tasms. These Prediction (Pes bases other both from miscultiford-cutulations, likelih(interesting microscopy, and the proposed typical learning. Therefores several syntems an abstract of models, a rang that adapt\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRpf5rIdIoU8"
      },
      "source": [
        "### Alternative sampling strategies (1 point)\n",
        "\n",
        "So far we've sampled tokens from the model in proportion with their probability.\n",
        "However, this approach can sometimes generate nonsense words due to the fact that softmax probabilities of these words are never exactly zero. This issue can be somewhat mitigated with sampling temperature, but low temperature harms sampling diversity. Can we remove the nonsense words without sacrificing diversity? __Yes, we can!__ But it takes a different sampling strategy.\n",
        "\n",
        "__Top-k sampling:__ on each step, sample the next token from __k most likely__ candidates from the language model.\n",
        "\n",
        "Suppose $k=3$ and the token probabilities are $p=[0.1, 0.35, 0.05, 0.2, 0.3]$. You first need to select $k$ most likely words and set the probability of the rest to zero: $\\hat p=[0.0, 0.35, 0.0, 0.2, 0.3]$ and re-normalize: \n",
        "$p^*\\approx[0.0, 0.412, 0.0, 0.235, 0.353]$.\n",
        "\n",
        "__Nucleus sampling:__ similar to top-k sampling, but this time we select $k$ dynamically. In nucleous sampling, we sample from top-__N%__ fraction of the probability mass.\n",
        "\n",
        "Using the same  $p=[0.1, 0.35, 0.05, 0.2, 0.3]$ and nucleous N=0.9, the nucleous words consist of:\n",
        "1. most likely token $w_2$, because $p(w_2) < N$\n",
        "2. second most likely token $w_5$, $p(w_2) + p(w_5) = 0.65 < N$\n",
        "3. third most likely token $w_4$ because $p(w_2) + p(w_5) + p(w_4) = 0.85 < N$\n",
        "\n",
        "And thats it, because the next most likely word would overflow: $p(w_2) + p(w_5) + p(w_4) + p(w_1) = 0.95 > N$.\n",
        "\n",
        "After you've selected the nucleous words, you need to re-normalize them as in top-k sampling and generate the next token.\n",
        "\n",
        "__Your task__ is to implement nucleus sampling variant and see if its any good."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "Z7UgNuyFIoU9"
      },
      "outputs": [],
      "source": [
        "def generate_nucleus(model, prefix=BOS, nucleus=0.9, max_len=100):\n",
        "    \"\"\"\n",
        "    Generate a sequence with nucleous sampling\n",
        "    :param prefix: a string containing space-separated previous tokens\n",
        "    :param nucleus: N from the formulae above, N \\in [0, 1]\n",
        "    :param max_len: generate sequences with at most this many tokens, including prefix\n",
        "    \n",
        "    :note: make sure that nucleous always contains at least one word, even if p(w*) > nucleus\n",
        "    \n",
        "    \"\"\"\n",
        "    while True:\n",
        "        token_probs = model.get_possible_next_tokens(prefix)\n",
        "        tokens, probs = zip(*token_probs.items())\n",
        "        probs = np.array(probs)\n",
        "\n",
        "        sorted_probs_indices = np.argsort(probs)[::-1]\n",
        "        sorted_probs = probs[sorted_probs_indices]\n",
        "        # choose only N % highest probs, discard rest\n",
        "    \n",
        "        cumulative_probs = np.cumsum(sorted_probs)\n",
        "\n",
        "        remove_sorted_indices = cumulative_probs >= nucleus\n",
        "        # add one extra word to make sure \n",
        "        # we cover at least one word even if p(w*) > nucleus\n",
        "        remove_sorted_indices[..., 1:] = np.copy(remove_sorted_indices[..., :-1]) # last True -> False\n",
        "        remove_sorted_indices[..., 0] = 0 # if all of them were True, make at least first one word to False \n",
        "\n",
        "        indices_to_remove = sorted_probs_indices[remove_sorted_indices]\n",
        "        probs[indices_to_remove] = -float('Inf')\n",
        "\n",
        "        # compute softmax over changed array of probabilities\n",
        "        probs = np.exp(probs)\n",
        "        probs /= np.sum(probs)\n",
        "\n",
        "        next_token = np.random.choice(tokens, p=probs)\n",
        "        prefix += next_token\n",
        "        if next_token == EOS or len(prefix) > max_len: break\n",
        "\n",
        "    return prefix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "v4dVgU5jIoU9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3a2c874-9e80-457c-df36-b50d520e9f82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nucleus sampling with CNN model\n",
            "\n",
            "Nucleus value = 0.5\n",
            " Complexity of Structured Speech Recognition and More Probabilistic Models ; The constraint that are experiently for the maps of a popular process of properties in the discourse of problems with a probabilistic detection of convolutional neural networks are not be introduced to the segmentation of th\n",
            " Learning Segmentation of Segmentation in Social Statistics ; We consider the most context of the approach to compute the field of machine learning in computational experts of a probabilistic descriptor in a sequence of computer vision problems. The mapping of stochastic analysis is compared to such \n",
            " Learning Sparse Analysis for Sparse Sparsing State Segmentation ; The semantic system is a probability of the research to the sentence of point structures of such problems. The approach is a complete set of a control that are increasingly specifically applied to the relational sparse and experimenta\n",
            " Tool in an autonomous of social methods ; We propose a social method to entire process of the context of a point state of the problem of detection and implementations are the distribution of the problem. The state of speech recognition is a method to a proper to context on a computational sparse of \n",
            " An Interest of Artificial Learning with A Complex Sensitive Approach ; We propose a theoretical order to produce a computational construction for computational datasets in the context of an efficity to analyze the context structure. In this paper, we propose a more computational model of a problem o\n",
            "\n",
            "Nucleus value = 0.6\n",
            " Sentence Decomposition for Multiple Markov Analysis ; Algorithm that performance can also develop support to state of computer ordering approaches for the transformation that serves of a different class a point particular distribution of the exploration and such a linear theoretical strong to provid\n",
            " Multi-Dimensional Distribution with Events with Computational Complexity ; The same state-of-the-art data is proposed to derive simultaneously extensions are used in model selection problem (resears. Here introduces in constructing the proposed performance of many constraints that the posterior from\n",
            " The Extraction with Point Decision Retrieval for Extraction ;ou Manifold Memory for   Surference for Speech Computational Completion ; In this paper, we propose a fore in a distributed search to be also considered by such and accuracy for machine learning model and person which is contextually invol\n",
            " Deep Neural Networks for Rolernative Approaches ; In this paper, we propose a sequence of parallelisation algorithms that provides a majority of complex superior detection and an input approach for experimental approaches in statistical point models of distributed and the analysis of most provides a\n",
            " Essumption of Graph and Content-or Decision ; The supervised performance of the function algorithm is contrast of speech studies and allorithms with detecting patterns that allow an increase and sentence in more to the target are enabled in the original structure for machine learning. Here, we propo\n",
            "\n",
            "Nucleus value = 0.7\n",
            " Maximum Clustering (LDMM) by Bayesian Machine Sparse Supervised Single Axgression   Reconstructions ; Monotonous computer speech algorithms (capacle (AUSIM has a simple important tool in order summarizations. We study medical energy context and effective are provided by carronotonic analysis is more\n",
            " Automatic Estimation and Montological Transfer Image Marker Decomposition ; In this paper introduces a neural network to internales computational interaction for dependencies of prediction scale algorithms are computationally developed to model such as process studies. We propose that several encrep\n",
            " Sparse Learning (PAEGS and Son-Approximation Mechanisms ; The representation that they serves a description of component algorithms to the distance is instead, they so a multi-dimensional constraint information problem. The image is provided in the automatic disease, are estimated in constraint sens\n",
            " Optimization and a Set of approximation ; We consider a major analysis of an instances of deep neural optimization or facial context standard maximization that also rely as object classification. We present a novel toor sequence is to extract the complex training processes which developed the constr\n",
            " Construction of Learning Compression Experimental Strategies ; We study the computer random interest from context algorithms are derived from devision methods have been proved for pursourcy by different more specific studies. We present an improved topic and manifold are about and different image tr\n",
            "\n",
            "Nucleus value = 0.8\n",
            " Extensional Nodes beide Regression Approaches ; In additional corpusses products have active memory as fality analysis have treated efficiency, our continuent formulae, search domains, interesting efficiently and optimizes of a formalism well-known functions. Such stochastic features, as suchay, whe\n",
            " Stochastic Properties of Inference Adversarial Learning with Causal Correspondences of The Study   for The Implication for Poly-Also Memory   Features ; We adapt the cloud topolocical segmentation of our resolution interactions are computationally interesting to experts the taple-persistent variable\n",
            " Order-Non-Position Association Using Features   Camera   only Tood on Segmentation ; This artificis and representations approach to perform data piecewise modeling is possible in the signers of the can interaction testing. Forecesting process called recently, hierarchical examples called targeting t\n",
            " Re the Particularly Temporal Exploiting Temporal Routeristodic Evolution of Development Process in Reasoning ; Measure of different intelligent reconstructions for only user each algorithm, it significantly, where algorithms can apply domains competing many of image differences in ontology. On the c\n",
            " Fuzzy a Model Evolutionary Estimation Using a New Identification Model for Local Normal   Component Posteriors   Model Representation and Many Respect Positions ; Commard spatial costs, increasingly object variables with the time-point complex concepts. Sequence learning methods for component signif\n",
            "\n",
            "Nucleus value = 0.9\n",
            " Termal-automatic Bighorteating Opper-Telput Sum-time Po Optical Lickpared   Veribrability Surfaces in Max Synthesis: and   How-Main-Suronocy-sumble Multiple Algorithm where Lexical Scordalities ; Oun fingually, hoch learned uptass, fall-effectively optical storacating dimensions been ite. We prevely\n",
            " Relayions Cressour Generative field scalarly Paportise Foleriated Actions in   Takes Significantly, Binary Elgebues is LSI Aractive ; We enables collaboratically introductional foeld corefards bayed essential poor suffers, do performance of case, to medicate recurrent models. It combine these multi-\n",
            " Global Dongs (Net Digidorighisms Training Time-vocalization Settence for Flant-Vayious Person, Fast Futon Images is Vehy Notcomogy Even Denoireneity of Tike Estimation   Bounds of Data   Gramsmest Bayesian Environments   from Gradiciant Programs of Accurately, Gaussian Training Returnes with Trans  \n",
            " Subtoul data matterized elitivinian functions using generated perturbing mutear ophy medamating changes ; Cloud-tree-code by very intensity data driven by more graphs when each catch perspects retrieval to any well-two insentiated arguments. Frow spocks simule is resters for rark, whows whose closit\n",
            " Using With Loorseppect Perperts to   Barediens from Vulnering Object and testure Group Models   With Impurtature Scenes ; Surneted A robot optimisation from nave-goal-calculus, it plipsizing for buttle hom to efficient interactive wayesian machines. Approximating simpler, normal set regression, main\n",
            "\n"
          ]
        }
      ],
      "source": [
        "nucleus_values = (0.5, 0.6, 0.7, 0.8, 0.9)\n",
        "\n",
        "print('Nucleus sampling with CNN model\\n')\n",
        "\n",
        "for nucleus in nucleus_values:\n",
        "    print('Nucleus value = {}'.format(nucleus))\n",
        "    for i in range(5):\n",
        "        print(generate_nucleus(rnn_model, nucleus=nucleus, max_len=300))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwrKcsTPIoU-"
      },
      "source": [
        "### Bonus quest I: Beam Search (2 pts incl. samples)\n",
        "\n",
        "At times, you don't really want the model to generate diverse outputs as much as you want a __single most likely hypothesis.__ A single best translation, most likely continuation of the search query given prefix, etc. Except, you can't get it. \n",
        "\n",
        "In order to find the exact most likely sequence containing 10 tokens, you would need to enumerate all $|V|^{10}$ possible hypotheses. In practice, 9 times out of 10 you will instead find an approximate most likely output using __beam search__.\n",
        "\n",
        "Here's how it works:\n",
        "0. Initial `beam` = [prefix], max beam_size = k\n",
        "1. for T steps:\n",
        "2. ` ... ` generate all possible next tokens for all hypotheses in beam, formulate `len(beam) * len(vocab)` candidates\n",
        "3. ` ... ` select beam_size best for all candidates as new `beam`\n",
        "4. Select best hypothesis (-es?) from beam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "OLTJ3G8JIoU-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "05ae8ea5-88d3-4361-ce13-b05d8c8c7425"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<!DOCTYPE html>\n",
              "<html lang=\"en\">\n",
              "    <head>\n",
              "        <meta charset=\"utf-8\">\n",
              "        <title>Bokeh Plot</title>\n",
              "        \n",
              "<link rel=\"stylesheet\" href=\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.7.min.css\" type=\"text/css\" />\n",
              "        \n",
              "<script type=\"text/javascript\" src=\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.7.min.js\"></script>\n",
              "<script type=\"text/javascript\">\n",
              "    Bokeh.set_log_level(\"info\");\n",
              "</script>\n",
              "        <style>\n",
              "          html {\n",
              "            width: 100%;\n",
              "            height: 100%;\n",
              "          }\n",
              "          body {\n",
              "            width: 90%;\n",
              "            height: 100%;\n",
              "            margin: auto;\n",
              "          }\n",
              "        </style>\n",
              "    </head>\n",
              "    <body>\n",
              "        \n",
              "        <div class=\"bk-root\">\n",
              "            <div class=\"bk-plotdiv\" id=\"ff8c3f31-952d-4c2f-8b58-13e7cec51b58\"></div>\n",
              "        </div>\n",
              "        \n",
              "        <script type=\"text/javascript\">\n",
              "            (function() {\n",
              "          var fn = function() {\n",
              "            Bokeh.safely(function() {\n",
              "              var docs_json = {\"ba84f797-d201-498d-a731-5adafa5447b7\":{\"roots\":{\"references\":[{\"attributes\":{\"plot\":null,\"text\":\"Beam search\"},\"id\":\"5af81591-5793-4721-a459-e0a4ca700855\",\"type\":\"Title\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"line_width\":{\"field\":\"line_width\"},\"size\":{\"units\":\"screen\",\"value\":24},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"43ae4eb3-d229-4335-a758-a5d18149bd65\",\"type\":\"Circle\"},{\"attributes\":{\"bounds\":[-10.0,20.0],\"callback\":null,\"end\":12,\"js_property_callbacks\":{\"change:end\":[{\"id\":\"eab13fb4-9405-4d14-8bf6-46c6f698b4bb\",\"type\":\"CustomJS\"}]},\"start\":-1},\"id\":\"14e9976c-9458-4bce-be96-da2f3c304cec\",\"type\":\"Range1d\"},{\"attributes\":{\"source\":{\"id\":\"53346d3c-7b75-4689-95d1-c395b23fa5b8\",\"type\":\"ColumnDataSource\"}},\"id\":\"2e9dcf01-3f79-4337-b7dd-26f0525180b9\",\"type\":\"CDSView\"},{\"attributes\":{\"data_source\":{\"id\":\"49a8987d-d307-4797-9fae-8ec771f76b48\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"08bbcb52-00b5-4803-9e7a-9252db09708d\",\"type\":\"Text\"},\"hover_glyph\":null,\"muted_glyph\":null,\"name\":\"tokens\",\"nonselection_glyph\":{\"id\":\"3621fd59-3cb3-4305-8802-6f7f75a23fd6\",\"type\":\"Text\"},\"selection_glyph\":null,\"view\":{\"id\":\"ddf587aa-019f-43a1-af8c-52144e732785\",\"type\":\"CDSView\"}},\"id\":\"a2ab1853-0966-4519-874f-956f801f1c72\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"id\",\"parent_id\",\"children_ids\",\"is_best\",\"depth\",\"hypo_i\",\"token\",\"token_id\",\"x\",\"y\",\"circle_fill_color\",\"line_color\",\"line_width\",\"edge_xx\",\"edge_yy\",\"token_text\",\"token_font_size\",\"hypo_i_text\",\"hypo_i_offset\",\"_on_hover_token\",\"_on_hover_token_id\",\"_on_hover_score\"],\"data\":{\"_on_hover_score\":[\"-4.7282\",\"-4.2071\",\"-4.8782\",\"-1.3577\",\"-4.0420\",\"-4.2097\",\"-4.5624\",\"-3.2410\",\"-6.9225\",\"-4.6717\",\"-2.8497\",\"-3.8023\",\"-4.1460\",\"-3.6443\",\"-4.3135\",\"-4.7070\",\"-3.1373\",\"-4.5105\",\"-6.1526\",\"-3.0974\",\"-3.7421\",\"-3.4956\",\"0.0000\",\"-5.2694\",\"-4.4104\",\"-6.1752\",\"-3.7617\",\"-4.6281\",\"-6.5626\",\"-3.7175\",\"-0.7384\",\"-4.5439\",\"-5.2967\",\"-3.6831\",\"-5.2153\",\"-3.7830\",\"-3.1335\",\"-0.8716\",\"-3.7312\",\"-3.8895\",\"-6.5984\"],\"_on_hover_token\":[\"_EOS_\",\"\\u043f\\u0440\\u0430\\u0432\\u0438\\u0442\\u0435\\u043b\\u044c\\u0441\\u0442\\u0432\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0439\",\",\",\"\\u0443\\u043f\\u043b\\u0430\\u0442\\u044b\",\"\\u043e\\u0442\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u0432\",\"_EOS_\",\"\\u0432\\u044b\\u0441\\u0442\\u0443\\u043f\\u0430\\u0435\\u0442\",\"_EOS_\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\\u0435\",\"\\u043d\\u0430\\u043b\\u043e\\u0433\\u043e\\u0432\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043a\\u0438\",\"\\u043d\\u0435\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\",\"<empty>\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\".\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u043d\\u0435\",\"\\u043e\\u0442\\u0432\\u0435\\u0442\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u0438\",\"\\u0437\\u0430\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\",\",\",\"\\u043e\\u0442\",\"`\\u0430\\u044e\\u0449\\u0438\\u0435\",\"_EOS_\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0430\\u043b\",\"\\u043d\\u0435\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0435\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u043f\\u0440\\u0435\\u0442\\u0435\\u043d\\u0437\\u0438\\u0439\"],\"_on_hover_token_id\":[1,780,25,2482,15356,2482,1879,3,17084,25,13862,5,1,7691,1,15356,25,15328,4255,11685,11,3487,-1,21342,15356,4,21342,11,1781,30,3622,3,25,1078,1,5093,26478,11,2451,13862,22624],\"children_ids\":[[],[[2,3]],[[10,3],[10,2],[10,1],[10,0]],[[4,3],[4,1],[4,0]],[[9,3],[9,1],[9,0]],[],[],[[6,1]],[],[[8,2]],[[5,0]],[[2,2]],[],[[4,2]],[],[[7,2]],[[6,2],[6,0]],[],[],[[5,1]],[[2,1]],[],[[1,3],[1,2],[1,1],[1,0]],[[9,2]],[[8,1]],[],[[7,3],[7,1]],[],[],[[5,2]],[[2,0]],[],[],[[8,3],[8,0]],[],[[7,0]],[],[[3,3],[3,2],[3,1],[3,0]],[[5,3]],[[6,3]],[]],\"circle_fill_color\":[\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\"],\"depth\":[7,1,9,3,8,2,6,5,10,7,4,1,9,3,8,6,5,2,10,4,1,3,0,8,7,9,6,2,10,4,1,5,8,7,9,6,3,2,4,5,10],\"edge_xx\":[[6.0,7.0],[0.0,1.0],[8.0,9.0],[2.0,3.0],[7.0,8.0],[1.0,2.0],[5.0,6.0],[4.0,5.0],[9.0,10.0],[6.0,7.0],[3.0,4.0],[0.0,1.0],[8.0,9.0],[2.0,3.0],[7.0,8.0],[5.0,6.0],[4.0,5.0],[1.0,2.0],[9.0,10.0],[3.0,4.0],[0.0,1.0],[2.0,3.0],[0.0,0.0],[7.0,8.0],[6.0,7.0],[8.0,9.0],[5.0,6.0],[1.0,2.0],[9.0,10.0],[3.0,4.0],[0.0,1.0],[4.0,5.0],[7.0,8.0],[6.0,7.0],[8.0,9.0],[5.0,6.0],[2.0,3.0],[1.0,2.0],[3.0,4.0],[4.0,5.0],[9.0,10.0]],\"edge_yy\":[[4.833333333333333,4.333333333333333],[0.0,-1.5],[3.333333333333333,3.333333333333333],[1.5,3.0],[2.833333333333333,3.333333333333333],[0.5,0.5],[4.333333333333333,3.833333333333333],[2.833333333333333,2.833333333333333],[3.333333333333333,1.833333333333333],[0.0,0.0],[3.0,4.333333333333333],[0.0,-0.5],[3.333333333333333,4.333333333333333],[1.5,0.0],[5.333333333333333,5.333333333333333],[0.0,0.0],[4.333333333333333,4.333333333333333],[-0.5,-0.5],[3.333333333333333,4.833333333333333],[3.0,2.833333333333333],[0.0,0.5],[1.5,1.0],[0.0,0.0],[0.0,0.0],[4.833333333333333,5.333333333333333],[3.333333333333333,2.333333333333333],[4.333333333333333,4.833333333333333],[-1.5,-1.5],[3.333333333333333,3.833333333333333],[0.0,0.0],[0.0,1.5],[1.8333333333333333,1.8333333333333333],[2.833333333333333,2.333333333333333],[2.833333333333333,2.833333333333333],[0.0,0.0],[2.833333333333333,2.833333333333333],[1.5,2.0],[1.5,1.5],[3.0,1.8333333333333333],[0.0,0.0],[3.333333333333333,2.833333333333333]],\"hypo_i\":[3,3,1,0,0,1,2,1,3,2,0,2,0,3,1,3,0,2,0,1,1,2,0,2,1,3,0,3,1,2,0,3,3,0,2,1,1,0,3,2,2],\"hypo_i_offset\":[-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8],\"hypo_i_text\":[\"#3\",\"#3\",\"#1\",\"#0\",\"#0\",\"#1\",\"#2\",\"#1\",\"#3\",\"#2\",\"#0\",\"#2\",\"#0\",\"#3\",\"#1\",\"#3\",\"#0\",\"#2\",\"#0\",\"#1\",\"#1\",\"#2\",\"#0\",\"#2\",\"#1\",\"#3\",\"#0\",\"#3\",\"#1\",\"#2\",\"#0\",\"#3\",\"#3\",\"#0\",\"#2\",\"#1\",\"#1\",\"#0\",\"#3\",\"#2\",\"#2\"],\"id\":[[7,3],[1,3],[9,1],[3,0],[8,0],[2,1],[6,2],[5,1],[10,3],[7,2],[4,0],[1,2],[9,0],[3,3],[8,1],[6,3],[5,0],[2,2],[10,0],[4,1],[1,1],[3,2],[0,0],[8,2],[7,1],[9,3],[6,0],[2,3],[10,1],[4,2],[1,0],[5,3],[8,3],[7,0],[9,2],[6,1],[3,1],[2,0],[4,3],[5,2],[10,2]],\"is_best\":[false,false,false,true,true,false,false,true,false,false,false,false,true,false,false,false,false,false,false,true,false,false,true,false,false,false,false,false,false,false,true,false,false,true,false,true,false,true,false,false,false],\"line_color\":[\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\"],\"line_width\":[1,1,1,3,3,1,1,3,1,1,1,1,3,1,1,1,1,1,1,3,1,1,3,1,1,1,1,1,1,1,3,1,1,3,1,3,1,3,1,1,1],\"parent_id\":[[6,0],[0,0],[8,0],[2,0],[7,0],[1,1],[5,0],[4,1],[9,1],[6,3],[3,0],[0,0],[8,0],[2,0],[7,1],[5,2],[4,0],[1,2],[9,1],[3,0],[0,0],[2,0],[0,0],[7,2],[6,0],[8,0],[5,0],[1,3],[9,1],[3,3],[0,0],[4,3],[7,0],[6,1],[8,2],[5,1],[2,0],[1,0],[3,0],[4,2],[9,1]],\"token\":[\"_EOS_\",\"\\u043f\\u0440\\u0430\\u0432\\u0438\\u0442\\u0435\\u043b\\u044c\\u0441\\u0442\\u0432\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0439\",\",\",\"\\u0443\\u043f\\u043b\\u0430\\u0442\\u044b\",\"\\u043e\\u0442\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u0432\",\"_EOS_\",\"\\u0432\\u044b\\u0441\\u0442\\u0443\\u043f\\u0430\\u0435\\u0442\",\"_EOS_\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\\u0435\",\"\\u043d\\u0430\\u043b\\u043e\\u0433\\u043e\\u0432\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043a\\u0438\",\"\\u043d\\u0435\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\",\"<empty>\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\".\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u043d\\u0435\",\"\\u043e\\u0442\\u0432\\u0435\\u0442\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u0438\",\"\\u0437\\u0430\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\",\",\",\"\\u043e\\u0442\",\"`\\u0430\\u044e\\u0449\\u0438\\u0435\",\"_EOS_\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0430\\u043b\",\"\\u043d\\u0435\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0435\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u043f\\u0440\\u0435\\u0442\\u0435\\u043d\\u0437\\u0438\\u0439\"],\"token_font_size\":[\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\"],\"token_id\":[1,780,25,2482,15356,2482,1879,3,17084,25,13862,5,1,7691,1,15356,25,15328,4255,11685,11,3487,-1,21342,15356,4,21342,11,1781,30,3622,3,25,1078,1,5093,26478,11,2451,13862,22624],\"token_text\":[\"_EOS_\",\"\\u043f\\u0440\\u0430\\u0432\\u0438\\u0442\\u0435\\u043b\\u044c\\u0441\\u0442\\u0432\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0439\",\",\",\"\\u0443\\u043f\\u043b\\u0430\\u0442\\u044b\",\"\\u043e\\u0442\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u0432\",\"_EOS_\",\"\\u0432\\u044b\\u0441\\u0442\\u0443\\u043f\\u0430\\u0435\\u0442\",\"_EOS_\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\\u0435\",\"\\u043d\\u0430\\u043b\\u043e\\u0433\\u043e\\u0432\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043a\\u0438\",\"\\u043d\\u0435\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\",\"<empty>\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\".\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u043d\\u0435\",\"\\u043e\\u0442\\u0432\\u0435\\u0442\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u0438\",\"\\u0437\\u0430\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\",\",\",\"\\u043e\\u0442\",\"`\\u0430\\u044e\\u0449\\u0438\\u0435\",\"_EOS_\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0430\\u043b\",\"\\u043d\\u0435\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0435\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u043f\\u0440\\u0435\\u0442\\u0435\\u043d\\u0437\\u0438\\u0439\"],\"x\":[7.0,1.0,9.0,3.0,8.0,2.0,6.0,5.0,10.0,7.0,4.0,1.0,9.0,3.0,8.0,6.0,5.0,2.0,10.0,4.0,1.0,3.0,0.0,8.0,7.0,9.0,6.0,2.0,10.0,4.0,1.0,5.0,8.0,7.0,9.0,6.0,3.0,2.0,4.0,5.0,10.0],\"y\":[4.333333333333333,-1.5,3.333333333333333,3.0,3.333333333333333,0.5,3.833333333333333,2.833333333333333,1.833333333333333,0.0,4.333333333333333,-0.5,4.333333333333333,0.0,5.333333333333333,0.0,4.333333333333333,-0.5,4.833333333333333,2.833333333333333,0.5,1.0,0.0,0.0,5.333333333333333,2.333333333333333,4.833333333333333,-1.5,3.833333333333333,0.0,1.5,1.8333333333333333,2.333333333333333,2.833333333333333,0.0,2.833333333333333,2.0,1.5,1.8333333333333333,0.0,2.833333333333333]}},\"id\":\"53346d3c-7b75-4689-95d1-c395b23fa5b8\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_scroll\":{\"id\":\"09035c87-81aa-4e21-b586-281eec2ac195\",\"type\":\"WheelZoomTool\"},\"active_tap\":\"auto\",\"tools\":[{\"id\":\"ca8bf926-99b6-41f8-aa58-31f717609a35\",\"type\":\"PanTool\"},{\"id\":\"0170b055-6020-406e-b0a6-9a42a9bb2816\",\"type\":\"BoxZoomTool\"},{\"id\":\"09035c87-81aa-4e21-b586-281eec2ac195\",\"type\":\"WheelZoomTool\"},{\"id\":\"e4e04654-061b-4633-a71e-1c9003f6bda9\",\"type\":\"WheelZoomTool\"},{\"id\":\"7d459b4a-fdec-48b5-bc95-828fb94c3c34\",\"type\":\"SaveTool\"},{\"id\":\"cacbfd1a-7e88-471a-a3c1-bc588c6af09c\",\"type\":\"ResetTool\"},{\"id\":\"7a2caa64-375a-466a-8caf-c9f17555b42d\",\"type\":\"HoverTool\"}]},\"id\":\"58f26099-07c1-4f46-ba5f-eec6d745dd12\",\"type\":\"Toolbar\"},{\"attributes\":{\"data_source\":{\"id\":\"53346d3c-7b75-4689-95d1-c395b23fa5b8\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"dd1456df-a3c2-4dcc-a8aa-769ac1a8a3b2\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"name\":\"vertices\",\"nonselection_glyph\":{\"id\":\"43ae4eb3-d229-4335-a758-a5d18149bd65\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"2e9dcf01-3f79-4337-b7dd-26f0525180b9\",\"type\":\"CDSView\"}},\"id\":\"eb321d59-0d82-41f4-966f-a84597d90781\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"source\":{\"id\":\"49a8987d-d307-4797-9fae-8ec771f76b48\",\"type\":\"ColumnDataSource\"}},\"id\":\"b0f21dbb-aa92-495c-9a38-9881564a3daa\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"a3e71058-805b-4461-9009-2e76107d0dad\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"eabb5e31-0740-4852-bcac-887155d4c0fc\",\"type\":\"LinearScale\"},{\"attributes\":{\"source\":{\"id\":\"49a8987d-d307-4797-9fae-8ec771f76b48\",\"type\":\"ColumnDataSource\"}},\"id\":\"ddf587aa-019f-43a1-af8c-52144e732785\",\"type\":\"CDSView\"},{\"attributes\":{\"axis_label\":\"decoding step (aka output length)\",\"formatter\":{\"id\":\"f3857a7d-2e4f-4ce0-b897-cec5490c8436\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"327205fd-12df-449f-9614-e6816136cb23\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"859d93b7-acac-4853-be74-6f20da679d8b\",\"type\":\"SingleIntervalTicker\"}},\"id\":\"84db9569-9f28-4a47-82c8-bfa01402aeb6\",\"type\":\"LinearAxis\"},{\"attributes\":{\"text\":{\"field\":\"token_text\"},\"text_align\":\"center\",\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"field\":\"token_font_size\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"},\"y_offset\":{\"value\":-10}},\"id\":\"d4854422-d3bc-43fa-b7c0-a791e40b8a05\",\"type\":\"Text\"},{\"attributes\":{},\"id\":\"ca8bf926-99b6-41f8-aa58-31f717609a35\",\"type\":\"PanTool\"},{\"attributes\":{\"source\":{\"id\":\"53346d3c-7b75-4689-95d1-c395b23fa5b8\",\"type\":\"ColumnDataSource\"}},\"id\":\"a931ac6d-24c6-489e-873f-3785a4f5cec4\",\"type\":\"CDSView\"},{\"attributes\":{\"bounds\":[-11.5,15.333333333333332],\"callback\":null,\"range_padding\":1.0,\"range_padding_units\":\"absolute\"},\"id\":\"b765029c-d346-49e8-b769-3c8155e12984\",\"type\":\"DataRange1d\"},{\"attributes\":{\"interval\":1},\"id\":\"859d93b7-acac-4853-be74-6f20da679d8b\",\"type\":\"SingleIntervalTicker\"},{\"attributes\":{\"text\":{\"field\":\"hypo_i_text\"},\"text_align\":\"center\",\"text_baseline\":\"middle\",\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"value\":\"12px\"},\"text_font_style\":\"bold\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"323b0eea-1d7f-4a0d-84fe-74a9d6ac6b56\",\"type\":\"Text\"},{\"attributes\":{\"below\":[{\"id\":\"13a5308f-70ad-452e-af22-43861fd0cb71\",\"type\":\"LinearAxis\"}],\"plot_width\":900,\"renderers\":[{\"id\":\"59657239-6604-4439-8bf1-83a5ab659d4a\",\"type\":\"BoxAnnotation\"},{\"id\":\"13a5308f-70ad-452e-af22-43861fd0cb71\",\"type\":\"LinearAxis\"},{\"id\":\"004b26cb-f235-4bd3-9464-2eed25a63945\",\"type\":\"Grid\"},{\"id\":\"0657e055-fa58-43fe-a3b8-b2b719174d8a\",\"type\":\"GlyphRenderer\"},{\"id\":\"eb321d59-0d82-41f4-966f-a84597d90781\",\"type\":\"GlyphRenderer\"},{\"id\":\"6ac35546-2a42-40c8-aa62-146d57d2f556\",\"type\":\"GlyphRenderer\"},{\"id\":\"5a71f9c4-4314-4eda-a1d8-25124c2b23d2\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"f04dd0c6-c641-4580-87f7-c0f878d1c0e6\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"3175d402-0679-462f-b9d4-d947ff7ad2b3\",\"type\":\"Toolbar\"},\"toolbar_location\":\"above\",\"x_range\":{\"id\":\"a5cebe80-c44b-4d0a-aae2-c2f3b9ef2475\",\"type\":\"Range1d\"},\"x_scale\":{\"id\":\"d8a43b51-3e35-4c6f-bb08-c7be8e8c3ce2\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"b765029c-d346-49e8-b769-3c8155e12984\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"a0929048-0a2d-4f8f-8d58-e4a6d0106ad1\",\"type\":\"LinearScale\"}},\"id\":\"91387928-8f01-4237-9a5d-24f1d6f93c23\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"8033ebc0-a196-49a8-acdc-dbc0ae3fcf54\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"data_source\":{\"id\":\"53346d3c-7b75-4689-95d1-c395b23fa5b8\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"d4854422-d3bc-43fa-b7c0-a791e40b8a05\",\"type\":\"Text\"},\"hover_glyph\":null,\"muted_glyph\":null,\"name\":\"tokens\",\"nonselection_glyph\":{\"id\":\"17a29716-8304-471c-9f48-6dd74ade100a\",\"type\":\"Text\"},\"selection_glyph\":null,\"view\":{\"id\":\"5d7f1257-5695-4ee0-8fe7-f29e498f83c7\",\"type\":\"CDSView\"}},\"id\":\"6ac35546-2a42-40c8-aa62-146d57d2f556\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"text\":{\"field\":\"hypo_i_text\"},\"text_align\":\"center\",\"text_alpha\":{\"value\":0.1},\"text_baseline\":\"middle\",\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"value\":\"12px\"},\"text_font_style\":\"bold\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"a10f3b4a-982e-4c44-b026-f8088199fe6f\",\"type\":\"Text\"},{\"attributes\":{\"plot\":{\"id\":\"327205fd-12df-449f-9614-e6816136cb23\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"21b07e0f-be09-4617-8f50-6b55adf59566\",\"type\":\"SingleIntervalTicker\"}},\"id\":\"d3dfd7db-0295-4068-89b6-6a86f9141772\",\"type\":\"Grid\"},{\"attributes\":{\"plot\":null,\"text\":\"Beam search\"},\"id\":\"f04dd0c6-c641-4580-87f7-c0f878d1c0e6\",\"type\":\"Title\"},{\"attributes\":{\"data_source\":{\"id\":\"49a8987d-d307-4797-9fae-8ec771f76b48\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"323b0eea-1d7f-4a0d-84fe-74a9d6ac6b56\",\"type\":\"Text\"},\"hover_glyph\":null,\"muted_glyph\":null,\"name\":\"hypo_i\",\"nonselection_glyph\":{\"id\":\"a10f3b4a-982e-4c44-b026-f8088199fe6f\",\"type\":\"Text\"},\"selection_glyph\":null,\"view\":{\"id\":\"b0f21dbb-aa92-495c-9a38-9881564a3daa\",\"type\":\"CDSView\"}},\"id\":\"d6b0d875-955c-493f-8f95-7f155de46db0\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"source\":{\"id\":\"53346d3c-7b75-4689-95d1-c395b23fa5b8\",\"type\":\"ColumnDataSource\"}},\"id\":\"5d7f1257-5695-4ee0-8fe7-f29e498f83c7\",\"type\":\"CDSView\"},{\"attributes\":{\"overlay\":{\"id\":\"8033ebc0-a196-49a8-acdc-dbc0ae3fcf54\",\"type\":\"BoxAnnotation\"}},\"id\":\"0170b055-6020-406e-b0a6-9a42a9bb2816\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"bounds\":[-10.0,20.0],\"callback\":null,\"end\":12,\"js_property_callbacks\":{\"change:end\":[{\"id\":\"b56f32a3-d365-46b7-9f4f-323382959b2d\",\"type\":\"CustomJS\"}]},\"start\":-1},\"id\":\"a5cebe80-c44b-4d0a-aae2-c2f3b9ef2475\",\"type\":\"Range1d\"},{\"attributes\":{\"dimensions\":\"width\"},\"id\":\"09035c87-81aa-4e21-b586-281eec2ac195\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_scroll\":{\"id\":\"fbfb1ccf-b06f-43d0-93b8-b7a2feb2e51e\",\"type\":\"WheelZoomTool\"},\"active_tap\":\"auto\",\"tools\":[{\"id\":\"cab35414-b6a5-47d8-a8cd-55b26f7a7b54\",\"type\":\"PanTool\"},{\"id\":\"64b6f55d-2417-4701-8845-e39dd839273e\",\"type\":\"BoxZoomTool\"},{\"id\":\"fbfb1ccf-b06f-43d0-93b8-b7a2feb2e51e\",\"type\":\"WheelZoomTool\"},{\"id\":\"7a97466a-a0c3-4c9a-93bc-f7d1909c09da\",\"type\":\"WheelZoomTool\"},{\"id\":\"a8cad25e-8062-4326-b4df-8b1d1db279c7\",\"type\":\"SaveTool\"},{\"id\":\"237d0e16-242f-45d0-81da-35c5b34ecdb4\",\"type\":\"ResetTool\"},{\"id\":\"ce962e54-6eaa-4a63-b602-ab2387d43716\",\"type\":\"HoverTool\"}]},\"id\":\"3175d402-0679-462f-b9d4-d947ff7ad2b3\",\"type\":\"Toolbar\"},{\"attributes\":{\"dimensions\":\"height\"},\"id\":\"e4e04654-061b-4633-a71e-1c9003f6bda9\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"text\":{\"field\":\"hypo_i_text\"},\"text_align\":\"center\",\"text_baseline\":\"middle\",\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"value\":\"12px\"},\"text_font_style\":\"bold\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"8c80ac74-34fc-4790-a100-c62b151ded39\",\"type\":\"Text\"},{\"attributes\":{},\"id\":\"a0929048-0a2d-4f8f-8d58-e4a6d0106ad1\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"7d459b4a-fdec-48b5-bc95-828fb94c3c34\",\"type\":\"SaveTool\"},{\"attributes\":{\"text\":{\"field\":\"hypo_i_text\"},\"text_align\":\"center\",\"text_alpha\":{\"value\":0.1},\"text_baseline\":\"middle\",\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"value\":\"12px\"},\"text_font_style\":\"bold\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1aaad8cb-1c94-4061-b4ce-f70e91e33b07\",\"type\":\"Text\"},{\"attributes\":{},\"id\":\"d8a43b51-3e35-4c6f-bb08-c7be8e8c3ce2\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"cacbfd1a-7e88-471a-a3c1-bc588c6af09c\",\"type\":\"ResetTool\"},{\"attributes\":{\"data_source\":{\"id\":\"53346d3c-7b75-4689-95d1-c395b23fa5b8\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"8c80ac74-34fc-4790-a100-c62b151ded39\",\"type\":\"Text\"},\"hover_glyph\":null,\"muted_glyph\":null,\"name\":\"hypo_i\",\"nonselection_glyph\":{\"id\":\"1aaad8cb-1c94-4061-b4ce-f70e91e33b07\",\"type\":\"Text\"},\"selection_glyph\":null,\"view\":{\"id\":\"a931ac6d-24c6-489e-873f-3785a4f5cec4\",\"type\":\"CDSView\"}},\"id\":\"5a71f9c4-4314-4eda-a1d8-25124c2b23d2\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"callback\":null,\"names\":[\"vertices\"],\"tooltips\":[[\"token\",\"@_on_hover_token\"],[\"token_id\",\"@_on_hover_token_id\"],[\"score\",\"@_on_hover_score\"]]},\"id\":\"7a2caa64-375a-466a-8caf-c9f17555b42d\",\"type\":\"HoverTool\"},{\"attributes\":{\"interval\":1},\"id\":\"21b07e0f-be09-4617-8f50-6b55adf59566\",\"type\":\"SingleIntervalTicker\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"59657239-6604-4439-8bf1-83a5ab659d4a\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"axis_label\":\"decoding step (aka output length)\",\"formatter\":{\"id\":\"e26ba807-3a3a-43be-8db2-601d280799bd\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"91387928-8f01-4237-9a5d-24f1d6f93c23\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"c0496269-9e06-46af-ab94-5438bfbff629\",\"type\":\"SingleIntervalTicker\"}},\"id\":\"13a5308f-70ad-452e-af22-43861fd0cb71\",\"type\":\"LinearAxis\"},{\"attributes\":{\"interval\":1},\"id\":\"c0496269-9e06-46af-ab94-5438bfbff629\",\"type\":\"SingleIntervalTicker\"},{\"attributes\":{\"args\":{\"source\":{\"id\":\"49a8987d-d307-4797-9fae-8ec771f76b48\",\"type\":\"ColumnDataSource\"}},\"code\":\"\\n            var x_range = cb_obj;\\n            var font_size = Math.round(14 * 13.0 / (x_range.end - x_range.start));\\n\\n            font_size = Math.min(24, Math.max(font_size, 0));\\n            \\n            var data = source.data;\\n            var fs = data['token_font_size']\\n            \\n            for (var i = 0; i < fs.length; i++)\\n                fs[i] = font_size.toString() + \\\"px\\\";\\n            \\n            source.change.emit();\\n        \"},\"id\":\"eab13fb4-9405-4d14-8bf6-46c6f698b4bb\",\"type\":\"CustomJS\"},{\"attributes\":{},\"id\":\"cab35414-b6a5-47d8-a8cd-55b26f7a7b54\",\"type\":\"PanTool\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"line_width\":{\"field\":\"line_width\"},\"size\":{\"units\":\"screen\",\"value\":24},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"d47ea423-d422-49a0-b24d-798a01f47236\",\"type\":\"Circle\"},{\"attributes\":{\"line_color\":{\"value\":\"#1f77b4\"},\"line_width\":{\"field\":\"line_width\"},\"xs\":{\"field\":\"edge_xx\"},\"ys\":{\"field\":\"edge_yy\"}},\"id\":\"d4677ed5-e7ac-4a9f-960d-0463a72b4947\",\"type\":\"MultiLine\"},{\"attributes\":{\"callback\":null,\"names\":[\"vertices\"],\"tooltips\":[[\"token\",\"@_on_hover_token\"],[\"token_id\",\"@_on_hover_token_id\"],[\"score\",\"@_on_hover_score\"]]},\"id\":\"ce962e54-6eaa-4a63-b602-ab2387d43716\",\"type\":\"HoverTool\"},{\"attributes\":{\"overlay\":{\"id\":\"59657239-6604-4439-8bf1-83a5ab659d4a\",\"type\":\"BoxAnnotation\"}},\"id\":\"64b6f55d-2417-4701-8845-e39dd839273e\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"line_width\":{\"field\":\"line_width\"},\"xs\":{\"field\":\"edge_xx\"},\"ys\":{\"field\":\"edge_yy\"}},\"id\":\"72b704fc-ca92-4d86-89ae-f33aeb3c23a8\",\"type\":\"MultiLine\"},{\"attributes\":{\"dimensions\":\"width\"},\"id\":\"fbfb1ccf-b06f-43d0-93b8-b7a2feb2e51e\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"line_color\":{\"value\":\"#1f77b4\"},\"line_width\":{\"field\":\"line_width\"},\"xs\":{\"field\":\"edge_xx\"},\"ys\":{\"field\":\"edge_yy\"}},\"id\":\"5574258e-e46b-409c-af99-02931bf42c33\",\"type\":\"MultiLine\"},{\"attributes\":{\"fill_color\":{\"field\":\"circle_fill_color\"},\"line_color\":{\"field\":\"line_color\"},\"line_width\":{\"field\":\"line_width\"},\"size\":{\"units\":\"screen\",\"value\":24},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"dd1456df-a3c2-4dcc-a8aa-769ac1a8a3b2\",\"type\":\"Circle\"},{\"attributes\":{\"dimensions\":\"height\"},\"id\":\"7a97466a-a0c3-4c9a-93bc-f7d1909c09da\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"fill_color\":{\"field\":\"circle_fill_color\"},\"line_color\":{\"field\":\"line_color\"},\"line_width\":{\"field\":\"line_width\"},\"size\":{\"units\":\"screen\",\"value\":24},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"14c2f957-8e96-40e9-8d5a-bbe84ee0131a\",\"type\":\"Circle\"},{\"attributes\":{\"args\":{\"source\":{\"id\":\"53346d3c-7b75-4689-95d1-c395b23fa5b8\",\"type\":\"ColumnDataSource\"}},\"code\":\"\\n            var x_range = cb_obj;\\n            var font_size = Math.round(14 * 13.0 / (x_range.end - x_range.start));\\n\\n            font_size = Math.min(24, Math.max(font_size, 0));\\n            \\n            var data = source.data;\\n            var fs = data['token_font_size']\\n            \\n            for (var i = 0; i < fs.length; i++)\\n                fs[i] = font_size.toString() + \\\"px\\\";\\n            \\n            source.change.emit();\\n        \"},\"id\":\"b56f32a3-d365-46b7-9f4f-323382959b2d\",\"type\":\"CustomJS\"},{\"attributes\":{},\"id\":\"a8cad25e-8062-4326-b4df-8b1d1db279c7\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"237d0e16-242f-45d0-81da-35c5b34ecdb4\",\"type\":\"ResetTool\"},{\"attributes\":{\"data_source\":{\"id\":\"49a8987d-d307-4797-9fae-8ec771f76b48\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"5574258e-e46b-409c-af99-02931bf42c33\",\"type\":\"MultiLine\"},\"hover_glyph\":null,\"muted_glyph\":null,\"name\":\"edges\",\"nonselection_glyph\":{\"id\":\"72b704fc-ca92-4d86-89ae-f33aeb3c23a8\",\"type\":\"MultiLine\"},\"selection_glyph\":null,\"view\":{\"id\":\"95e7b794-7dda-4997-b7c6-a664964f6b5a\",\"type\":\"CDSView\"}},\"id\":\"978e683c-44ae-4bb3-b7d1-4660f2415803\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"interval\":1},\"id\":\"aaf5ec9a-6f51-45f4-a1b5-d2a8ac2cb5e0\",\"type\":\"SingleIntervalTicker\"},{\"attributes\":{\"data_source\":{\"id\":\"49a8987d-d307-4797-9fae-8ec771f76b48\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"14c2f957-8e96-40e9-8d5a-bbe84ee0131a\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"name\":\"vertices\",\"nonselection_glyph\":{\"id\":\"d47ea423-d422-49a0-b24d-798a01f47236\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"dd940c41-87e6-4094-8be6-3dc7f22f0921\",\"type\":\"CDSView\"}},\"id\":\"cdf3b0e4-fe4f-450f-86d4-c51260c5d8c0\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"plot\":{\"id\":\"91387928-8f01-4237-9a5d-24f1d6f93c23\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"aaf5ec9a-6f51-45f4-a1b5-d2a8ac2cb5e0\",\"type\":\"SingleIntervalTicker\"}},\"id\":\"004b26cb-f235-4bd3-9464-2eed25a63945\",\"type\":\"Grid\"},{\"attributes\":{\"source\":{\"id\":\"53346d3c-7b75-4689-95d1-c395b23fa5b8\",\"type\":\"ColumnDataSource\"}},\"id\":\"b553c853-cc22-4ae3-aea6-95ad360ad0fc\",\"type\":\"CDSView\"},{\"attributes\":{\"source\":{\"id\":\"49a8987d-d307-4797-9fae-8ec771f76b48\",\"type\":\"ColumnDataSource\"}},\"id\":\"95e7b794-7dda-4997-b7c6-a664964f6b5a\",\"type\":\"CDSView\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"id\",\"parent_id\",\"children_ids\",\"is_best\",\"depth\",\"hypo_i\",\"token\",\"token_id\",\"x\",\"y\",\"circle_fill_color\",\"line_color\",\"line_width\",\"edge_xx\",\"edge_yy\",\"token_text\",\"token_font_size\",\"hypo_i_text\",\"hypo_i_offset\",\"_on_hover_token\",\"_on_hover_token_id\",\"_on_hover_score\"],\"data\":{\"_on_hover_score\":[\"-4.7282\",\"-4.2071\",\"-4.8782\",\"-1.3577\",\"-4.0420\",\"-4.2097\",\"-4.5624\",\"-3.2410\",\"-6.9225\",\"-4.6717\",\"-2.8497\",\"-3.8023\",\"-4.1460\",\"-3.6443\",\"-4.3135\",\"-4.7070\",\"-3.1373\",\"-4.5105\",\"-6.1526\",\"-3.0974\",\"-3.7421\",\"-3.4956\",\"0.0000\",\"-5.2694\",\"-4.4104\",\"-6.1752\",\"-3.7617\",\"-4.6281\",\"-6.5626\",\"-3.7175\",\"-0.7384\",\"-4.5439\",\"-5.2967\",\"-3.6831\",\"-5.2153\",\"-3.7830\",\"-3.1335\",\"-0.8716\",\"-3.7312\",\"-3.8895\",\"-6.5984\"],\"_on_hover_token\":[\"_EOS_\",\"\\u043f\\u0440\\u0430\\u0432\\u0438\\u0442\\u0435\\u043b\\u044c\\u0441\\u0442\\u0432\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0439\",\",\",\"\\u0443\\u043f\\u043b\\u0430\\u0442\\u044b\",\"\\u043e\\u0442\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u0432\",\"_EOS_\",\"\\u0432\\u044b\\u0441\\u0442\\u0443\\u043f\\u0430\\u0435\\u0442\",\"_EOS_\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\\u0435\",\"\\u043d\\u0430\\u043b\\u043e\\u0433\\u043e\\u0432\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043a\\u0438\",\"\\u043d\\u0435\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\",\"<empty>\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\".\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u043d\\u0435\",\"\\u043e\\u0442\\u0432\\u0435\\u0442\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u0438\",\"\\u0437\\u0430\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\",\",\",\"\\u043e\\u0442\",\"`\\u0430\\u044e\\u0449\\u0438\\u0435\",\"_EOS_\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0430\\u043b\",\"\\u043d\\u0435\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0435\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u043f\\u0440\\u0435\\u0442\\u0435\\u043d\\u0437\\u0438\\u0439\"],\"_on_hover_token_id\":[1,780,25,2482,15356,2482,1879,3,17084,25,13862,5,1,7691,1,15356,25,15328,4255,11685,11,3487,-1,21342,15356,4,21342,11,1781,30,3622,3,25,1078,1,5093,26478,11,2451,13862,22624],\"children_ids\":[[],[[2,3]],[[10,3],[10,2],[10,1],[10,0]],[[4,3],[4,1],[4,0]],[[9,3],[9,1],[9,0]],[],[],[[6,1]],[],[[8,2]],[[5,0]],[[2,2]],[],[[4,2]],[],[[7,2]],[[6,2],[6,0]],[],[],[[5,1]],[[2,1]],[],[[1,3],[1,2],[1,1],[1,0]],[[9,2]],[[8,1]],[],[[7,3],[7,1]],[],[],[[5,2]],[[2,0]],[],[],[[8,3],[8,0]],[],[[7,0]],[],[[3,3],[3,2],[3,1],[3,0]],[[5,3]],[[6,3]],[]],\"circle_fill_color\":[\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\"],\"depth\":[7,1,9,3,8,2,6,5,10,7,4,1,9,3,8,6,5,2,10,4,1,3,0,8,7,9,6,2,10,4,1,5,8,7,9,6,3,2,4,5,10],\"edge_xx\":[[6.0,7.0],[0.0,1.0],[8.0,9.0],[2.0,3.0],[7.0,8.0],[1.0,2.0],[5.0,6.0],[4.0,5.0],[9.0,10.0],[6.0,7.0],[3.0,4.0],[0.0,1.0],[8.0,9.0],[2.0,3.0],[7.0,8.0],[5.0,6.0],[4.0,5.0],[1.0,2.0],[9.0,10.0],[3.0,4.0],[0.0,1.0],[2.0,3.0],[0.0,0.0],[7.0,8.0],[6.0,7.0],[8.0,9.0],[5.0,6.0],[1.0,2.0],[9.0,10.0],[3.0,4.0],[0.0,1.0],[4.0,5.0],[7.0,8.0],[6.0,7.0],[8.0,9.0],[5.0,6.0],[2.0,3.0],[1.0,2.0],[3.0,4.0],[4.0,5.0],[9.0,10.0]],\"edge_yy\":[[4.833333333333333,4.333333333333333],[0.0,-1.5],[3.333333333333333,3.333333333333333],[1.5,3.0],[2.833333333333333,3.333333333333333],[0.5,0.5],[4.333333333333333,3.833333333333333],[2.833333333333333,2.833333333333333],[3.333333333333333,1.833333333333333],[0.0,0.0],[3.0,4.333333333333333],[0.0,-0.5],[3.333333333333333,4.333333333333333],[1.5,0.0],[5.333333333333333,5.333333333333333],[0.0,0.0],[4.333333333333333,4.333333333333333],[-0.5,-0.5],[3.333333333333333,4.833333333333333],[3.0,2.833333333333333],[0.0,0.5],[1.5,1.0],[0.0,0.0],[0.0,0.0],[4.833333333333333,5.333333333333333],[3.333333333333333,2.333333333333333],[4.333333333333333,4.833333333333333],[-1.5,-1.5],[3.333333333333333,3.833333333333333],[0.0,0.0],[0.0,1.5],[1.8333333333333333,1.8333333333333333],[2.833333333333333,2.333333333333333],[2.833333333333333,2.833333333333333],[0.0,0.0],[2.833333333333333,2.833333333333333],[1.5,2.0],[1.5,1.5],[3.0,1.8333333333333333],[0.0,0.0],[3.333333333333333,2.833333333333333]],\"hypo_i\":[3,3,1,0,0,1,2,1,3,2,0,2,0,3,1,3,0,2,0,1,1,2,0,2,1,3,0,3,1,2,0,3,3,0,2,1,1,0,3,2,2],\"hypo_i_offset\":[-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8],\"hypo_i_text\":[\"#3\",\"#3\",\"#1\",\"#0\",\"#0\",\"#1\",\"#2\",\"#1\",\"#3\",\"#2\",\"#0\",\"#2\",\"#0\",\"#3\",\"#1\",\"#3\",\"#0\",\"#2\",\"#0\",\"#1\",\"#1\",\"#2\",\"#0\",\"#2\",\"#1\",\"#3\",\"#0\",\"#3\",\"#1\",\"#2\",\"#0\",\"#3\",\"#3\",\"#0\",\"#2\",\"#1\",\"#1\",\"#0\",\"#3\",\"#2\",\"#2\"],\"id\":[[7,3],[1,3],[9,1],[3,0],[8,0],[2,1],[6,2],[5,1],[10,3],[7,2],[4,0],[1,2],[9,0],[3,3],[8,1],[6,3],[5,0],[2,2],[10,0],[4,1],[1,1],[3,2],[0,0],[8,2],[7,1],[9,3],[6,0],[2,3],[10,1],[4,2],[1,0],[5,3],[8,3],[7,0],[9,2],[6,1],[3,1],[2,0],[4,3],[5,2],[10,2]],\"is_best\":[false,false,false,true,true,false,false,true,false,false,false,false,true,false,false,false,false,false,false,true,false,false,true,false,false,false,false,false,false,false,true,false,false,true,false,true,false,true,false,false,false],\"line_color\":[\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\"],\"line_width\":[1,1,1,3,3,1,1,3,1,1,1,1,3,1,1,1,1,1,1,3,1,1,3,1,1,1,1,1,1,1,3,1,1,3,1,3,1,3,1,1,1],\"parent_id\":[[6,0],[0,0],[8,0],[2,0],[7,0],[1,1],[5,0],[4,1],[9,1],[6,3],[3,0],[0,0],[8,0],[2,0],[7,1],[5,2],[4,0],[1,2],[9,1],[3,0],[0,0],[2,0],[0,0],[7,2],[6,0],[8,0],[5,0],[1,3],[9,1],[3,3],[0,0],[4,3],[7,0],[6,1],[8,2],[5,1],[2,0],[1,0],[3,0],[4,2],[9,1]],\"token\":[\"_EOS_\",\"\\u043f\\u0440\\u0430\\u0432\\u0438\\u0442\\u0435\\u043b\\u044c\\u0441\\u0442\\u0432\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0439\",\",\",\"\\u0443\\u043f\\u043b\\u0430\\u0442\\u044b\",\"\\u043e\\u0442\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u0432\",\"_EOS_\",\"\\u0432\\u044b\\u0441\\u0442\\u0443\\u043f\\u0430\\u0435\\u0442\",\"_EOS_\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\\u0435\",\"\\u043d\\u0430\\u043b\\u043e\\u0433\\u043e\\u0432\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043a\\u0438\",\"\\u043d\\u0435\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\",\"<empty>\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\".\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u043d\\u0435\",\"\\u043e\\u0442\\u0432\\u0435\\u0442\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u0438\",\"\\u0437\\u0430\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\",\",\",\"\\u043e\\u0442\",\"`\\u0430\\u044e\\u0449\\u0438\\u0435\",\"_EOS_\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0430\\u043b\",\"\\u043d\\u0435\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0435\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u043f\\u0440\\u0435\\u0442\\u0435\\u043d\\u0437\\u0438\\u0439\"],\"token_font_size\":[\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\"],\"token_id\":[1,780,25,2482,15356,2482,1879,3,17084,25,13862,5,1,7691,1,15356,25,15328,4255,11685,11,3487,-1,21342,15356,4,21342,11,1781,30,3622,3,25,1078,1,5093,26478,11,2451,13862,22624],\"token_text\":[\"_EOS_\",\"\\u043f\\u0440\\u0430\\u0432\\u0438\\u0442\\u0435\\u043b\\u044c\\u0441\\u0442\\u0432\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0439\",\",\",\"\\u0443\\u043f\\u043b\\u0430\\u0442\\u044b\",\"\\u043e\\u0442\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u0432\",\"_EOS_\",\"\\u0432\\u044b\\u0441\\u0442\\u0443\\u043f\\u0430\\u0435\\u0442\",\"_EOS_\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\\u0435\",\"\\u043d\\u0430\\u043b\\u043e\\u0433\\u043e\\u0432\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043a\\u0438\",\"\\u043d\\u0435\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\",\"<empty>\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\".\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u043d\\u0435\",\"\\u043e\\u0442\\u0432\\u0435\\u0442\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u0438\",\"\\u0437\\u0430\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\",\",\",\"\\u043e\\u0442\",\"`\\u0430\\u044e\\u0449\\u0438\\u0435\",\"_EOS_\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0430\\u043b\",\"\\u043d\\u0435\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0435\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u043f\\u0440\\u0435\\u0442\\u0435\\u043d\\u0437\\u0438\\u0439\"],\"x\":[7.0,1.0,9.0,3.0,8.0,2.0,6.0,5.0,10.0,7.0,4.0,1.0,9.0,3.0,8.0,6.0,5.0,2.0,10.0,4.0,1.0,3.0,0.0,8.0,7.0,9.0,6.0,2.0,10.0,4.0,1.0,5.0,8.0,7.0,9.0,6.0,3.0,2.0,4.0,5.0,10.0],\"y\":[4.333333333333333,-1.5,3.333333333333333,3.0,3.333333333333333,0.5,3.833333333333333,2.833333333333333,1.833333333333333,0.0,4.333333333333333,-0.5,4.333333333333333,0.0,5.333333333333333,0.0,4.333333333333333,-0.5,4.833333333333333,2.833333333333333,0.5,1.0,0.0,0.0,5.333333333333333,2.333333333333333,4.833333333333333,-1.5,3.833333333333333,0.0,1.5,1.8333333333333333,2.333333333333333,2.833333333333333,0.0,2.833333333333333,2.0,1.5,1.8333333333333333,0.0,2.833333333333333]}},\"id\":\"49a8987d-d307-4797-9fae-8ec771f76b48\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"e26ba807-3a3a-43be-8db2-601d280799bd\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"line_width\":{\"field\":\"line_width\"},\"xs\":{\"field\":\"edge_xx\"},\"ys\":{\"field\":\"edge_yy\"}},\"id\":\"d41b7979-8903-4ee5-a861-1bf88bfb14ea\",\"type\":\"MultiLine\"},{\"attributes\":{\"source\":{\"id\":\"49a8987d-d307-4797-9fae-8ec771f76b48\",\"type\":\"ColumnDataSource\"}},\"id\":\"dd940c41-87e6-4094-8be6-3dc7f22f0921\",\"type\":\"CDSView\"},{\"attributes\":{\"text\":{\"field\":\"token_text\"},\"text_align\":\"center\",\"text_alpha\":{\"value\":0.1},\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"field\":\"token_font_size\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"},\"y_offset\":{\"value\":-10}},\"id\":\"17a29716-8304-471c-9f48-6dd74ade100a\",\"type\":\"Text\"},{\"attributes\":{},\"id\":\"f3857a7d-2e4f-4ce0-b897-cec5490c8436\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"data_source\":{\"id\":\"53346d3c-7b75-4689-95d1-c395b23fa5b8\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"d4677ed5-e7ac-4a9f-960d-0463a72b4947\",\"type\":\"MultiLine\"},\"hover_glyph\":null,\"muted_glyph\":null,\"name\":\"edges\",\"nonselection_glyph\":{\"id\":\"d41b7979-8903-4ee5-a861-1bf88bfb14ea\",\"type\":\"MultiLine\"},\"selection_glyph\":null,\"view\":{\"id\":\"b553c853-cc22-4ae3-aea6-95ad360ad0fc\",\"type\":\"CDSView\"}},\"id\":\"0657e055-fa58-43fe-a3b8-b2b719174d8a\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"text\":{\"field\":\"token_text\"},\"text_align\":\"center\",\"text_alpha\":{\"value\":0.1},\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"field\":\"token_font_size\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"},\"y_offset\":{\"value\":-10}},\"id\":\"3621fd59-3cb3-4305-8802-6f7f75a23fd6\",\"type\":\"Text\"},{\"attributes\":{\"bounds\":[-11.5,15.333333333333332],\"callback\":null,\"range_padding\":1.0,\"range_padding_units\":\"absolute\"},\"id\":\"3868f6e3-8193-418f-af89-ee5749e490a1\",\"type\":\"DataRange1d\"},{\"attributes\":{\"text\":{\"field\":\"token_text\"},\"text_align\":\"center\",\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"field\":\"token_font_size\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"},\"y_offset\":{\"value\":-10}},\"id\":\"08bbcb52-00b5-4803-9e7a-9252db09708d\",\"type\":\"Text\"},{\"attributes\":{\"below\":[{\"id\":\"84db9569-9f28-4a47-82c8-bfa01402aeb6\",\"type\":\"LinearAxis\"}],\"plot_width\":900,\"renderers\":[{\"id\":\"8033ebc0-a196-49a8-acdc-dbc0ae3fcf54\",\"type\":\"BoxAnnotation\"},{\"id\":\"84db9569-9f28-4a47-82c8-bfa01402aeb6\",\"type\":\"LinearAxis\"},{\"id\":\"d3dfd7db-0295-4068-89b6-6a86f9141772\",\"type\":\"Grid\"},{\"id\":\"978e683c-44ae-4bb3-b7d1-4660f2415803\",\"type\":\"GlyphRenderer\"},{\"id\":\"cdf3b0e4-fe4f-450f-86d4-c51260c5d8c0\",\"type\":\"GlyphRenderer\"},{\"id\":\"a2ab1853-0966-4519-874f-956f801f1c72\",\"type\":\"GlyphRenderer\"},{\"id\":\"d6b0d875-955c-493f-8f95-7f155de46db0\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"5af81591-5793-4721-a459-e0a4ca700855\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"58f26099-07c1-4f46-ba5f-eec6d745dd12\",\"type\":\"Toolbar\"},\"toolbar_location\":\"above\",\"x_range\":{\"id\":\"14e9976c-9458-4bce-be96-da2f3c304cec\",\"type\":\"Range1d\"},\"x_scale\":{\"id\":\"eabb5e31-0740-4852-bcac-887155d4c0fc\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"3868f6e3-8193-418f-af89-ee5749e490a1\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"a3e71058-805b-4461-9009-2e76107d0dad\",\"type\":\"LinearScale\"}},\"id\":\"327205fd-12df-449f-9614-e6816136cb23\",\"subtype\":\"Figure\",\"type\":\"Plot\"}],\"root_ids\":[\"327205fd-12df-449f-9614-e6816136cb23\",\"91387928-8f01-4237-9a5d-24f1d6f93c23\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.7\"}};\n",
              "              var render_items = [{\"docid\":\"ba84f797-d201-498d-a731-5adafa5447b7\",\"elementid\":\"ff8c3f31-952d-4c2f-8b58-13e7cec51b58\",\"modelid\":\"91387928-8f01-4237-9a5d-24f1d6f93c23\"}];\n",
              "              \n",
              "              Bokeh.embed.embed_items(docs_json, render_items);\n",
              "            });\n",
              "          };\n",
              "          if (document.readyState != \"loading\") fn();\n",
              "          else document.addEventListener(\"DOMContentLoaded\", fn);\n",
              "        })();\n",
              "        \n",
              "        </script>\n",
              "    </body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ],
      "source": [
        "from IPython.display import HTML\n",
        "# Here's what it looks like:\n",
        "!wget -q https://raw.githubusercontent.com/yandexdataschool/nlp_course/2020/resources/beam_search.html\n",
        "HTML(\"beam_search.html\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "PgYYCrN9IoU-"
      },
      "outputs": [],
      "source": [
        "def generate_beamsearch(model, prefix=BOS, nucleus=0.9, beam_size=3, max_len=10):\n",
        "    \"\"\"\n",
        "    Generate a sequence with nucleous sampling\n",
        "    :param prefix: a string containing space-separated previous tokens\n",
        "    :param nucleus: N from the formulae above, N \\in [0, 1]\n",
        "    :param beam_size: numebr of branches to keep\n",
        "    :param max_len: generate sequences with at most this many tokens, NOT INCLUDING PREFIX\n",
        "    :returns: beam_size most likely candidates\n",
        "    :note: make sure that nucleous always contains at least one word, even if p(w*) > nucleus\n",
        "    \"\"\"\n",
        "\n",
        "    beam_probs = np.zeros(shape=(beam_size,)) # to accumulate log-probs for each beam \n",
        "    outputs = [prefix] # [_BOS_] or [prefix] at the beginning\n",
        "    for _ in range(max_len):\n",
        "        next_beams = []\n",
        "        for i in range(len(outputs)):\n",
        "            # generate 3 samples for each branch\n",
        "            prefix_i = outputs[i]\n",
        "\n",
        "            token_probs = model.get_possible_next_tokens(prefix_i)\n",
        "            tokens, probs = zip(*token_probs.items())\n",
        "            probs = np.array(probs)\n",
        "\n",
        "            sorted_probs_indices = np.argsort(probs)[::-1]\n",
        "            sorted_probs = probs[sorted_probs_indices]\n",
        "            # choose only N % highest probs, discard rest\n",
        "        \n",
        "            cumulative_probs = np.cumsum(sorted_probs)\n",
        "\n",
        "            remove_sorted_indices = cumulative_probs >= nucleus\n",
        "            # add one extra word to make sure \n",
        "            # we cover at least one word even if p(w*) > nucleus\n",
        "            remove_sorted_indices[..., 1:] = np.copy(remove_sorted_indices[..., :-1]) # last True -> False\n",
        "            remove_sorted_indices[..., 0] = 0 # if all of them were True, make at least first one word to False \n",
        "\n",
        "            indices_to_remove = sorted_probs_indices[remove_sorted_indices]\n",
        "            probs[indices_to_remove] = -float('Inf')\n",
        "\n",
        "            # compute softmax over changed array of probabilities\n",
        "            probs = np.exp(probs)\n",
        "            probs /= np.sum(probs)\n",
        "\n",
        "            # choose beam_size = 3 tokens with max probabilities\n",
        "            if outputs[i][-1] == EOS:\n",
        "                next_beams.append([outputs[i], beam_probs[i], i])\n",
        "            else:\n",
        "                beam_size_best_idx = sorted_probs_indices[:beam_size]\n",
        "                for j in range(beam_size):          \n",
        "                    next_beams.append([outputs[i] + tokens[beam_size_best_idx[j]], \n",
        "                                       beam_probs[i] + np.log(probs[beam_size_best_idx[j]]), \n",
        "                                       i])       \n",
        "                              \n",
        "        #print('next_beams = ', next_beams) \n",
        "\n",
        "        # now we sort next_beams to prune branches with low probabilities (keep only beam_size candidates)\n",
        "        outputs = [None] * beam_size\n",
        "        next_beams.sort(key=lambda x: x[1], reverse=True)\n",
        "        #print('next_beams sorted = ', next_beams)\n",
        "\n",
        "        for i in range(beam_size):\n",
        "            outputs[i], beam_probs[i], beam_idx = next_beams[i]\n",
        "            #states[j][0][i] = states_history[beam_idx][0][i]\n",
        "        #print('outputs = \\n', outputs)\n",
        "    \n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# network / Network / system / systems\n",
        "generate_beamsearch(model, 'Natural ', beam_size=10, nucleus=0.5, max_len=7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAwIpYdMR8pZ",
        "outputId": "1f74a944-73d4-45bb-9137-8ea0d1464ea1"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: RuntimeWarning: divide by zero encountered in log\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural network',\n",
              " 'Natural Network',\n",
              " 'Natural system ',\n",
              " 'Natural systems',\n",
              " 'Natural from th',\n",
              " 'Natural decogni',\n",
              " 'Natural from a ',\n",
              " 'Natural describ',\n",
              " 'Natural from de',\n",
              " 'Natural from in']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Propose / prove / proves\n",
        "generate_beamsearch(model, 'NLP ', beam_size=10, nucleus=0.5, max_len=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tDdoHNMUFpK",
        "outputId": "0434ec4e-2b1a-4e9b-e639-0b46c154a76c"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: RuntimeWarning: divide by zero encountered in log\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP Learning',\n",
              " 'NLP Methods ',\n",
              " 'NLP Propose ',\n",
              " 'NLP Problem ',\n",
              " 'NLP efficien',\n",
              " 'NLP efficati',\n",
              " 'NLP existing',\n",
              " 'NLP effectio',\n",
              " 'NLP effectiv',\n",
              " 'NLP Method a']"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nucleus = 0.65 (diverse outputs)\n",
        "# network / problem / variati(on) / variabl(e) / option / systems / varient\n",
        "generate_beamsearch(model, 'Deep Neural Networks (DNNs) are powerful ', beam_size=20, nucleus=0.65, max_len=7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pz5H6GXtUZSb",
        "outputId": "15484ce8-8030-4348-a5d7-5a67d71afe7e"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: RuntimeWarning: divide by zero encountered in log\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Deep Neural Networks (DNNs) are powerful network',\n",
              " 'Deep Neural Networks (DNNs) are powerful problem',\n",
              " 'Deep Neural Networks (DNNs) are powerful vatwork',\n",
              " 'Deep Neural Networks (DNNs) are powerful by the ',\n",
              " 'Deep Neural Networks (DNNs) are powerful variati',\n",
              " 'Deep Neural Networks (DNNs) are powerful variabl',\n",
              " 'Deep Neural Networks (DNNs) are powerful varial ',\n",
              " 'Deep Neural Networks (DNNs) are powerful propose',\n",
              " 'Deep Neural Networks (DNNs) are powerful varies ',\n",
              " 'Deep Neural Networks (DNNs) are powerful vasion ',\n",
              " 'Deep Neural Networks (DNNs) are powerful process',\n",
              " 'Deep Neural Networks (DNNs) are powerful vasiona',\n",
              " 'Deep Neural Networks (DNNs) are powerful proposs',\n",
              " 'Deep Neural Networks (DNNs) are powerful systati',\n",
              " 'Deep Neural Networks (DNNs) are powerful system ',\n",
              " 'Deep Neural Networks (DNNs) are powerful option ',\n",
              " 'Deep Neural Networks (DNNs) are powerful systems',\n",
              " 'Deep Neural Networks (DNNs) are powerful computa',\n",
              " 'Deep Neural Networks (DNNs) are powerful varient',\n",
              " 'Deep Neural Networks (DNNs) are powerful vature ']"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nucleus = 0.9 (almost no variety, similar outputs) \n",
        "# network / problem / variati(on) / variabl(e) / systems / varient\n",
        "generate_beamsearch(model, 'Deep Neural Networks (DNNs) are powerful ', beam_size=20, nucleus=0.9, max_len=7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frMsa-_SWsUz",
        "outputId": "c3a47682-11b1-44d7-9e28-b61f54579d11"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: RuntimeWarning: divide by zero encountered in log\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Deep Neural Networks (DNNs) are powerful Image, ',\n",
              " 'Deep Neural Networks (DNNs) are powerful network',\n",
              " 'Deep Neural Networks (DNNs) are powerful Network',\n",
              " 'Deep Neural Networks (DNNs) are powerful Images ',\n",
              " 'Deep Neural Networks (DNNs) are powerful Images.',\n",
              " 'Deep Neural Networks (DNNs) are powerful neural ',\n",
              " 'Deep Neural Networks (DNNs) are powerful Imagion',\n",
              " 'Deep Neural Networks (DNNs) are powerful natural',\n",
              " 'Deep Neural Networks (DNNs) are powerful Imagica',\n",
              " 'Deep Neural Networks (DNNs) are powerful Imaging',\n",
              " 'Deep Neural Networks (DNNs) are powerful Imagima',\n",
              " 'Deep Neural Networks (DNNs) are powerful nature ',\n",
              " 'Deep Neural Networks (DNNs) are powerful natures',\n",
              " 'Deep Neural Networks (DNNs) are powerful Imagive',\n",
              " 'Deep Neural Networks (DNNs) are powerful Imagina',\n",
              " 'Deep Neural Networks (DNNs) are powerful Imagivi',\n",
              " 'Deep Neural Networks (DNNs) are powerful Imagine',\n",
              " 'Deep Neural Networks (DNNs) are powerful dation ',\n",
              " 'Deep Neural Networks (DNNs) are powerful Imagini',\n",
              " 'Deep Neural Networks (DNNs) are powerful natured']"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "5fxmm931IoU_"
      },
      "outputs": [],
      "source": [
        "# check it out: which beam size works best?\n",
        "# find at least 5 prefixes where beam_size=1 and 8 generates different sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J35GFFXLIoVA"
      },
      "source": [
        "### Bonus quest II: Ultimate Language Model (2+ pts)\n",
        "\n",
        "So you've learned the building blocks of neural language models, you can now build the ultimate monster:  \n",
        "* Make it char-level, word level or maybe use sub-word units like [bpe](https://github.com/rsennrich/subword-nmt);\n",
        "* Combine convolutions, recurrent cells, pre-trained embeddings and all the black magic deep learning has to offer;\n",
        "  * Use strides to get larger window size quickly. Here's a [scheme](https://storage.googleapis.com/deepmind-live-cms/documents/BlogPost-Fig2-Anim-160908-r01.gif) from google wavenet.\n",
        "* Train on large data. Like... really large. Try [1 Billion Words](http://www.statmt.org/lm-benchmark/1-billion-word-language-modeling-benchmark-r13output.tar.gz) benchmark;\n",
        "* Use training schedules to speed up training. Start with small length and increase over time; Take a look at [one cycle](https://medium.com/@nachiket.tanksale/finding-good-learning-rate-and-the-one-cycle-policy-7159fe1db5d6) for learning rate;\n",
        "\n",
        "_You are NOT required to submit this assignment. Please make sure you don't miss your deadline because of it :)_"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py38",
      "language": "python",
      "name": "py38"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "J35GFFXLIoVA"
      ],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}