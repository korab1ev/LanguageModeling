{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHIDS-pkIoUT"
      },
      "source": [
        "### Homework: going neural (6 pts)\n",
        "\n",
        "We've checked out statistical approaches to language models in the last notebook. Now let's go find out what deep learning has to offer.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/expanding_mind_lm_kn_3.png' width=300px>\n",
        "\n",
        "We're gonna use the same dataset as before, except this time we build a language model that's character-level, not word level. Before you go:\n",
        "* If you haven't done seminar already, use `seminar.ipynb` to download the data.\n",
        "* This homework uses Pytorch v1.x: this is [how you install it](https://pytorch.org/get-started/locally/); and that's [how you use it](https://github.com/yandexdataschool/Practical_RL/tree/9f89e98d7df7ad47f5d6c85a70a38283e06be16a/week04_%5Brecap%5D_deep_learning)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EM_DT0x0IoUf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative manual download link: https://yadi.sk/d/_nGyU2IajjR9-w\n",
        "!wget \"https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\" -O arxivData.json.tar.gz\n",
        "!tar -xvzf arxivData.json.tar.gz\n",
        "data = pd.read_json(\"./arxivData.json\")\n",
        "data.sample(n=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "id": "N2AUQGW1JDB3",
        "outputId": "a92c80c2-9e57-4f47-b33c-4fac34d1b77f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-20 07:00:01--  https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/dl/99az9n1b57qkd9j/arxivData.json.tar.gz [following]\n",
            "--2022-09-20 07:00:01--  https://www.dropbox.com/s/dl/99az9n1b57qkd9j/arxivData.json.tar.gz\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uca79d834d9ab796d680bcea958f.dl.dropboxusercontent.com/cd/0/get/BtSsPyLfF7X-p4QMx9Don6QFayB99dUn8ViNQ_vRHP0ppqEo6XyprfeAwWgx2Fo4jt2NKMxordr3chBgzsyxo61xPxzyUWhgHWMMN-imGJ85UaC82-Dt8IDmNzvfjoIB6fibJPf-Mh9SsEw1GG_A6k-poaRjDio7pDoaBbwlOwAPDg/file?dl=1# [following]\n",
            "--2022-09-20 07:00:02--  https://uca79d834d9ab796d680bcea958f.dl.dropboxusercontent.com/cd/0/get/BtSsPyLfF7X-p4QMx9Don6QFayB99dUn8ViNQ_vRHP0ppqEo6XyprfeAwWgx2Fo4jt2NKMxordr3chBgzsyxo61xPxzyUWhgHWMMN-imGJ85UaC82-Dt8IDmNzvfjoIB6fibJPf-Mh9SsEw1GG_A6k-poaRjDio7pDoaBbwlOwAPDg/file?dl=1\n",
            "Resolving uca79d834d9ab796d680bcea958f.dl.dropboxusercontent.com (uca79d834d9ab796d680bcea958f.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to uca79d834d9ab796d680bcea958f.dl.dropboxusercontent.com (uca79d834d9ab796d680bcea958f.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18933283 (18M) [application/binary]\n",
            "Saving to: ‘arxivData.json.tar.gz’\n",
            "\n",
            "arxivData.json.tar. 100%[===================>]  18.06M  78.0MB/s    in 0.2s    \n",
            "\n",
            "2022-09-20 07:00:02 (78.0 MB/s) - ‘arxivData.json.tar.gz’ saved [18933283/18933283]\n",
            "\n",
            "arxivData.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  author  day            id  \\\n",
              "22452  [{'name': 'Didier Rullière'}, {'name': 'Nicola...   19  1607.05432v3   \n",
              "3723   [{'name': 'AmirEmad Ghassami'}, {'name': 'Sabe...    5  1802.01239v1   \n",
              "10597  [{'name': 'Nihar B. Shah'}, {'name': 'Sivarama...    6  1505.01462v1   \n",
              "17019                          [{'name': 'Dilek Küçük'}]   30  1707.09611v1   \n",
              "3597   [{'name': 'Hanyuan Zhang'}, {'name': 'Hao Wu'}...    6  1802.02147v1   \n",
              "\n",
              "                                                    link  month  \\\n",
              "22452  [{'rel': 'alternate', 'href': 'http://arxiv.or...      7   \n",
              "3723   [{'rel': 'alternate', 'href': 'http://arxiv.or...      2   \n",
              "10597  [{'rel': 'alternate', 'href': 'http://arxiv.or...      5   \n",
              "17019  [{'rel': 'alternate', 'href': 'http://arxiv.or...      7   \n",
              "3597   [{'rel': 'alternate', 'href': 'http://arxiv.or...      2   \n",
              "\n",
              "                                                 summary  \\\n",
              "22452  This work falls within the context of predicti...   \n",
              "3723   We propose an exact solution for the problem o...   \n",
              "10597  Data in the form of pairwise comparisons arise...   \n",
              "17019  Named entity recognition (NER) is a well-estab...   \n",
              "3597   Estimating the travel time of a path is of gre...   \n",
              "\n",
              "                                                     tag  \\\n",
              "22452  [{'term': 'stat.ML', 'scheme': 'http://arxiv.o...   \n",
              "3723   [{'term': 'cs.DS', 'scheme': 'http://arxiv.org...   \n",
              "10597  [{'term': 'cs.LG', 'scheme': 'http://arxiv.org...   \n",
              "17019  [{'term': 'cs.CL', 'scheme': 'http://arxiv.org...   \n",
              "3597   [{'term': 'cs.LG', 'scheme': 'http://arxiv.org...   \n",
              "\n",
              "                                                   title  year  \n",
              "22452  Nested Kriging predictions for datasets with l...  2016  \n",
              "3723   Counting and Uniform Sampling from Markov Equi...  2018  \n",
              "10597  Estimation from Pairwise Comparisons: Sharp Mi...  2015  \n",
              "17019  Joint Named Entity Recognition and Stance Dete...  2017  \n",
              "3597   DeepTravel: a Neural Network Based Travel Time...  2018  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8fc4a34b-8b79-476c-8158-410779e046c7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>day</th>\n",
              "      <th>id</th>\n",
              "      <th>link</th>\n",
              "      <th>month</th>\n",
              "      <th>summary</th>\n",
              "      <th>tag</th>\n",
              "      <th>title</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>22452</th>\n",
              "      <td>[{'name': 'Didier Rullière'}, {'name': 'Nicola...</td>\n",
              "      <td>19</td>\n",
              "      <td>1607.05432v3</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>7</td>\n",
              "      <td>This work falls within the context of predicti...</td>\n",
              "      <td>[{'term': 'stat.ML', 'scheme': 'http://arxiv.o...</td>\n",
              "      <td>Nested Kriging predictions for datasets with l...</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3723</th>\n",
              "      <td>[{'name': 'AmirEmad Ghassami'}, {'name': 'Sabe...</td>\n",
              "      <td>5</td>\n",
              "      <td>1802.01239v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>2</td>\n",
              "      <td>We propose an exact solution for the problem o...</td>\n",
              "      <td>[{'term': 'cs.DS', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Counting and Uniform Sampling from Markov Equi...</td>\n",
              "      <td>2018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10597</th>\n",
              "      <td>[{'name': 'Nihar B. Shah'}, {'name': 'Sivarama...</td>\n",
              "      <td>6</td>\n",
              "      <td>1505.01462v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>5</td>\n",
              "      <td>Data in the form of pairwise comparisons arise...</td>\n",
              "      <td>[{'term': 'cs.LG', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Estimation from Pairwise Comparisons: Sharp Mi...</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17019</th>\n",
              "      <td>[{'name': 'Dilek Küçük'}]</td>\n",
              "      <td>30</td>\n",
              "      <td>1707.09611v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>7</td>\n",
              "      <td>Named entity recognition (NER) is a well-estab...</td>\n",
              "      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Joint Named Entity Recognition and Stance Dete...</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3597</th>\n",
              "      <td>[{'name': 'Hanyuan Zhang'}, {'name': 'Hao Wu'}...</td>\n",
              "      <td>6</td>\n",
              "      <td>1802.02147v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>2</td>\n",
              "      <td>Estimating the travel time of a path is of gre...</td>\n",
              "      <td>[{'term': 'cs.LG', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>DeepTravel: a Neural Network Based Travel Time...</td>\n",
              "      <td>2018</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8fc4a34b-8b79-476c-8158-410779e046c7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8fc4a34b-8b79-476c-8158-410779e046c7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8fc4a34b-8b79-476c-8158-410779e046c7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1j3_BBoFIoUi"
      },
      "source": [
        "Working on character level means that we don't need to deal with large vocabulary or missing words. Heck, we can even keep uppercase words in text! The downside, however, is that all our sequences just got a lot longer.\n",
        "\n",
        "However, we still need special tokens:\n",
        "* Begin Of Sequence  (__BOS__) - this token is at the start of each sequence. We use it so that we always have non-empty input to our neural network. $P(x_t) = P(x_1 | BOS)$\n",
        "* End Of Sequence (__EOS__) - you guess it... this token is at the end of each sequence. The catch is that it should __not__ occur anywhere else except at the very end. If our model produces this token, the sequence is over.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "F_WXfz1fIoUj"
      },
      "outputs": [],
      "source": [
        "BOS, EOS = ' ', '\\n'\n",
        "\n",
        "lines = data.apply(lambda row: (row['title'] + ' ; ' + row['summary'])[:512], axis=1) \\\n",
        "            .apply(lambda line: BOS + line.replace(EOS, ' ') + EOS) \\\n",
        "            .tolist()\n",
        "\n",
        "# if you missed the seminar, download data here - https://yadi.sk/d/_nGyU2IajjR9-w"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "lsprIqJsJp7S",
        "outputId": "7f64e03a-dbea-4607-9639-2e75bbb056d8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Dual Recurrent Attention Units for Visual Question Answering ; We propose an architecture for VQA which utilizes recurrent layers to generate visual and textual attention. The memory characteristic of the proposed recurrent attention units offers a rich joint embedding of visual and textual features and enables the model to reason relations between several parts of the image and question. Our single model outperforms the first place winner on the VQA 1.0 dataset, performs within margin to the current state-\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOYsLiXMIoUk"
      },
      "source": [
        "Our next step is __building char-level vocabulary__. Put simply, you need to assemble a list of all unique tokens in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVqTxTPQIoUl",
        "outputId": "0486655f-2c01-4b51-f8fe-84edc1f3bff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_tokens =  136\n"
          ]
        }
      ],
      "source": [
        "# get all unique characters from lines (including capital letters and symbols)\n",
        "\n",
        "# tokens = set(''.join(line for line in lines))\n",
        "\n",
        "tokens = set()\n",
        "for line in lines:\n",
        "    tokens.update(set(line))\n",
        "\n",
        "tokens = sorted(tokens)\n",
        "n_tokens = len(tokens)\n",
        "print ('n_tokens = ',n_tokens)\n",
        "assert 100 < n_tokens < 150\n",
        "assert BOS in tokens, EOS in tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJWacuxZIoUl"
      },
      "source": [
        "We can now assign each character with it's index in tokens list. This way we can encode a string into a torch-friendly integer vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QpvVX_svIoUm"
      },
      "outputs": [],
      "source": [
        "# dictionary of character -> its identifier (index in tokens list)\n",
        "token_to_id = {char : id for (char,id)  in zip(tokens, range(n_tokens)) }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mquHo2wIoUn",
        "outputId": "10ea632a-9570-40d6-96ea-381ade088cc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seems alright!\n"
          ]
        }
      ],
      "source": [
        "assert len(tokens) == len(token_to_id), \"dictionaries must have same size\"\n",
        "for i in range(n_tokens):\n",
        "    assert token_to_id[tokens[i]] == i, \"token identifier must be it's position in tokens list\"\n",
        "\n",
        "print(\"Seems alright!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fALM9NgaIoUn"
      },
      "source": [
        "Our final step is to assemble several strings in a integet matrix `[batch_size, text_length]`. \n",
        "\n",
        "The only problem is that each sequence has a different length. We can work around that by padding short sequences with extra _EOS_ or cropping long sequences. Here's how it works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Vyx6YB05IoUo"
      },
      "outputs": [],
      "source": [
        "def to_matrix(lines, max_len=None, pad=token_to_id[EOS], dtype=np.int64):\n",
        "    \"\"\"Casts a list of lines into torch-digestable matrix\"\"\"\n",
        "    max_len = max_len or max(map(len, lines))\n",
        "    lines_ix = np.full([len(lines), max_len], pad, dtype=dtype)\n",
        "    for i in range(len(lines)):\n",
        "        line_ix = list(map(token_to_id.get, lines[i][:max_len]))\n",
        "        lines_ix[i, :len(line_ix)] = line_ix\n",
        "    return lines_ix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiFn3iEfIoUp",
        "outputId": "be2a7c00-592d-4cf1-a5a5-b4952dbdc08a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1 66 67 68  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 1 66 67 66 68 66 67 66  0  0  0  0  0  0  0]\n",
            " [ 1 66 67 68 18 19 20 21 22 23 24 25 26 17  0]]\n"
          ]
        }
      ],
      "source": [
        "#Example: cast 3 random lines to matrices, pad with zeros\n",
        "dummy_lines = [\n",
        "    ' abc\\n',\n",
        "    ' abacaba\\n',\n",
        "    ' abc1234567890\\n',\n",
        "]\n",
        "print(to_matrix(dummy_lines))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWzKDi2dIoUr"
      },
      "source": [
        "### Neural Language Model (2 points including training)\n",
        "\n",
        "Just like for N-gram LMs, we want to estimate probability of text as a joint probability of tokens (symbols this time).\n",
        "\n",
        "$$P(X) = \\prod_t P(x_t \\mid x_0, \\dots, x_{t-1}).$$ \n",
        "\n",
        "Instead of counting all possible statistics, we want to train a neural network with parameters $\\theta$ that estimates the conditional probabilities:\n",
        "\n",
        "$$ P(x_t \\mid x_0, \\dots, x_{t-1}) \\approx p(x_t \\mid x_0, \\dots, x_{t-1}, \\theta) $$\n",
        "\n",
        "\n",
        "But before we optimize, we need to define our neural network. Let's start with a fixed-window (aka convolutional) architecture:\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/fixed_window_lm.jpg' width=400px>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2lf3uSBQIoUs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7SMGTcCpKlm",
        "outputId": "81fe602a-8f9d-4c79-d07c-d5d4162a9594"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "02hVEWHLIoUt"
      },
      "outputs": [],
      "source": [
        "class FixedWindowLanguageModel(nn.Module):\n",
        "    def __init__(self, device, n_tokens=n_tokens, emb_size=16, hid_size=64, kernel_size=5):\n",
        "        \"\"\" \n",
        "        A fixed window model that looks on at least 5 previous symbols (kernel_size).\n",
        "        \n",
        "        Note: fixed window LM is effectively performing a convolution over a sequence of words.\n",
        "        This convolution only looks on current and previous words.\n",
        "        Such convolution can be represented as a sequence of 2 operations:\n",
        "        - pad input vectors by {strides * (filter_size - 1)} zero vectors on the \"left\", do not pad right\n",
        "        - perform regular convolution with {filter_size} and {strides}\n",
        "        \n",
        "        - If you're absolutely lost, here's a hint: use nn.ZeroPad2d((NUM_LEADING_ZEROS, 0, 0, 0))\n",
        "          followed by a nn.Conv1d(..., padding=0). And yes, its okay that padding is technically \"2d\".\n",
        "        \"\"\"\n",
        "        super().__init__() # initialize base class to track sub-layers, trainable variables, etc.\n",
        "        \n",
        "        self.device = device\n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        self.emb = nn.Embedding(n_tokens, emb_size)\n",
        "        self.conv1d = nn.Conv1d(in_channels=emb_size,\n",
        "                                out_channels=hid_size,\n",
        "                                kernel_size=5)\n",
        "        self.fc1 = nn.Linear(hid_size, hid_size)\n",
        "        self.fc2 = nn.Linear(hid_size, n_tokens)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def __call__(self, input_ix):\n",
        "        \"\"\"\n",
        "        compute language model logits given input tokens\n",
        "        :param input_ix: batch of sequences with token indices, tensor: int32[batch_size, sequence_length]\n",
        "        :returns: pre-softmax linear outputs of language model [batch_size, sequence_length, n_tokens]\n",
        "            these outputs will be used as logits to compute P(x_t | x_0, ..., x_{t - 1})\n",
        "            \n",
        "        :note: that convolutions operate with tensors of shape [batch, channels, length], while linear layers\n",
        "         and *embeddings* use [batch, length, channels] tensors. Use tensor.permute(...) to adjust shapes.\n",
        "\n",
        "        \"\"\"\n",
        "        inp_emb = self.emb(input_ix) # [batch_size, sequence_length, emb_dim]\n",
        "        inp_emb = inp_emb.permute((0, 2, 1)) # [batch_size, emb_dim, sequence_length]        \n",
        "        \n",
        "        # apply padding to keep tensor size\n",
        "        # pad (with zeros) last dim by kernel_size - 1 on the left and 0 on the right\n",
        "        inp_emb = F.pad(inp_emb, pad=(self.kernel_size - 1, 0)) \n",
        "        #print(inp_emb[0][0])\n",
        "        '''\n",
        "        tensor([ 0.0000,  0.0000,  0.0000,  0.0000, -0.0653, -0.4699,  1.1353, -0.8066,\n",
        "         0.4302,  0.4302,  0.4302,  0.4302,  0.4302,  0.4302,  0.4302,  0.4302,\n",
        "         0.4302,  0.4302,  0.4302], grad_fn=<SelectBackward0>)\n",
        "        '''\n",
        "        inp_emb = self.conv1d(inp_emb) # [batch_size, hid_size, sequence_length]\n",
        "        #print(inp_emb.size())\n",
        "        # if we haven't used the padding we would have gotten out_shape = [batch_size, hid_size, sequence_length - 4]\n",
        "        \n",
        "        inp_emb = inp_emb.permute((0, 2, 1)) # [batch_size, sequence_length, hid_size]       \n",
        "\n",
        "        inp_emb = self.fc1(inp_emb) # [batch_size, sequence_length, hid_size]  \n",
        "        inp_emb = self.relu(inp_emb)  \n",
        "        inp_emb = self.fc2(inp_emb) # [batch_size, sequence_length, n_tokens]  \n",
        "        \n",
        "        return inp_emb # [batch_size, sequence_length, n_tokens]\n",
        "    \n",
        "    def get_possible_next_tokens(self, prefix=BOS, temperature=1.0, max_len=100):\n",
        "        \"\"\" :returns: probabilities of next token, dict {token : prob} for all tokens \"\"\"\n",
        "        prefix_ix = torch.as_tensor(to_matrix([prefix]), dtype=torch.int64).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            probs = torch.softmax(self(prefix_ix)[0, -1], dim=-1).cpu().numpy()  # shape: [n_tokens]\n",
        "        return dict(zip(tokens, probs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2aR5C9cIoUu",
        "outputId": "e1575339-5a54-47ac-f2f0-bd6c1fc79ee5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: ('emb.weight', 'conv1d.weight', 'conv1d.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias')\n"
          ]
        }
      ],
      "source": [
        "dummy_model = FixedWindowLanguageModel(device=device)\n",
        "\n",
        "dummy_input_ix = torch.as_tensor(to_matrix(dummy_lines))\n",
        "dummy_logits = dummy_model(dummy_input_ix)\n",
        "\n",
        "print('Weights:', tuple(name for name, w in dummy_model.named_parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8oeS6DYFIoUv"
      },
      "outputs": [],
      "source": [
        "assert isinstance(dummy_logits, torch.Tensor)\n",
        "assert dummy_logits.shape == (len(dummy_lines), max(map(len, dummy_lines)), n_tokens), \"please check output shape\"\n",
        "assert np.all(np.isfinite(dummy_logits.data.cpu().numpy())), \"inf/nan encountered\"\n",
        "assert not np.allclose(dummy_logits.data.cpu().numpy().sum(-1), 1), \"please predict linear outputs, don't use softmax (maybe you've just got unlucky)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "avB9WHl1IoUw"
      },
      "outputs": [],
      "source": [
        "# test for lookahead\n",
        "dummy_input_ix_2 = torch.as_tensor(to_matrix([line[:3] + 'e' * (len(line) - 3) for line in dummy_lines]))\n",
        "dummy_logits_2 = dummy_model(dummy_input_ix_2)\n",
        "\n",
        "assert torch.allclose(dummy_logits[:, :3], dummy_logits_2[:, :3]), \"your model's predictions depend on FUTURE tokens. \" \\\n",
        "    \" Make sure you don't allow any layers to look ahead of current token.\" \\\n",
        "    \" You can also get this error if your model is not deterministic (e.g. dropout). Disable it for this test.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V51YcEXIoUw"
      },
      "source": [
        "We can now tune our network's parameters to minimize categorical crossentropy over training dataset $D$:\n",
        "\n",
        "$$ L = {\\frac1{|D|}} \\sum_{X \\in D} \\sum_{x_i \\in X} - \\log p(x_t \\mid x_1, \\dots, x_{t-1}, \\theta) $$\n",
        "\n",
        "As usual with with neural nets, this optimization is performed via stochastic gradient descent with backprop.  One can also note that minimizing crossentropy is equivalent to minimizing model __perplexity__, KL-divergence or maximizng log-likelihood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5vlP6_SIoUx",
        "outputId": "a61c183d-2ed7-4e6a-caec-696316292542"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "matrix:\n",
            " [[ 1 66 67 68  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 1 66 67 66 68 66 67 66  0  0  0  0  0  0  0]\n",
            " [ 1 66 67 68 18 19 20 21 22 23 24 25 26 17  0]]\n",
            "mask: [[1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 1 1 1 1 1 1 1 1 0 0 0 0 0 0]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
            "lengths: [ 5  9 15]\n"
          ]
        }
      ],
      "source": [
        "def compute_mask(input_ix, eos_ix=token_to_id[EOS]):\n",
        "    \"\"\" compute a boolean mask that equals \"1\" until first EOS (including that EOS) \"\"\"\n",
        "    return F.pad(torch.cumsum(input_ix == eos_ix, dim=-1)[..., :-1] < 1, pad=(1, 0, 0, 0), value=True)\n",
        "\n",
        "print('matrix:\\n', dummy_input_ix.numpy())\n",
        "print('mask:', compute_mask(dummy_input_ix).to(torch.int32).cpu().numpy())\n",
        "print('lengths:', compute_mask(dummy_input_ix).sum(-1).cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "30SksDA1IoUx"
      },
      "outputs": [],
      "source": [
        "def compute_loss(model, input_ix):\n",
        "    \"\"\"\n",
        "    :param model: language model that can compute next token logits given token indices\n",
        "    :param input ix: int32 matrix of tokens, shape: [batch_size, length]; padded with eos_ix\n",
        "    :returns: scalar loss function, mean crossentropy over non-eos tokens\n",
        "    \"\"\"\n",
        "    input_ix = torch.as_tensor(input_ix, dtype=torch.int64)\n",
        "    '''\n",
        "    tensor([[ 1, 66, 67, 68,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "        [ 1, 66, 67, 66, 68, 66, 67, 66,  0,  0,  0,  0,  0,  0,  0],\n",
        "        [ 1, 66, 67, 68, 18, 19, 20, 21, 22, 23, 24, 25, 26, 17,  0]])\n",
        "    '''\n",
        "    targets = input_ix[:, 1:]\n",
        "    '''\n",
        "    tensor([[66, 67, 68,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "        [66, 67, 66, 68, 66, 67, 66,  0,  0,  0,  0,  0,  0,  0],\n",
        "        [66, 67, 68, 18, 19, 20, 21, 22, 23, 24, 25, 26, 17,  0]])\n",
        "    '''\n",
        "    mask = compute_mask(targets)\n",
        "    targets_1hot = F.one_hot(targets, n_tokens).to(torch.float32)\n",
        "\n",
        "    logits = model(input_ix[:, :-1])\n",
        "    logits = torch.log_softmax(logits, dim=-1)\n",
        "\n",
        "    # log-probabilities of correct outputs, [batch_size, n_tokens]\n",
        "    logp_out = (logits * targets_1hot).sum(dim=-1)  \n",
        "\n",
        "    return -logp_out[mask].mean() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "IrmuyUdnIoUy"
      },
      "outputs": [],
      "source": [
        "loss_1 = compute_loss(dummy_model, to_matrix(dummy_lines, max_len=15))\n",
        "loss_2 = compute_loss(dummy_model, to_matrix(dummy_lines, max_len=16))\n",
        "assert (np.ndim(loss_1) == 0) and (0 < loss_1 < 100), \"loss must be a positive scalar\"\n",
        "assert torch.allclose(loss_1, loss_2), 'do not include  AFTER first EOS into loss. '\\\n",
        "    'Hint: use compute_mask. Beware +/-1 errors. And be careful when averaging!' "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLTKBrJsIoUz"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "You will need two functions: one to compute test loss and another to generate samples. For your convenience, we implemented them both in your stead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "wzqN6my8IoUz"
      },
      "outputs": [],
      "source": [
        "def score_lines(model, dev_lines, batch_size):\n",
        "    \"\"\" computes average loss over the entire dataset \"\"\"\n",
        "    dev_loss_num, dev_loss_len = 0., 0.\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(dev_lines), batch_size):\n",
        "            batch_ix = torch.as_tensor(to_matrix(dev_lines[i: i + batch_size])).to(device)\n",
        "            dev_loss_num += compute_loss(model, batch_ix).item() * len(batch_ix)\n",
        "            dev_loss_len += len(batch_ix)\n",
        "    return dev_loss_num / dev_loss_len\n",
        "\n",
        "def generate(model, prefix=BOS, temperature=1.0, max_len=100):\n",
        "    \"\"\"\n",
        "    Samples output sequence from probability distribution obtained by model\n",
        "    :param temperature: samples proportionally to model probabilities ^ temperature\n",
        "        if temperature == 0, always takes most likely token. Break ties arbitrarily.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        while True:\n",
        "            token_probs = model.get_possible_next_tokens(prefix)\n",
        "            tokens, probs = zip(*token_probs.items())\n",
        "            if temperature == 0:\n",
        "                next_token = tokens[np.argmax(probs)]\n",
        "            else:\n",
        "                probs = np.array([p ** (1. / temperature) for p in probs])\n",
        "                probs /= sum(probs)\n",
        "                next_token = np.random.choice(tokens, p=probs)\n",
        "\n",
        "            prefix += next_token\n",
        "            if next_token == EOS or len(prefix) > max_len: break\n",
        "    return prefix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vI9QmixFIoU0"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "Finally, let's train our model on minibatches of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VKvkF_mIoU0",
        "outputId": "8d2349f5-cb8a-4229-a440-381b4d6247c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev history =  [(0, 4.925588013625727)]\n",
            "Sample before training: Bridging;μ/+.~ãôgtk^Σï>α2\"DY3uρêατKun3sçX!μpτ9Π:qrőXÜ/k]LRÜt\"84)Jρ!à,\"4s{Łz2á3χáyèïΠ4Oæõbíα#M/x$śóô^ν\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_lines, dev_lines = train_test_split(lines, test_size=0.25, random_state=42)\n",
        "\n",
        "batch_size = 256\n",
        "score_dev_every = 250\n",
        "train_history, dev_history = [], []\n",
        "model = FixedWindowLanguageModel(device=device)\n",
        "opt = torch.optim.Adam(model.parameters())\n",
        "model.to(device)\n",
        "# hint: if you ever wanted to switch to cuda, do it now.\n",
        "\n",
        "# score untrained model\n",
        "dev_history.append((0, score_lines(model, dev_lines, batch_size)))\n",
        "print(\"Dev history = \", dev_history)\n",
        "print(\"Sample before training:\", generate(model, 'Bridging'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "pSyG4GuxIoU1",
        "outputId": "a216cfe1-3804-48d2-f7b6-a12fb0d80781"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated examples (tau=0.5):\n",
            " In the intery provideas an and endural networks the the the coder the proposed how the solve patter \n",
            " the Recognition facthe computional Rearning the partical set the sepresent in a presentation of the \n",
            " Language of the enterong and the distric problem in a detection of the stople with for hement and pr\n",
            "Scoring dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|██████████| 5000/5000 [03:19<00:00, 25.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#4999 Dev loss: 1.641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "from random import sample\n",
        "from tqdm import trange\n",
        "\n",
        "for i in trange(5000):\n",
        "    batch = torch.as_tensor(to_matrix(sample(train_lines, batch_size))).to(device)\n",
        "    \n",
        "    loss_i = compute_loss(model, batch)\n",
        "    \n",
        "    opt.zero_grad()\n",
        "    loss_i.backward()\n",
        "    opt.step()\n",
        "        \n",
        "    train_history.append((i, loss_i.item()))\n",
        "    \n",
        "    if (i + 1) % 50 == 0:\n",
        "        clear_output(True)\n",
        "        plt.scatter(*zip(*train_history), alpha=0.1, label='train_loss')\n",
        "        if len(dev_history):\n",
        "            plt.plot(*zip(*dev_history), color='red', label='dev_loss')\n",
        "        plt.legend(); plt.grid(); plt.show()\n",
        "        print(\"Generated examples (tau=0.5):\")\n",
        "        for _ in range(3):\n",
        "            print(generate(model, temperature=0.6, max_len=300))\n",
        "    \n",
        "    if (i + 1) % score_dev_every == 0:\n",
        "        print(\"Scoring dev...\")\n",
        "        dev_history.append((i, score_lines(model, dev_lines, batch_size)))\n",
        "        print('#%i Dev loss: %.3f' % dev_history[-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYe7gJc4IoU2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81f1ebe3-cfc5-44b1-8cd9-a1d677be8e89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final dev loss: 1.6409961111487412\n",
            " A diven a generation and regred under interfaction with the the prodonition of deep intere abound cl\n",
            " Heed an idents and station of the proposes of the emplexification for the intering this problem of t\n",
            " The detertate and methods and the the model to complet is a fixtural diversourting are can language \n",
            " Comment a model on that the systributional problem for a solve been interforms a for the model deep \n",
            " Simple networks ; We conver a compled of the propice training combined for the agian networks the in\n",
            " A results use a neural neural Stributional framewer and the sent the layer when the proposed and ent\n",
            " A complexical sumples the space the semantic system a present and to deters and the different algori\n",
            " Data as everal condoming on the problem of the low algores the are and convolutional dention and fra\n",
            " Desical for and in the minimation of the of the stated on that commone the systems of the and to stu\n",
            " The the algorition of the rese between in the deep neural pepatiwility of adven to studing and of i \n"
          ]
        }
      ],
      "source": [
        "assert np.mean(train_history[:10], axis=0)[1] > np.mean(train_history[-10:], axis=0)[1], \"The model didn't converge.\"\n",
        "print(\"Final dev loss:\", dev_history[-1][-1])\n",
        "\n",
        "for i in range(10):\n",
        "    print(generate(model, temperature=0.5, max_len=300))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    print(generate(model, temperature=.4, max_len=300))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NgDINYFSu8J",
        "outputId": "68dbd07d-9429-4b95-90e4-d2a350d872bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A neural networks for the systems ; This paper, we problem is a constribution of the segments of the\n",
            " A graph a such of the intere is the condent in the intern be and the interactions of the the systemb\n",
            " And and by the propose of the distributional Gaussive of the classifications of the and detections o\n",
            " Deter the such as the intered to emporal Anal-domain the conding and as a problem is of in the and o\n",
            " A simples sament and and the models and to essime and problems the standing and the sement as and on\n",
            " Adence of the subsed by the the visual experse introdiction of the model of the intering of the comp\n",
            " Problem problems and in the proposed in the of computer the intering the simple and to the in the co\n",
            " Multi-structure as a computation of and the distribution of the the problem can be used a such as ar\n",
            " A scomputation of the settoring the and in problem is a soblem application of the the a set on the s\n",
            " Sentation of the syper convolutional Networks the proposed and stor a problem locomation winder in t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9NzDDoNIoU3"
      },
      "source": [
        "### RNN Language Models (3 points including training)\n",
        "\n",
        "Fixed-size architectures are reasonably good when capturing short-term dependencies, but their design prevents them from capturing any signal outside their window. We can mitigate this problem by using a __recurrent neural network__:\n",
        "\n",
        "$$ h_0 = \\vec 0 ; \\quad h_{t+1} = RNN(x_t, h_t) $$\n",
        "\n",
        "$$ p(x_t \\mid x_0, \\dots, x_{t-1}, \\theta) = dense_{softmax}(h_{t-1}) $$\n",
        "\n",
        "Such model processes one token at a time, left to right, and maintains a hidden state vector between them. Theoretically, it can learn arbitrarily long temporal dependencies given large enough hidden size.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/rnn_lm.jpg' width=480px>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "kHmhGo_lIoU4"
      },
      "outputs": [],
      "source": [
        "class RNNLanguageModel(nn.Module):\n",
        "    def __init__(self, device, n_tokens=n_tokens, emb_size=16, hid_size=256, num_layers=2, dropout=0.2, bid=False):\n",
        "        \"\"\" \n",
        "        Build a recurrent language model.\n",
        "        You are free to choose anything you want, but the recommended architecture is\n",
        "        - token embeddings\n",
        "        - one or more LSTM/GRU layers with hid size\n",
        "        - linear layer to predict logits\n",
        "        \n",
        "        :note: if you use nn.RNN/GRU/LSTM, make sure you specify batch_first=True\n",
        "         With batch_first, your model operates with tensors of shape [batch_size, sequence_length, num_units]\n",
        "         Also, please read the docs carefully: they don't just return what you want them to return :)\n",
        "        \"\"\"\n",
        "        super().__init__() # initialize base class to track sub-layers, trainable variables, etc.\n",
        "        self.device = device\n",
        "        \n",
        "        self.emb = nn.Embedding(n_tokens, emb_size) \n",
        "\n",
        "        self.lstm = nn.LSTM(emb_size, hid_size, \n",
        "                            num_layers, batch_first=True, \n",
        "                            bidirectional=bid) # bidirectional sucks for character-level approach\n",
        "                                               # so don't use it\n",
        "        self.hid1 = nn.Linear(hid_size + hid_size*bid, hid_size + hid_size*bid)\n",
        "        self.hid_to_logits = nn.Linear(hid_size + hid_size*bid, n_tokens)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "    def __call__(self, input_ix):\n",
        "        \"\"\"\n",
        "        compute language model logits given input tokens\n",
        "        :param input_ix: batch of sequences with token indices, tensor: int32[batch_size, sequence_length]\n",
        "        :returns: pre-softmax linear outputs of language model [batch_size, sequence_length, n_tokens]\n",
        "            these outputs will be used as logits to compute P(x_t | x_0, ..., x_{t - 1})\n",
        "        \"\"\"\n",
        "        inp_emb = self.emb(input_ix) # [batch_size, sequence_length, emb_dim]\n",
        "        inp_emb = self.dropout(inp_emb)\n",
        "\n",
        "        enc_seq, last_state_but_not_really = self.lstm(inp_emb)\n",
        "        # enc_seq: [batch, time, hid_size], last_state: [batch, hid_size]\n",
        "        # enc_seq -> contains the output features (h_t) from the last layer of the LSTM, for each t\n",
        "        # last_state -> last state h_t of encoder (h_0 for decoder)\n",
        "        enc_seq = self.hid1(enc_seq)\n",
        "        enc_seq = self.relu(enc_seq)\n",
        "\n",
        "        next_logits = self.hid_to_logits(enc_seq)\n",
        "        next_logp = F.log_softmax(next_logits, dim=-1)\n",
        "\n",
        "        return next_logp # [batch_size, sequence_length, n_tokens]\n",
        "\n",
        "\n",
        "    def get_possible_next_tokens(self, prefix=BOS, temperature=1.0, max_len=100):\n",
        "        \"\"\" :returns: probabilities of next token, dict {token : prob} for all tokens \"\"\"\n",
        "        prefix_ix = torch.as_tensor(to_matrix([prefix]), dtype=torch.int64).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            probs = torch.softmax(self(prefix_ix)[0, -1], dim=-1).cpu().numpy()  # shape: [n_tokens]\n",
        "        return dict(zip(tokens, probs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "554JsYrVIoU5",
        "outputId": "9e81b581-8854-44e2-b625-dbb697f6b24d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: ('emb.weight', 'lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'lstm.weight_ih_l1', 'lstm.weight_hh_l1', 'lstm.bias_ih_l1', 'lstm.bias_hh_l1', 'lstm.weight_ih_l2', 'lstm.weight_hh_l2', 'lstm.bias_ih_l2', 'lstm.bias_hh_l2', 'hid1.weight', 'hid1.bias', 'hid_to_logits.weight', 'hid_to_logits.bias')\n"
          ]
        }
      ],
      "source": [
        "rnn_model = RNNLanguageModel(device=device, dropout=0, num_layers=3, bid=False)\n",
        "\n",
        "dummy_input_ix = torch.as_tensor(to_matrix(dummy_lines))\n",
        "dummy_logits = rnn_model(dummy_input_ix)\n",
        "\n",
        "assert isinstance(dummy_logits, torch.Tensor)\n",
        "assert dummy_logits.shape == (len(dummy_lines), max(map(len, dummy_lines)), n_tokens), \"please check output shape\"\n",
        "assert not np.allclose(dummy_logits.cpu().data.numpy().sum(-1), 1), \"please predict linear outputs, don't use softmax (maybe you've just got unlucky)\"\n",
        "print('Weights:', tuple(name for name, w in rnn_model.named_parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "UfRsmkOQIoU5"
      },
      "outputs": [],
      "source": [
        "# test for lookahead\n",
        "dummy_input_ix_2 = torch.as_tensor(to_matrix([line[:3] + 'e' * (len(line) - 3) for line in dummy_lines]))\n",
        "dummy_logits_2 = rnn_model(dummy_input_ix_2)\n",
        "\n",
        "assert torch.allclose(dummy_logits[:, :3], dummy_logits_2[:, :3]), \"your model's predictions depend on FUTURE tokens. \" \\\n",
        "    \" Make sure you don't allow any layers to look ahead of current token.\" \\\n",
        "    \" You can also get this error if your model is not deterministic (e.g. dropout). Disable it for this test.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL7S0HfmIoU5"
      },
      "source": [
        "### RNN training\n",
        "\n",
        "Our RNN language model should optimize the same loss function as fixed-window model. But there's a catch. Since RNN recurrently multiplies gradients through many time-steps, gradient values may explode, [ruining](https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/nan.jpg) your model.\n",
        "The common solution to that problem is to clip gradients either [individually](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/clip_by_value) or [globally](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/clip_by_global_norm).\n",
        "\n",
        "Your task here is to implement the training code that minimizes the loss function. If you encounter large loss fluctuations during training, please add [gradient clipping](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html) using urls above. But its **not necessary** to use gradient clipping if you don't need it.\n",
        "\n",
        "_Note: gradient clipping is not exclusive to RNNs. Convolutional networks with enough depth often suffer from the same issue._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79ob0PtXIoU6",
        "outputId": "c1142277-5dba-4be8-d69d-60c29306efda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 4.904841408706293)]\n",
            "Sample before training: Bridgingk7ózS.D$v(Zx33Ωä$ŁŁγêbΠ%oó*õ%BDãyIz%2}fwet(I.tm0Gtaσ0mJxÜáô9αóKg;ΩRàKêä`ö+=+vByχ>í87phTAcqρ\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128 # <-- please tune batch size to fit your CPU/GPU configuration\n",
        "score_dev_every = 250\n",
        "train_history, dev_history = [], []\n",
        "\n",
        "# bidirectional sucks with character-level, so don't use it\n",
        "# higher num_layers - slower convergence\n",
        "rnn_model = RNNLanguageModel(device=device, num_layers=2, dropout=0.2, bid=False)\n",
        "opt = torch.optim.Adam(rnn_model.parameters())\n",
        "rnn_model.to(device)\n",
        "\n",
        "# score untrained model\n",
        "dev_history.append((0, score_lines(rnn_model, dev_lines, batch_size)))\n",
        "print(dev_history)\n",
        "print(\"Sample before training:\", generate(rnn_model, 'Bridging'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "ApE01DvlIoU7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "0a13242b-5ff3-48bb-e67e-7226768efcf8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated examples (tau=0.5):\n",
            " Unsupervised Learning and Deep Neural Networks ; We propose a new problem of distributed algorithms \n",
            " Deep Convolutional Neural Networks for Texture Learning ; Convolutional neural networks are set of t\n",
            " Stochastic Models for Multi-Sparse Real-Simulation of Adversarial Networks ; Fuzzy one of the proble\n",
            "Scoring dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [17:09<00:00,  4.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#4999 Dev loss: 1.098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "from random import sample\n",
        "from tqdm import trange\n",
        "\n",
        "for i in trange(5000):\n",
        "    batch = torch.as_tensor(to_matrix(sample(train_lines, batch_size))).to(device)\n",
        "\n",
        "    loss_i = compute_loss(rnn_model, batch)\n",
        "    \n",
        "    opt.zero_grad()\n",
        "    loss_i.backward()\n",
        "    opt.step()\n",
        "\n",
        "    train_history.append((i, float(loss_i)))\n",
        "    \n",
        "    if (i + 1) % 50 == 0:\n",
        "        clear_output(True)\n",
        "        plt.scatter(*zip(*train_history), alpha=0.1, label='train_loss')\n",
        "        if len(dev_history):\n",
        "            plt.plot(*zip(*dev_history), color='red', label='dev_loss')\n",
        "        plt.legend(); plt.grid(); plt.show()\n",
        "        print(\"Generated examples (tau=0.5):\")\n",
        "        for _ in range(3):\n",
        "            print(generate(rnn_model, temperature=0.5))\n",
        "    \n",
        "    if (i + 1) % score_dev_every == 0:\n",
        "        print(\"Scoring dev...\")\n",
        "        dev_history.append((i, score_lines(rnn_model, dev_lines, batch_size)))\n",
        "        print('#%i Dev loss: %.3f' % dev_history[-1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating samples with RNN model"
      ],
      "metadata": {
        "id": "0H5WSkAF06Yr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "aBg7r_2eIoU7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b68f8c17-2471-4007-fbe1-3cac8601207d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final dev loss: 1.0984088114761725\n",
            " A Bayesian Approach for Embedding Algorithms ; In this paper, we propose a set of the first of the statistical approach for exploration in the set of the set of the control control in convolutional neural networks (CNNs) is a structure of the problem of the parameters of the systematic problems. The\n",
            " Method for Semantic Infully Step to Similar Bourt Detection ; We propose a new model and the presence of simulations are inverse and accountered in statistical streaming, and the developing on the problem of all the field of the approach of the discriminative relative supervised methods that there a\n",
            " A new probabilistic topic for context of the device and   interval recognition ; We present a method for emotional contextual spectral constituent of the construction of complexity of search analysis of the articipant and algorithms that enh accurate in the problem of times and a set of the probabil\n",
            " A Robust Complexity of Deep Learning for Deep Learning ; The framework of the factorization of the statistical interaction programming, in particular, we propose to the structure of the sentence of a set of the problem. We consider the statistical veristic setting is a property of the to developing \n",
            " Analyzing a semantic and supervised learning of supervised learning ; We propose a proposed approach to many spans of the neural network in a prior of problems in constructing the comprehension of the sparse sufferences in the large orientation of the most object or the original distribution of the \n",
            " An Expirical Approach to Hule Complexity of Learning Sentences ; This issue is a complete dataset of the internal data setting and sensing the sequence of the computational field of the problem of the same scale in the structure. The popular techniques are becomed to the success of the motion to a s\n",
            " Towards Function Graph Detection for Action Networks ; We propose a new role of the fundamental scenario for the internal optimization algorithm for contextual research in the same convex optimizing the specific scene in the problem by a set of the computer vision computer vision activity to solve t\n",
            " Realing Synthesis of Deep Networks for Long   Data ; We present a traffic to the important of an approach to be proposed and adversarial ambiguous relations in a probability of probability researchers that be allows to the proposed mathematical components for the continuous approaches that allows th\n",
            " A Side Context-Based Deep Structure of Support Regression ; Recent years to support complexity can be containing the content of the computer vision of the new classifiers of the context of the number of distance and automatic object recognition. The world that structure is the definition of computer\n",
            " Deep Convorulal Networks with Word Contributions ; The problem of a probabilistic information that can gene two models in the control of the problem with the prediction of computer vision or applicability and multiple classification on the brain distribution of the problem of the problem. We propose\n"
          ]
        }
      ],
      "source": [
        "assert np.mean(train_history[:10], axis=0)[1] > np.mean(train_history[-10:], axis=0)[1], \"The model didn't converge.\"\n",
        "print(\"Final dev loss:\", dev_history[-1][-1])\n",
        "for i in range(10):\n",
        "    print(generate(rnn_model, temperature=0.5, max_len=300))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert np.mean(train_history[:10], axis=0)[1] > np.mean(train_history[-10:], axis=0)[1], \"The model didn't converge.\"\n",
        "for i in range(10):\n",
        "    print(generate(rnn_model, temperature=0.6, max_len=300))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjPFqZGP9q5s",
        "outputId": "fa862a45-c472-4eb0-8421-fbae2abbdeaf"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Learning and Broad Learning Estimation Cases ; The sub-set of this article on the model for compositionally sparse methods have been lower to perform such as For-Post-Instance Amonomography, which is a simple dependent or an algorithm for a noun of the need for the analysis of the notion of the trai\n",
            " Large-Analysis Of Target Face Retreesing and Semantic Pose ; Auto-entropy deep neural networks that rely one of the sample setmentation for the model approach to accousteric information strategy constructions and remarkable statistical problems, the objective of the theoretical relationships and its\n",
            " A Supervised Learning Approach for Language To word State-Recognition ; This paper presents a novel distribution of the relationship between the automatic problem based on the pattern model that they makes a storether constraint process of population models such data and semantics and problems, such\n",
            " Data Principal Exploration of Revices of Alternative ;ould Detection ; The fine-grained model network to this single-based sentences in the problem of the factorization of the spatial analysis. The main comprehensive and exploration of different design on a constrainting computer vision of the perso\n",
            " Experimental Algorithms for Activation Approach with Computer Transformation ; The context-constraint of a single method for operations or samples of the inference understanding of our relational performance. The means of artificial approaches for machine learning algorithms and compared to a fast t\n",
            " Modeling Acrues and Speech Monte Evaluation ; In this paper, we propose a formal function of the introducion of the memory network to a context statistical problem of the statistical problem, which is the local streve task of the inference of a neural network for computing analysis in the possibilit\n",
            " Term Convex Adeptor For Convergence Reason of Deep   Exploration Technologies ; In this paper, we address the statistical intelligence to accelerate the despite. Atchologies in very conceptual representation and problems are the computational Model for the modeled practical foal optimization and pro\n",
            " Spatially State-of-Graph Control Models for Interval Sparse Continuous   Croupls ; A human variational population of the area of the variational convolutional neural network autoencoders are information is to provide a sketched model and computer visive complexity such as the accuracy of projects, w\n",
            " Deep Convolutional Neural Networks for Relational Complexity ;ut ; This paper investigates the constraint system in detection, management of the setting by meaning-problems, and introduced relavionship between the non-solving deep learning problem in machine learning that are often in why applicatio\n",
            " Training the More Discretation for Explanation of Computing ; In this paper, we present an efficient convolutional neural network algorithm for shallow to the high-graph of the distribution of skin some from points of the substantial interaction sources. The single implementation of interest interes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert np.mean(train_history[:10], axis=0)[1] > np.mean(train_history[-10:], axis=0)[1], \"The model didn't converge.\"\n",
        "for i in range(10):\n",
        "    print(generate(rnn_model, 'Artificial intelligence', temperature=0.6, max_len=300))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Puh-HEHK95oV",
        "outputId": "e910bfaa-b599-4b90-aae5-6755040a1312"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial intelligence models using spatial intelligence and models ; The proposed stnames to practical and algorithm can be developed to our approach for small detection of the screeting or action learning problems. An approximate surface detection system the reconstruction system are contextual to\n",
            "Artificial intelligence of expertation of   programming mapping in the context-of-the-art part of the   the maximilation of the sequences ; The computational components of the target of latent classification problems. In this paper, we explore the problem of the modeling of the novel motion of multip\n",
            "Artificial intelligence surface approaches for problem in the decomposed Representation of Machine Ensembles ; Artificial particular recognition is a feature of the distribution of the simple training steps of the problem of the redularized distributions of the difeasion of a single gradient speech. \n",
            "Artificial intelligence to a polynomial image analysis of a component of   real-world and specific cancer datasets of controlle evolutional approximations of the presentation of an  the application of the task analysis of the complex fractal patterns of fast can be support to compare these stations a\n",
            "Artificial intelligence that miting a constraint of the computation of computer vision languages are the approach of the information of latent contentions consists of the underlying the regissue to the  -GAN methods are generally evaluated to biological and several approximated modeling the problem a\n",
            "Artificial intelligence to the notterial model for relational parameter properties of the non-start performance of extression problems state detection ; Complete or the physical scale interaction is a spectral property of the depths are attreaded to propose a neuromort to train a computational transf\n",
            "Artificial intelligence of the integration of image theory into a multi-label machine learning framework that can be recognized that shortest the problem of denous developments are considered to explore the end-to-end tool for the otherwide in multiple tracking approaches. In this paper, we propose a\n",
            "Artificial intelligence on object to impresent optimization in the problem ; In particular methods for proposed by practical analysis of one of the alternative object tree on the parameter related and semantic scale in the problem of other or some of the support specificity. We show that the training\n",
            "Artificial intelligence and computer vision on the interaction of polynomial variation systems, where the original problem with the use of interest of the recent problem of a counting in this paper proposes a brain and computation of similarity means of deep neural networks. The local optimization pr\n",
            "Artificial intelligence of the input invanions are successfully affected we explore an internal form of the contern of intelligent distribution algorithms for several methods are used to the same semantic features for meaningly detection. In this paper, we propose a novel reported to understand in th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert np.mean(train_history[:10], axis=0)[1] > np.mean(train_history[-10:], axis=0)[1], \"The model didn't converge.\"\n",
        "for i in range(10):\n",
        "    print(generate(rnn_model, 'Natural language', temperature=0.6, max_len=300))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAct7yNW0ShY",
        "outputId": "5e70d3f6-2610-4baa-b3ac-d0e98e8f80e5"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural language of the Development of Recurrent Neural Networks with Monological Models with Event Prior Machine Learning   Based Containing System   for Substantial Transfer Input Sparse Renots for All States of   Digital Pre System Study on Support Genetic  Abappling ; Unlinite theory are provided\n",
            "Natural language and analyses of the context of the paradigm of specifically supervised anomy models understanding the scalable transformation and artificial processing problems are experience of each of structure of a comparison of our work on the problem of extremes and computational by a represent\n",
            "Natural language of practical data is that an approximate classification of the spectral structure of subtle given statistical and introduction of the significant subject of short-based distributions in the problem of the topic resolution problem. In this paper, we propose a single problem of the app\n",
            "Natural language recognition in the grounding classification of the state-of-the-art to selifies a simulation of the recent addresses the loss of information and robust selection and computational relocation to an analyzing the orthogonal distribution of the automatically management, and sample to th\n",
            "Natural language of the sensitivity of a particula the shortcoming of neural networks, and textual renerators to the benefit problem of software classification based in the recent  not recognition and another developing complexity from the problem of solving analysis the research instent of the probl\n",
            "Natural language estimation using steps of image and convolutional neural networks ; We propose an overall and needed optimization of the article image color accuracy of the fact that are training and industries and a supervised learning method for the activity system that can a label of computer vis\n",
            "Natural language methodology and its linearity construction in deep learning algorithms for automatically introducing statistical models in the  of the vector is to selected showing tool and an integerable approach to learn the computer vision ; We propose a novel constraint to the problem of lower t\n",
            "Natural language representation in one of the source of the autononomous model tasks in computer sparse scales are consistent in a component optimization of the target of convex classical selection and wellows that computer therefore such as the problem of components and possibly analysis ; The probl\n",
            "Natural language processing of marme deporting recognition and subspaces with manifold extraction ; We propose a text data is an attribute to the problem. Our key task accurate in particular, we propose an information in the original model with the possible strategy, and the expert or correlation of \n",
            "Natural language of component correspondences are not better instance in the word probabilistic complexity and collecting expension of the controls in  the problem of continuous range of the art of computational  normalization and experiments to the  normalization to the another or interaction and of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert np.mean(train_history[:10], axis=0)[1] > np.mean(train_history[-10:], axis=0)[1], \"The model didn't converge.\"\n",
        "for i in range(10):\n",
        "    print(generate(rnn_model, 'Transformer', temperature=0.6, max_len=300))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJWNYMjN-OKX",
        "outputId": "c67e848a-6e76-4c3a-afd7-e1df82b362a7"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer-construction of computer optimization of a human parameter of interest problems ; The convex optimization and approaches concept to obtain the interaction of a discrete of the contextual content to the feature satisfactory. In this paper we partine the one of the system with speech space \n",
            "Transformer classification models recognition scheme that the problem of described minimization ; We the time algorithm discasted to a contextual processing combination of a seen approximation between a convergence of camera performance. An explain spatially defined and settings of the problem of nat\n",
            "Transformers of the attention of the Treedy Decision for veres of the function of a presence of   recognition ; We consider the results of the classification of a supervised power of an evolutionary processing that the distributional results of these languages or spatial textures in the sequence of c\n",
            "Transformer Variational Supervised Symbol Sorting   Support Description Estimation ; This paper describes the problem of the proposed framework to success the task of interactions from a consistent device on the system can be proposed to the number of computational specifically, we propose a simple c\n",
            "Transformertive Disambiguation:   Analysis of Automatic Programming ; Many optimization is provided the shorthing noisy of the images is a continuous contextual of prediction and interaction in the training splioted study. This paper introduces a probabilistic progress of the training of the state-of\n",
            "Transformer-entropy low-of-the-based context of the problems representation ; In this paper, we propose a specific traphical structure, optimal in the posed control in the enough-based approach to handle the statistical dataset in the manipulation of the problem of analysis of the supervision. In thi\n",
            "Transformered Filtering Sparse Problems ; We present a both activity of the detection of clustering is an important to be used-based corpus deformation to control selections are provided to the same content of the number of a further decomposition. The output involves of artificial neural network and\n",
            "Transformer-Controlled Search for Detection using Bayesian Networks ; In design long to the knowledge neural network (ASs and tensor problems in the linear model of the interaction of expert and much representation. This paper presents a major analysis of model despect learning is to design a combina\n",
            "Transformers and the View of Specifically and   Dynamic Targeting Machine   Processes with Stochastic Learning ; In this paper we propose a novel approach to exploit models for a graph-set of the sequence of source labels with deep convolutional neural networks. This paper introduces a personal frame\n",
            "Transformer: An Hardware Model and Complex Graphs for Combining Accuracy of   Resolution Structure ; The expert system of accurately, it is usually applied to the generalised convolutional neural networks for such mapping can be recommended by scale the people and the devicing the significant subspac\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert np.mean(train_history[:10], axis=0)[1] > np.mean(train_history[-10:], axis=0)[1], \"The model didn't converge.\"\n",
        "for i in range(10):\n",
        "    print(generate(rnn_model, 'Transformer', temperature=0.5, max_len=300))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmpjV7At-bpj",
        "outputId": "08718a23-9d2e-40e7-f43a-8ab9fdbde200"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformertic Image Detection with Sone Search ; Practical and representation of frameworks in many object terms of the performance of the fields of the state-of-the-art of the sequences of such as a composition of the task. However, the generative method for predicting the standard state of images \n",
            "Transformer-scale optimal context of the development of the high-resolution context ; We present a very analysis of the stare feature of the selection problem. We propose a novel term accurate and a classifier that the algorithms in the structured dataset by provide a correlation of a training proble\n",
            "Transformer Face Detection for Constrained Fast Machine Classification ; The experimental optimization of network to the detection of the source to develop an analysis of the computational variational system is introduced by the original approach to learn from a simultaneously research to construct t\n",
            "Transformer-Convolutional Neural Network Models ; We propose that are adversarial attention is a trained feature for the computer vision of the resulting and content of adversarial industries and an interpreting parallel in a single decomposition problem. In this paper, we propose a new model for the\n",
            "Transformer-based Factorization using Word Transformations ; In this paper, we propose a novel approach for the problem of a single form of the content, they are not have become that the computational models in the use of the captured datasets, we propose a novel accuracy of a subsets of the distance\n",
            "Transformers for real-time features for the sequence of sparse methods ; In this paper, we present an intringing support to solve that the system staces and unsupervised generations in a text. The conditioned approach is to consider a widely problem of a context in advanced and real with a large-scal\n",
            "Transformery stored by the State of Resolution of Evolutionary Programming ; The performance of probabilistic interval or computational problems are not only outperformed in a temporal approach to the stochastic problem of the sequence of a significant process of complex complexity. The theoretical d\n",
            "Transformerning structure of an example of manually detection of data models   with a concept of the point of the stabel that can be used ; We present a control of the search algorithm to extract content, and advanced information and the problem of which the system and deconstrained between convergen\n",
            "Transformers of a high-draw for computer vision of the optimal search is to describe a novel towards and algorithm that sometrics the many of the problem of the distribution of the statistical state of all problem in a computational response to provide an important and the developmed of the expert on\n",
            "Transformers of low-rank recognition methods in consistent from present expertation methods ; We propose a simple information of the computer vision problem on the content of the problems without an optimum detection of the analysis, compared to recorded to a single state-of-resolution and the orderi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert np.mean(train_history[:10], axis=0)[1] > np.mean(train_history[-10:], axis=0)[1], \"The model didn't converge.\"\n",
        "for i in range(10):\n",
        "    print(generate(rnn_model, 'Markov chain ', temperature=0.5, max_len=300))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jgd38gMp_CK8",
        "outputId": "cc6f938d-ec58-4298-c4e3-62c11895bb8e"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Markov chain state-of-the-art models ; We present a new information of event and the technique for speech problem in a contextume that the conditional context of a conventional convergence in a target content of the language from the same and estimation for transformations. Etch empirical actions are\n",
            "Markov chain open-task ; The problem of specifically, we propose a novel framework for a set of stochastic and problem using the training of the semantic infular inentifying state of the computer visual scales of variables dataset. The contextual encoder for image synthesis of a single approach to le\n",
            "Markov chain approaches ; Convolutional neural networks have been widely used to a proof of the result of the sequence of the contextural specific and context of an evolutionary tool in dimensionality of $\\mathbb{\\emph{linear) (GOC). This, we propose a novel model which is the development of the stru\n",
            "Markov chain an artificial analysis of the statistical model ; We consider the convolutional neural network that have a simple and alternative term of more analysis and automatic decision problems in a parameter description. In this paper, we propose a novel problem of functional common contexts of t\n",
            "Markov chain rules and computational   structures of a discourse conditional problem ; In this paper, we present a new approach is a set of models and interest in statistical processing techniques for solving the basical segmentation from behavior problems in the possible approaches in the most state\n",
            "Markov chain context of the   the outlier of the prediction of the   partition in the sparse state of   learning and function ; The proposed context of the proportional processing problem is a counterprint of the same policy and algorithm that control or the applications of the state-of-the-resulting\n",
            "Markov chain recognition of the variational color   for seconder framework in the pace of   has been combined on the machine learning   problems ; In this paper we aim to investigate the computational system in a function of probabilistic languages based on the first subspace to the large-scale conte\n",
            "Markov chain a summarient scale   surface of the detection of   extraction of representations for constrained and   approach to the concept of fuzed semantic   reasoning ; Restricted segmentation problems, is the problem of the incorporation of the task of real-world approaches that the contextual pr\n",
            "Markov chain limited settings ; The context-of-the-description of the supervised image segmentation of the most statistical arbitude is the number of an online labeled as a complex model and subjection of the interactions from the shart feature constraints. The discusses of the augmented based on the\n",
            "Markov chain components ; In this paper, we propose a new context oaserver of the original complexity of many researchers to a significant constraint of the matching setting is presented as a single model of researchers and continuous sensitivity. The constraint the construction of model to the prove\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert np.mean(train_history[:10], axis=0)[1] > np.mean(train_history[-10:], axis=0)[1], \"The model didn't converge.\"\n",
        "for i in range(10):\n",
        "    print(generate(rnn_model, temperature=0.2, max_len=500))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAwIaPTzxgs2",
        "outputId": "7f075a4c-950b-4e5f-ed3e-5ccc085b2a3b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A Convolutional Neural Network for Computation ; We propose a new approach to the application of the art of the analysis of the approximation of the problem of the problem of a set of the approach to the algorithm for the state-of-the-art state-of-the-art content of the problem of the problem of the set of the approach to consider the problem of the target structure of the statistical problem. In this paper, we propose a novel method for a set of the computational structure of the set of the pro\n",
            " A Convonnt Spatial Control of the Inference for Convex   Distribution of How Semantic Machine Learning ; In this paper, we propose a new method for the interpretation of the semantic data and a single content of the problem of the structure of the problem of the standard set of the approach to the context of the are and application of the area of the controllers to solve the strong of the art set of the set of the problem of the optimization of the computational system to the transform of the an\n",
            " A Control of Semantic Methods for Exploiting Model ; The problem of interest and the problem of the problem in the optimization of the problem of sensitive information and set of the context of the computational formal interestion by a single set of the state-of-the-art approach to a computational feature of the problem of the different space of the article of the set of the sequence of a simple of the problem of the problem of a set of the set of the state-of-the-art context of the article and \n",
            " A Convolutional Approach for Convolutional Neural Networks ; This paper proposes a novel approach to a simple transformation of the problem of the problem of a set of the system of the particular convolution and a set of the different problem. The problem of the such as a sparse probabilistic approach to the state of the problem of the computation of the context of the sensor setting is a computation of the optimal problem. We propose a novel approach to a signal task of the set of the control i\n",
            " A Convexity of the State-of-the-art Convergence of Expert ; The problem of the approach to as the problem of the search of the search of the set of the problem of a set of the setting of the distribution of the state-of-the-art contribution. In this paper, we propose a non-linear component of the problem is a computational formalism of the set of the set of some constraints of the computational conditions of the problem of a set of the accuracy of the state of the context of the computational mo\n",
            " A Convolutional Neural Network for Machine Learning ; We propose a new approach to a large set of the state-of-the-art and the problem of the set of the art and the constraint of the problem of the context of a simple network to constraint the state-of-the-art detection and accomproving the state of the state-of-the-art and in the problem of a graph context. The problem of the model that are compared to the problem of the context of the approach to a specifical context of the approach to the con\n",
            " A Convolutional Neural Network for Convolutional Neural Networks ; We propose a novel approach to a single semantic component of the matrix of the state of the set of the problem of the adaptation of the semantic structure of the stale of the context of the problem of the state-of-the-art and the content of the semantics of the problem. The restriction of the art of the approach is a convexity of the state-of-the algorithms to recognize the same set of the training of the network to the speech a\n",
            " A Search of Statistical Medical Context of Semi-Supervised   Deep Learning ; The problem of state-of-the-art probabilities is a set of the set of the set of the first state of the model that the operator of the problem of the problem of the artificial method for the experiment of the computational structure of the set of the context of the approach to solve the set of the context of the sensitive problem of the context of the semantic control problem in the state-of-the-art probability and const\n",
            " A Convolutional Neural Network for Large Search Analysis ; In this paper, we propose a novel approach to a computational strategy of the set of the complexity of the problem of the data of the approach to the simple of the set of the algorithm is the subspace of the oengingical and computational signals to the interest of the artificial data are applied to a state-of-the-art and semantic set of the systems. The problem of the problem of the problem of the art controlue to achieve the problem of \n",
            " A Computational Contingous Approach for Semantic Programming ; We propose a novel method for the problem of a set of the set of the problem of the structure of the problem of a set of the semantic problem of the problem of the most recognition and access of the structure of the sensitive and a set of the set of the set of the state-of-the-art classification of the stage of the analysis. The proposed approach to the problem of the state-of-the-art a deep neural network for the concept of the fina\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert np.mean(train_history[:10], axis=0)[1] > np.mean(train_history[-10:], axis=0)[1], \"The model didn't converge.\"\n",
        "for i in range(10):\n",
        "    print(generate(rnn_model, temperature=1, max_len=500))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uGDbZy6y5Ms",
        "outputId": "738a8470-41ba-466f-9f29-80502b9bcc5e"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Optified Location and Lolar Roword Analysis, Randov Quadratory (L Nearest   Geongsts And Intelligent Human Captures ; There categories in the first with non-side, although recognition to the veriete text of drawn and coordinately poperious penaltic wavelets which appropriates and canonused symbetchilated, capture than new method-bounded point intentio\n",
            "\n",
            " Simplears (QST) via Hu\n",
            "\n",
            " Wumps: A Neural Enviring Upon Rule Kernel TextS on Conditional IIT-P/IFDIC ; We refer\n",
            "\n",
            " Measury by Servicipation Slow for Several Problem ; Multi-agent model to free supervised level and monitoring image dense spiking, regademihic interest as the weak dimensionality referenc\n",
            "\n",
            " Troughnes for Combinatorial Localization Account in Texts of 3D ; A mapping of video experment combine of influence conventional and quantitity and bord outlinual analysis. Necroider much anotherior in this problem users, the runtry to vitoha the artions for existing parametrics, and utilizes the popular wirst, weneral applications, which couples, each intrusion stace tasked in the System network stMb is moved to doweriotes researchers with high-approach. They some pattern restence have been oft\n",
            " Braid Learning Element Has and Sentiment Temporalization ; To role similarity precision is a tree-ideal correlation in 2a)y popularity ones. One low-to-ener convolution) throughond underly we train sexment,\n",
            "\n",
            " Guided Actions, and Nonparisons Approximately Including Model-iser-W Speech variational   Sport setting ; Producing most Remotely are explored by a singlex extension that assisfed to probuse a distributed learning performance in most many fitness observable family extracting subver and consisting systems in whethere\n",
            "\n",
            " Class wfich center in presential models ; Good allows these semantic and a dissignal estimation of HAT region) negword (RGB) to different including words and automatically descriptions important ipic typical influence\n",
            "\n",
            " Offleed F-Bayesian networks for search tasks of   convolutional networks with learning filting emlibies however, hynamical intervals ; This data using the model, esemal specified architectures sized with extent models feasers there developed the last small statistical experiment. This paper  We are effectively distributed with utilities: We present a local search to a data of the do nould instances, with the madeity of linguistic, avaltitates law in donal finite. There is a variables continuous \n",
            " Evolutionary Quality Vector Processing in Recent Architecture ; Not deland, becomes a will large quaction writing approach from meaningfut statement of interact of the bandit assumption and computate statistical tasms. These Prediction (Pes bases other both from miscultiford-cutulations, likelih(interesting microscopy, and the proposed typical learning. Therefores several syntems an abstract of models, a rang that adapt\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRpf5rIdIoU8"
      },
      "source": [
        "### Alternative sampling strategies (1 point)\n",
        "\n",
        "So far we've sampled tokens from the model in proportion with their probability.\n",
        "However, this approach can sometimes generate nonsense words due to the fact that softmax probabilities of these words are never exactly zero. This issue can be somewhat mitigated with sampling temperature, but low temperature harms sampling diversity. Can we remove the nonsense words without sacrificing diversity? __Yes, we can!__ But it takes a different sampling strategy.\n",
        "\n",
        "__Top-k sampling:__ on each step, sample the next token from __k most likely__ candidates from the language model.\n",
        "\n",
        "Suppose $k=3$ and the token probabilities are $p=[0.1, 0.35, 0.05, 0.2, 0.3]$. You first need to select $k$ most likely words and set the probability of the rest to zero: $\\hat p=[0.0, 0.35, 0.0, 0.2, 0.3]$ and re-normalize: \n",
        "$p^*\\approx[0.0, 0.412, 0.0, 0.235, 0.353]$.\n",
        "\n",
        "__Nucleus sampling:__ similar to top-k sampling, but this time we select $k$ dynamically. In nucleous sampling, we sample from top-__N%__ fraction of the probability mass.\n",
        "\n",
        "Using the same  $p=[0.1, 0.35, 0.05, 0.2, 0.3]$ and nucleous N=0.9, the nucleous words consist of:\n",
        "1. most likely token $w_2$, because $p(w_2) < N$\n",
        "2. second most likely token $w_5$, $p(w_2) + p(w_5) = 0.65 < N$\n",
        "3. third most likely token $w_4$ because $p(w_2) + p(w_5) + p(w_4) = 0.85 < N$\n",
        "\n",
        "And thats it, because the next most likely word would overflow: $p(w_2) + p(w_5) + p(w_4) + p(w_1) = 0.95 > N$.\n",
        "\n",
        "After you've selected the nucleous words, you need to re-normalize them as in top-k sampling and generate the next token.\n",
        "\n",
        "__Your task__ is to implement nucleus sampling variant and see if its any good."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "Z7UgNuyFIoU9"
      },
      "outputs": [],
      "source": [
        "def generate_nucleus(model, prefix=BOS, nucleus=0.9, max_len=100):\n",
        "    \"\"\"\n",
        "    Generate a sequence with nucleous sampling\n",
        "    :param prefix: a string containing space-separated previous tokens\n",
        "    :param nucleus: N from the formulae above, N \\in [0, 1]\n",
        "    :param max_len: generate sequences with at most this many tokens, including prefix\n",
        "    \n",
        "    :note: make sure that nucleous always contains at least one word, even if p(w*) > nucleus\n",
        "    \n",
        "    \"\"\"\n",
        "    while True:\n",
        "        token_probs = model.get_possible_next_tokens(prefix)\n",
        "        tokens, probs = zip(*token_probs.items())\n",
        "        probs = np.array(probs)\n",
        "\n",
        "        sorted_probs_indices = np.argsort(probs)[::-1]\n",
        "        sorted_probs = probs[sorted_probs_indices]\n",
        "        # choose only N % highest probs, discard rest\n",
        "    \n",
        "        cumulative_probs = np.cumsum(sorted_probs)\n",
        "\n",
        "        remove_sorted_indices = cumulative_probs >= nucleus\n",
        "        # add one extra word to make sure \n",
        "        # we cover at least one word even if p(w*) > nucleus\n",
        "        remove_sorted_indices[..., 1:] = np.copy(remove_sorted_indices[..., :-1]) # last True -> False\n",
        "        remove_sorted_indices[..., 0] = 0 # if all of them were True, make at least first one word to False \n",
        "\n",
        "        indices_to_remove = sorted_probs_indices[remove_sorted_indices]\n",
        "        probs[indices_to_remove] = -float('Inf')\n",
        "\n",
        "        # compute softmax over changed array of probabilities\n",
        "        probs = np.exp(probs)\n",
        "        probs /= np.sum(probs)\n",
        "\n",
        "        next_token = np.random.choice(tokens, p=probs)\n",
        "        prefix += next_token\n",
        "        if next_token == EOS or len(prefix) > max_len: break\n",
        "\n",
        "    return prefix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "v4dVgU5jIoU9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3a2c874-9e80-457c-df36-b50d520e9f82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nucleus sampling with CNN model\n",
            "\n",
            "Nucleus value = 0.5\n",
            " Complexity of Structured Speech Recognition and More Probabilistic Models ; The constraint that are experiently for the maps of a popular process of properties in the discourse of problems with a probabilistic detection of convolutional neural networks are not be introduced to the segmentation of th\n",
            " Learning Segmentation of Segmentation in Social Statistics ; We consider the most context of the approach to compute the field of machine learning in computational experts of a probabilistic descriptor in a sequence of computer vision problems. The mapping of stochastic analysis is compared to such \n",
            " Learning Sparse Analysis for Sparse Sparsing State Segmentation ; The semantic system is a probability of the research to the sentence of point structures of such problems. The approach is a complete set of a control that are increasingly specifically applied to the relational sparse and experimenta\n",
            " Tool in an autonomous of social methods ; We propose a social method to entire process of the context of a point state of the problem of detection and implementations are the distribution of the problem. The state of speech recognition is a method to a proper to context on a computational sparse of \n",
            " An Interest of Artificial Learning with A Complex Sensitive Approach ; We propose a theoretical order to produce a computational construction for computational datasets in the context of an efficity to analyze the context structure. In this paper, we propose a more computational model of a problem o\n",
            "\n",
            "Nucleus value = 0.6\n",
            " Sentence Decomposition for Multiple Markov Analysis ; Algorithm that performance can also develop support to state of computer ordering approaches for the transformation that serves of a different class a point particular distribution of the exploration and such a linear theoretical strong to provid\n",
            " Multi-Dimensional Distribution with Events with Computational Complexity ; The same state-of-the-art data is proposed to derive simultaneously extensions are used in model selection problem (resears. Here introduces in constructing the proposed performance of many constraints that the posterior from\n",
            " The Extraction with Point Decision Retrieval for Extraction ;ou Manifold Memory for   Surference for Speech Computational Completion ; In this paper, we propose a fore in a distributed search to be also considered by such and accuracy for machine learning model and person which is contextually invol\n",
            " Deep Neural Networks for Rolernative Approaches ; In this paper, we propose a sequence of parallelisation algorithms that provides a majority of complex superior detection and an input approach for experimental approaches in statistical point models of distributed and the analysis of most provides a\n",
            " Essumption of Graph and Content-or Decision ; The supervised performance of the function algorithm is contrast of speech studies and allorithms with detecting patterns that allow an increase and sentence in more to the target are enabled in the original structure for machine learning. Here, we propo\n",
            "\n",
            "Nucleus value = 0.7\n",
            " Maximum Clustering (LDMM) by Bayesian Machine Sparse Supervised Single Axgression   Reconstructions ; Monotonous computer speech algorithms (capacle (AUSIM has a simple important tool in order summarizations. We study medical energy context and effective are provided by carronotonic analysis is more\n",
            " Automatic Estimation and Montological Transfer Image Marker Decomposition ; In this paper introduces a neural network to internales computational interaction for dependencies of prediction scale algorithms are computationally developed to model such as process studies. We propose that several encrep\n",
            " Sparse Learning (PAEGS and Son-Approximation Mechanisms ; The representation that they serves a description of component algorithms to the distance is instead, they so a multi-dimensional constraint information problem. The image is provided in the automatic disease, are estimated in constraint sens\n",
            " Optimization and a Set of approximation ; We consider a major analysis of an instances of deep neural optimization or facial context standard maximization that also rely as object classification. We present a novel toor sequence is to extract the complex training processes which developed the constr\n",
            " Construction of Learning Compression Experimental Strategies ; We study the computer random interest from context algorithms are derived from devision methods have been proved for pursourcy by different more specific studies. We present an improved topic and manifold are about and different image tr\n",
            "\n",
            "Nucleus value = 0.8\n",
            " Extensional Nodes beide Regression Approaches ; In additional corpusses products have active memory as fality analysis have treated efficiency, our continuent formulae, search domains, interesting efficiently and optimizes of a formalism well-known functions. Such stochastic features, as suchay, whe\n",
            " Stochastic Properties of Inference Adversarial Learning with Causal Correspondences of The Study   for The Implication for Poly-Also Memory   Features ; We adapt the cloud topolocical segmentation of our resolution interactions are computationally interesting to experts the taple-persistent variable\n",
            " Order-Non-Position Association Using Features   Camera   only Tood on Segmentation ; This artificis and representations approach to perform data piecewise modeling is possible in the signers of the can interaction testing. Forecesting process called recently, hierarchical examples called targeting t\n",
            " Re the Particularly Temporal Exploiting Temporal Routeristodic Evolution of Development Process in Reasoning ; Measure of different intelligent reconstructions for only user each algorithm, it significantly, where algorithms can apply domains competing many of image differences in ontology. On the c\n",
            " Fuzzy a Model Evolutionary Estimation Using a New Identification Model for Local Normal   Component Posteriors   Model Representation and Many Respect Positions ; Commard spatial costs, increasingly object variables with the time-point complex concepts. Sequence learning methods for component signif\n",
            "\n",
            "Nucleus value = 0.9\n",
            " Termal-automatic Bighorteating Opper-Telput Sum-time Po Optical Lickpared   Veribrability Surfaces in Max Synthesis: and   How-Main-Suronocy-sumble Multiple Algorithm where Lexical Scordalities ; Oun fingually, hoch learned uptass, fall-effectively optical storacating dimensions been ite. We prevely\n",
            " Relayions Cressour Generative field scalarly Paportise Foleriated Actions in   Takes Significantly, Binary Elgebues is LSI Aractive ; We enables collaboratically introductional foeld corefards bayed essential poor suffers, do performance of case, to medicate recurrent models. It combine these multi-\n",
            " Global Dongs (Net Digidorighisms Training Time-vocalization Settence for Flant-Vayious Person, Fast Futon Images is Vehy Notcomogy Even Denoireneity of Tike Estimation   Bounds of Data   Gramsmest Bayesian Environments   from Gradiciant Programs of Accurately, Gaussian Training Returnes with Trans  \n",
            " Subtoul data matterized elitivinian functions using generated perturbing mutear ophy medamating changes ; Cloud-tree-code by very intensity data driven by more graphs when each catch perspects retrieval to any well-two insentiated arguments. Frow spocks simule is resters for rark, whows whose closit\n",
            " Using With Loorseppect Perperts to   Barediens from Vulnering Object and testure Group Models   With Impurtature Scenes ; Surneted A robot optimisation from nave-goal-calculus, it plipsizing for buttle hom to efficient interactive wayesian machines. Approximating simpler, normal set regression, main\n",
            "\n"
          ]
        }
      ],
      "source": [
        "nucleus_values = (0.5, 0.6, 0.7, 0.8, 0.9)\n",
        "\n",
        "print('Nucleus sampling with CNN model\\n')\n",
        "\n",
        "for nucleus in nucleus_values:\n",
        "    print('Nucleus value = {}'.format(nucleus))\n",
        "    for i in range(5):\n",
        "        print(generate_nucleus(rnn_model, nucleus=nucleus, max_len=300))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwrKcsTPIoU-"
      },
      "source": [
        "### Bonus quest I: Beam Search (2 pts incl. samples)\n",
        "\n",
        "At times, you don't really want the model to generate diverse outputs as much as you want a __single most likely hypothesis.__ A single best translation, most likely continuation of the search query given prefix, etc. Except, you can't get it. \n",
        "\n",
        "In order to find the exact most likely sequence containing 10 tokens, you would need to enumerate all $|V|^{10}$ possible hypotheses. In practice, 9 times out of 10 you will instead find an approximate most likely output using __beam search__.\n",
        "\n",
        "Here's how it works:\n",
        "0. Initial `beam` = [prefix], max beam_size = k\n",
        "1. for T steps:\n",
        "2. ` ... ` generate all possible next tokens for all hypotheses in beam, formulate `len(beam) * len(vocab)` candidates\n",
        "3. ` ... ` select beam_size best for all candidates as new `beam`\n",
        "4. Select best hypothesis (-es?) from beam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "OLTJ3G8JIoU-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "05ae8ea5-88d3-4361-ce13-b05d8c8c7425"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<!DOCTYPE html>\n",
              "<html lang=\"en\">\n",
              "    <head>\n",
              "        <meta charset=\"utf-8\">\n",
              "        <title>Bokeh Plot</title>\n",
              "        \n",
              "<link rel=\"stylesheet\" href=\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.7.min.css\" type=\"text/css\" />\n",
              "        \n",
              "<script type=\"text/javascript\" src=\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.7.min.js\"></script>\n",
              "<script type=\"text/javascript\">\n",
              "    Bokeh.set_log_level(\"info\");\n",
              "</script>\n",
              "        <style>\n",
              "          html {\n",
              "            width: 100%;\n",
              "            height: 100%;\n",
              "          }\n",
              "          body {\n",
              "            width: 90%;\n",
              "            height: 100%;\n",
              "            margin: auto;\n",
              "          }\n",
              "        </style>\n",
              "    </head>\n",
              "    <body>\n",
              "        \n",
              "        <div class=\"bk-root\">\n",
              "            <div class=\"bk-plotdiv\" id=\"ff8c3f31-952d-4c2f-8b58-13e7cec51b58\"></div>\n",
              "        </div>\n",
              "        \n",
              "        <script type=\"text/javascript\">\n",
              "            (function() {\n",
              "          var fn = function() {\n",
              "            Bokeh.safely(function() {\n",
              "              var docs_json = {\"ba84f797-d201-498d-a731-5adafa5447b7\":{\"roots\":{\"references\":[{\"attributes\":{\"plot\":null,\"text\":\"Beam search\"},\"id\":\"5af81591-5793-4721-a459-e0a4ca700855\",\"type\":\"Title\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"line_width\":{\"field\":\"line_width\"},\"size\":{\"units\":\"screen\",\"value\":24},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"43ae4eb3-d229-4335-a758-a5d18149bd65\",\"type\":\"Circle\"},{\"attributes\":{\"bounds\":[-10.0,20.0],\"callback\":null,\"end\":12,\"js_property_callbacks\":{\"change:end\":[{\"id\":\"eab13fb4-9405-4d14-8bf6-46c6f698b4bb\",\"type\":\"CustomJS\"}]},\"start\":-1},\"id\":\"14e9976c-9458-4bce-be96-da2f3c304cec\",\"type\":\"Range1d\"},{\"attributes\":{\"source\":{\"id\":\"53346d3c-7b75-4689-95d1-c395b23fa5b8\",\"type\":\"ColumnDataSource\"}},\"id\":\"2e9dcf01-3f79-4337-b7dd-26f0525180b9\",\"type\":\"CDSView\"},{\"attributes\":{\"data_source\":{\"id\":\"49a8987d-d307-4797-9fae-8ec771f76b48\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"08bbcb52-00b5-4803-9e7a-9252db09708d\",\"type\":\"Text\"},\"hover_glyph\":null,\"muted_glyph\":null,\"name\":\"tokens\",\"nonselection_glyph\":{\"id\":\"3621fd59-3cb3-4305-8802-6f7f75a23fd6\",\"type\":\"Text\"},\"selection_glyph\":null,\"view\":{\"id\":\"ddf587aa-019f-43a1-af8c-52144e732785\",\"type\":\"CDSView\"}},\"id\":\"a2ab1853-0966-4519-874f-956f801f1c72\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"id\",\"parent_id\",\"children_ids\",\"is_best\",\"depth\",\"hypo_i\",\"token\",\"token_id\",\"x\",\"y\",\"circle_fill_color\",\"line_color\",\"line_width\",\"edge_xx\",\"edge_yy\",\"token_text\",\"token_font_size\",\"hypo_i_text\",\"hypo_i_offset\",\"_on_hover_token\",\"_on_hover_token_id\",\"_on_hover_score\"],\"data\":{\"_on_hover_score\":[\"-4.7282\",\"-4.2071\",\"-4.8782\",\"-1.3577\",\"-4.0420\",\"-4.2097\",\"-4.5624\",\"-3.2410\",\"-6.9225\",\"-4.6717\",\"-2.8497\",\"-3.8023\",\"-4.1460\",\"-3.6443\",\"-4.3135\",\"-4.7070\",\"-3.1373\",\"-4.5105\",\"-6.1526\",\"-3.0974\",\"-3.7421\",\"-3.4956\",\"0.0000\",\"-5.2694\",\"-4.4104\",\"-6.1752\",\"-3.7617\",\"-4.6281\",\"-6.5626\",\"-3.7175\",\"-0.7384\",\"-4.5439\",\"-5.2967\",\"-3.6831\",\"-5.2153\",\"-3.7830\",\"-3.1335\",\"-0.8716\",\"-3.7312\",\"-3.8895\",\"-6.5984\"],\"_on_hover_token\":[\"_EOS_\",\"\\u043f\\u0440\\u0430\\u0432\\u0438\\u0442\\u0435\\u043b\\u044c\\u0441\\u0442\\u0432\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0439\",\",\",\"\\u0443\\u043f\\u043b\\u0430\\u0442\\u044b\",\"\\u043e\\u0442\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u0432\",\"_EOS_\",\"\\u0432\\u044b\\u0441\\u0442\\u0443\\u043f\\u0430\\u0435\\u0442\",\"_EOS_\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\\u0435\",\"\\u043d\\u0430\\u043b\\u043e\\u0433\\u043e\\u0432\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043a\\u0438\",\"\\u043d\\u0435\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\",\"<empty>\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\".\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u043d\\u0435\",\"\\u043e\\u0442\\u0432\\u0435\\u0442\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u0438\",\"\\u0437\\u0430\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\",\",\",\"\\u043e\\u0442\",\"`\\u0430\\u044e\\u0449\\u0438\\u0435\",\"_EOS_\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0430\\u043b\",\"\\u043d\\u0435\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0435\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u043f\\u0440\\u0435\\u0442\\u0435\\u043d\\u0437\\u0438\\u0439\"],\"_on_hover_token_id\":[1,780,25,2482,15356,2482,1879,3,17084,25,13862,5,1,7691,1,15356,25,15328,4255,11685,11,3487,-1,21342,15356,4,21342,11,1781,30,3622,3,25,1078,1,5093,26478,11,2451,13862,22624],\"children_ids\":[[],[[2,3]],[[10,3],[10,2],[10,1],[10,0]],[[4,3],[4,1],[4,0]],[[9,3],[9,1],[9,0]],[],[],[[6,1]],[],[[8,2]],[[5,0]],[[2,2]],[],[[4,2]],[],[[7,2]],[[6,2],[6,0]],[],[],[[5,1]],[[2,1]],[],[[1,3],[1,2],[1,1],[1,0]],[[9,2]],[[8,1]],[],[[7,3],[7,1]],[],[],[[5,2]],[[2,0]],[],[],[[8,3],[8,0]],[],[[7,0]],[],[[3,3],[3,2],[3,1],[3,0]],[[5,3]],[[6,3]],[]],\"circle_fill_color\":[\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\"],\"depth\":[7,1,9,3,8,2,6,5,10,7,4,1,9,3,8,6,5,2,10,4,1,3,0,8,7,9,6,2,10,4,1,5,8,7,9,6,3,2,4,5,10],\"edge_xx\":[[6.0,7.0],[0.0,1.0],[8.0,9.0],[2.0,3.0],[7.0,8.0],[1.0,2.0],[5.0,6.0],[4.0,5.0],[9.0,10.0],[6.0,7.0],[3.0,4.0],[0.0,1.0],[8.0,9.0],[2.0,3.0],[7.0,8.0],[5.0,6.0],[4.0,5.0],[1.0,2.0],[9.0,10.0],[3.0,4.0],[0.0,1.0],[2.0,3.0],[0.0,0.0],[7.0,8.0],[6.0,7.0],[8.0,9.0],[5.0,6.0],[1.0,2.0],[9.0,10.0],[3.0,4.0],[0.0,1.0],[4.0,5.0],[7.0,8.0],[6.0,7.0],[8.0,9.0],[5.0,6.0],[2.0,3.0],[1.0,2.0],[3.0,4.0],[4.0,5.0],[9.0,10.0]],\"edge_yy\":[[4.833333333333333,4.333333333333333],[0.0,-1.5],[3.333333333333333,3.333333333333333],[1.5,3.0],[2.833333333333333,3.333333333333333],[0.5,0.5],[4.333333333333333,3.833333333333333],[2.833333333333333,2.833333333333333],[3.333333333333333,1.833333333333333],[0.0,0.0],[3.0,4.333333333333333],[0.0,-0.5],[3.333333333333333,4.333333333333333],[1.5,0.0],[5.333333333333333,5.333333333333333],[0.0,0.0],[4.333333333333333,4.333333333333333],[-0.5,-0.5],[3.333333333333333,4.833333333333333],[3.0,2.833333333333333],[0.0,0.5],[1.5,1.0],[0.0,0.0],[0.0,0.0],[4.833333333333333,5.333333333333333],[3.333333333333333,2.333333333333333],[4.333333333333333,4.833333333333333],[-1.5,-1.5],[3.333333333333333,3.833333333333333],[0.0,0.0],[0.0,1.5],[1.8333333333333333,1.8333333333333333],[2.833333333333333,2.333333333333333],[2.833333333333333,2.833333333333333],[0.0,0.0],[2.833333333333333,2.833333333333333],[1.5,2.0],[1.5,1.5],[3.0,1.8333333333333333],[0.0,0.0],[3.333333333333333,2.833333333333333]],\"hypo_i\":[3,3,1,0,0,1,2,1,3,2,0,2,0,3,1,3,0,2,0,1,1,2,0,2,1,3,0,3,1,2,0,3,3,0,2,1,1,0,3,2,2],\"hypo_i_offset\":[-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8],\"hypo_i_text\":[\"#3\",\"#3\",\"#1\",\"#0\",\"#0\",\"#1\",\"#2\",\"#1\",\"#3\",\"#2\",\"#0\",\"#2\",\"#0\",\"#3\",\"#1\",\"#3\",\"#0\",\"#2\",\"#0\",\"#1\",\"#1\",\"#2\",\"#0\",\"#2\",\"#1\",\"#3\",\"#0\",\"#3\",\"#1\",\"#2\",\"#0\",\"#3\",\"#3\",\"#0\",\"#2\",\"#1\",\"#1\",\"#0\",\"#3\",\"#2\",\"#2\"],\"id\":[[7,3],[1,3],[9,1],[3,0],[8,0],[2,1],[6,2],[5,1],[10,3],[7,2],[4,0],[1,2],[9,0],[3,3],[8,1],[6,3],[5,0],[2,2],[10,0],[4,1],[1,1],[3,2],[0,0],[8,2],[7,1],[9,3],[6,0],[2,3],[10,1],[4,2],[1,0],[5,3],[8,3],[7,0],[9,2],[6,1],[3,1],[2,0],[4,3],[5,2],[10,2]],\"is_best\":[false,false,false,true,true,false,false,true,false,false,false,false,true,false,false,false,false,false,false,true,false,false,true,false,false,false,false,false,false,false,true,false,false,true,false,true,false,true,false,false,false],\"line_color\":[\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\"],\"line_width\":[1,1,1,3,3,1,1,3,1,1,1,1,3,1,1,1,1,1,1,3,1,1,3,1,1,1,1,1,1,1,3,1,1,3,1,3,1,3,1,1,1],\"parent_id\":[[6,0],[0,0],[8,0],[2,0],[7,0],[1,1],[5,0],[4,1],[9,1],[6,3],[3,0],[0,0],[8,0],[2,0],[7,1],[5,2],[4,0],[1,2],[9,1],[3,0],[0,0],[2,0],[0,0],[7,2],[6,0],[8,0],[5,0],[1,3],[9,1],[3,3],[0,0],[4,3],[7,0],[6,1],[8,2],[5,1],[2,0],[1,0],[3,0],[4,2],[9,1]],\"token\":[\"_EOS_\",\"\\u043f\\u0440\\u0430\\u0432\\u0438\\u0442\\u0435\\u043b\\u044c\\u0441\\u0442\\u0432\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0439\",\",\",\"\\u0443\\u043f\\u043b\\u0430\\u0442\\u044b\",\"\\u043e\\u0442\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u0432\",\"_EOS_\",\"\\u0432\\u044b\\u0441\\u0442\\u0443\\u043f\\u0430\\u0435\\u0442\",\"_EOS_\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\\u0435\",\"\\u043d\\u0430\\u043b\\u043e\\u0433\\u043e\\u0432\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043a\\u0438\",\"\\u043d\\u0435\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\",\"<empty>\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\".\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u043d\\u0435\",\"\\u043e\\u0442\\u0432\\u0435\\u0442\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u0438\",\"\\u0437\\u0430\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\",\",\",\"\\u043e\\u0442\",\"`\\u0430\\u044e\\u0449\\u0438\\u0435\",\"_EOS_\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0430\\u043b\",\"\\u043d\\u0435\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0435\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u043f\\u0440\\u0435\\u0442\\u0435\\u043d\\u0437\\u0438\\u0439\"],\"token_font_size\":[\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\"],\"token_id\":[1,780,25,2482,15356,2482,1879,3,17084,25,13862,5,1,7691,1,15356,25,15328,4255,11685,11,3487,-1,21342,15356,4,21342,11,1781,30,3622,3,25,1078,1,5093,26478,11,2451,13862,22624],\"token_text\":[\"_EOS_\",\"\\u043f\\u0440\\u0430\\u0432\\u0438\\u0442\\u0435\\u043b\\u044c\\u0441\\u0442\\u0432\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0439\",\",\",\"\\u0443\\u043f\\u043b\\u0430\\u0442\\u044b\",\"\\u043e\\u0442\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u0432\",\"_EOS_\",\"\\u0432\\u044b\\u0441\\u0442\\u0443\\u043f\\u0430\\u0435\\u0442\",\"_EOS_\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\\u0435\",\"\\u043d\\u0430\\u043b\\u043e\\u0433\\u043e\\u0432\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043a\\u0438\",\"\\u043d\\u0435\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\",\"<empty>\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\".\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u043d\\u0435\",\"\\u043e\\u0442\\u0432\\u0435\\u0442\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u0438\",\"\\u0437\\u0430\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\",\",\",\"\\u043e\\u0442\",\"`\\u0430\\u044e\\u0449\\u0438\\u0435\",\"_EOS_\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0430\\u043b\",\"\\u043d\\u0435\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0435\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u043f\\u0440\\u0435\\u0442\\u0435\\u043d\\u0437\\u0438\\u0439\"],\"x\":[7.0,1.0,9.0,3.0,8.0,2.0,6.0,5.0,10.0,7.0,4.0,1.0,9.0,3.0,8.0,6.0,5.0,2.0,10.0,4.0,1.0,3.0,0.0,8.0,7.0,9.0,6.0,2.0,10.0,4.0,1.0,5.0,8.0,7.0,9.0,6.0,3.0,2.0,4.0,5.0,10.0],\"y\":[4.333333333333333,-1.5,3.333333333333333,3.0,3.333333333333333,0.5,3.833333333333333,2.833333333333333,1.833333333333333,0.0,4.333333333333333,-0.5,4.333333333333333,0.0,5.333333333333333,0.0,4.333333333333333,-0.5,4.833333333333333,2.833333333333333,0.5,1.0,0.0,0.0,5.333333333333333,2.333333333333333,4.833333333333333,-1.5,3.833333333333333,0.0,1.5,1.8333333333333333,2.333333333333333,2.833333333333333,0.0,2.833333333333333,2.0,1.5,1.8333333333333333,0.0,2.833333333333333]}},\"id\":\"53346d3c-7b75-4689-95d1-c395b23fa5b8\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_scroll\":{\"id\":\"09035c87-81aa-4e21-b586-281eec2ac195\",\"type\":\"WheelZoomTool\"},\"active_tap\":\"auto\",\"tools\":[{\"id\":\"ca8bf926-99b6-41f8-aa58-31f717609a35\",\"type\":\"PanTool\"},{\"id\":\"0170b055-6020-406e-b0a6-9a42a9bb2816\",\"type\":\"BoxZoomTool\"},{\"id\":\"09035c87-81aa-4e21-b586-281eec2ac195\",\"type\":\"WheelZoomTool\"},{\"id\":\"e4e04654-061b-4633-a71e-1c9003f6bda9\",\"type\":\"WheelZoomTool\"},{\"id\":\"7d459b4a-fdec-48b5-bc95-828fb94c3c34\",\"type\":\"SaveTool\"},{\"id\":\"cacbfd1a-7e88-471a-a3c1-bc588c6af09c\",\"type\":\"ResetTool\"},{\"id\":\"7a2caa64-375a-466a-8caf-c9f17555b42d\",\"type\":\"HoverTool\"}]},\"id\":\"58f26099-07c1-4f46-ba5f-eec6d745dd12\",\"type\":\"Toolbar\"},{\"attributes\":{\"data_source\":{\"id\":\"53346d3c-7b75-4689-95d1-c395b23fa5b8\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"dd1456df-a3c2-4dcc-a8aa-769ac1a8a3b2\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"name\":\"vertices\",\"nonselection_glyph\":{\"id\":\"43ae4eb3-d229-4335-a758-a5d18149bd65\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"2e9dcf01-3f79-4337-b7dd-26f0525180b9\",\"type\":\"CDSView\"}},\"id\":\"eb321d59-0d82-41f4-966f-a84597d90781\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"source\":{\"id\":\"49a8987d-d307-4797-9fae-8ec771f76b48\",\"type\":\"ColumnDataSource\"}},\"id\":\"b0f21dbb-aa92-495c-9a38-9881564a3daa\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"a3e71058-805b-4461-9009-2e76107d0dad\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"eabb5e31-0740-4852-bcac-887155d4c0fc\",\"type\":\"LinearScale\"},{\"attributes\":{\"source\":{\"id\":\"49a8987d-d307-4797-9fae-8ec771f76b48\",\"type\":\"ColumnDataSource\"}},\"id\":\"ddf587aa-019f-43a1-af8c-52144e732785\",\"type\":\"CDSView\"},{\"attributes\":{\"axis_label\":\"decoding step (aka output length)\",\"formatter\":{\"id\":\"f3857a7d-2e4f-4ce0-b897-cec5490c8436\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"327205fd-12df-449f-9614-e6816136cb23\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"859d93b7-acac-4853-be74-6f20da679d8b\",\"type\":\"SingleIntervalTicker\"}},\"id\":\"84db9569-9f28-4a47-82c8-bfa01402aeb6\",\"type\":\"LinearAxis\"},{\"attributes\":{\"text\":{\"field\":\"token_text\"},\"text_align\":\"center\",\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"field\":\"token_font_size\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"},\"y_offset\":{\"value\":-10}},\"id\":\"d4854422-d3bc-43fa-b7c0-a791e40b8a05\",\"type\":\"Text\"},{\"attributes\":{},\"id\":\"ca8bf926-99b6-41f8-aa58-31f717609a35\",\"type\":\"PanTool\"},{\"attributes\":{\"source\":{\"id\":\"53346d3c-7b75-4689-95d1-c395b23fa5b8\",\"type\":\"ColumnDataSource\"}},\"id\":\"a931ac6d-24c6-489e-873f-3785a4f5cec4\",\"type\":\"CDSView\"},{\"attributes\":{\"bounds\":[-11.5,15.333333333333332],\"callback\":null,\"range_padding\":1.0,\"range_padding_units\":\"absolute\"},\"id\":\"b765029c-d346-49e8-b769-3c8155e12984\",\"type\":\"DataRange1d\"},{\"attributes\":{\"interval\":1},\"id\":\"859d93b7-acac-4853-be74-6f20da679d8b\",\"type\":\"SingleIntervalTicker\"},{\"attributes\":{\"text\":{\"field\":\"hypo_i_text\"},\"text_align\":\"center\",\"text_baseline\":\"middle\",\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"value\":\"12px\"},\"text_font_style\":\"bold\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"323b0eea-1d7f-4a0d-84fe-74a9d6ac6b56\",\"type\":\"Text\"},{\"attributes\":{\"below\":[{\"id\":\"13a5308f-70ad-452e-af22-43861fd0cb71\",\"type\":\"LinearAxis\"}],\"plot_width\":900,\"renderers\":[{\"id\":\"59657239-6604-4439-8bf1-83a5ab659d4a\",\"type\":\"BoxAnnotation\"},{\"id\":\"13a5308f-70ad-452e-af22-43861fd0cb71\",\"type\":\"LinearAxis\"},{\"id\":\"004b26cb-f235-4bd3-9464-2eed25a63945\",\"type\":\"Grid\"},{\"id\":\"0657e055-fa58-43fe-a3b8-b2b719174d8a\",\"type\":\"GlyphRenderer\"},{\"id\":\"eb321d59-0d82-41f4-966f-a84597d90781\",\"type\":\"GlyphRenderer\"},{\"id\":\"6ac35546-2a42-40c8-aa62-146d57d2f556\",\"type\":\"GlyphRenderer\"},{\"id\":\"5a71f9c4-4314-4eda-a1d8-25124c2b23d2\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"f04dd0c6-c641-4580-87f7-c0f878d1c0e6\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"3175d402-0679-462f-b9d4-d947ff7ad2b3\",\"type\":\"Toolbar\"},\"toolbar_location\":\"above\",\"x_range\":{\"id\":\"a5cebe80-c44b-4d0a-aae2-c2f3b9ef2475\",\"type\":\"Range1d\"},\"x_scale\":{\"id\":\"d8a43b51-3e35-4c6f-bb08-c7be8e8c3ce2\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"b765029c-d346-49e8-b769-3c8155e12984\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"a0929048-0a2d-4f8f-8d58-e4a6d0106ad1\",\"type\":\"LinearScale\"}},\"id\":\"91387928-8f01-4237-9a5d-24f1d6f93c23\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"8033ebc0-a196-49a8-acdc-dbc0ae3fcf54\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"data_source\":{\"id\":\"53346d3c-7b75-4689-95d1-c395b23fa5b8\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"d4854422-d3bc-43fa-b7c0-a791e40b8a05\",\"type\":\"Text\"},\"hover_glyph\":null,\"muted_glyph\":null,\"name\":\"tokens\",\"nonselection_glyph\":{\"id\":\"17a29716-8304-471c-9f48-6dd74ade100a\",\"type\":\"Text\"},\"selection_glyph\":null,\"view\":{\"id\":\"5d7f1257-5695-4ee0-8fe7-f29e498f83c7\",\"type\":\"CDSView\"}},\"id\":\"6ac35546-2a42-40c8-aa62-146d57d2f556\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"text\":{\"field\":\"hypo_i_text\"},\"text_align\":\"center\",\"text_alpha\":{\"value\":0.1},\"text_baseline\":\"middle\",\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"value\":\"12px\"},\"text_font_style\":\"bold\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"a10f3b4a-982e-4c44-b026-f8088199fe6f\",\"type\":\"Text\"},{\"attributes\":{\"plot\":{\"id\":\"327205fd-12df-449f-9614-e6816136cb23\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"21b07e0f-be09-4617-8f50-6b55adf59566\",\"type\":\"SingleIntervalTicker\"}},\"id\":\"d3dfd7db-0295-4068-89b6-6a86f9141772\",\"type\":\"Grid\"},{\"attributes\":{\"plot\":null,\"text\":\"Beam search\"},\"id\":\"f04dd0c6-c641-4580-87f7-c0f878d1c0e6\",\"type\":\"Title\"},{\"attributes\":{\"data_source\":{\"id\":\"49a8987d-d307-4797-9fae-8ec771f76b48\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"323b0eea-1d7f-4a0d-84fe-74a9d6ac6b56\",\"type\":\"Text\"},\"hover_glyph\":null,\"muted_glyph\":null,\"name\":\"hypo_i\",\"nonselection_glyph\":{\"id\":\"a10f3b4a-982e-4c44-b026-f8088199fe6f\",\"type\":\"Text\"},\"selection_glyph\":null,\"view\":{\"id\":\"b0f21dbb-aa92-495c-9a38-9881564a3daa\",\"type\":\"CDSView\"}},\"id\":\"d6b0d875-955c-493f-8f95-7f155de46db0\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"source\":{\"id\":\"53346d3c-7b75-4689-95d1-c395b23fa5b8\",\"type\":\"ColumnDataSource\"}},\"id\":\"5d7f1257-5695-4ee0-8fe7-f29e498f83c7\",\"type\":\"CDSView\"},{\"attributes\":{\"overlay\":{\"id\":\"8033ebc0-a196-49a8-acdc-dbc0ae3fcf54\",\"type\":\"BoxAnnotation\"}},\"id\":\"0170b055-6020-406e-b0a6-9a42a9bb2816\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"bounds\":[-10.0,20.0],\"callback\":null,\"end\":12,\"js_property_callbacks\":{\"change:end\":[{\"id\":\"b56f32a3-d365-46b7-9f4f-323382959b2d\",\"type\":\"CustomJS\"}]},\"start\":-1},\"id\":\"a5cebe80-c44b-4d0a-aae2-c2f3b9ef2475\",\"type\":\"Range1d\"},{\"attributes\":{\"dimensions\":\"width\"},\"id\":\"09035c87-81aa-4e21-b586-281eec2ac195\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_scroll\":{\"id\":\"fbfb1ccf-b06f-43d0-93b8-b7a2feb2e51e\",\"type\":\"WheelZoomTool\"},\"active_tap\":\"auto\",\"tools\":[{\"id\":\"cab35414-b6a5-47d8-a8cd-55b26f7a7b54\",\"type\":\"PanTool\"},{\"id\":\"64b6f55d-2417-4701-8845-e39dd839273e\",\"type\":\"BoxZoomTool\"},{\"id\":\"fbfb1ccf-b06f-43d0-93b8-b7a2feb2e51e\",\"type\":\"WheelZoomTool\"},{\"id\":\"7a97466a-a0c3-4c9a-93bc-f7d1909c09da\",\"type\":\"WheelZoomTool\"},{\"id\":\"a8cad25e-8062-4326-b4df-8b1d1db279c7\",\"type\":\"SaveTool\"},{\"id\":\"237d0e16-242f-45d0-81da-35c5b34ecdb4\",\"type\":\"ResetTool\"},{\"id\":\"ce962e54-6eaa-4a63-b602-ab2387d43716\",\"type\":\"HoverTool\"}]},\"id\":\"3175d402-0679-462f-b9d4-d947ff7ad2b3\",\"type\":\"Toolbar\"},{\"attributes\":{\"dimensions\":\"height\"},\"id\":\"e4e04654-061b-4633-a71e-1c9003f6bda9\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"text\":{\"field\":\"hypo_i_text\"},\"text_align\":\"center\",\"text_baseline\":\"middle\",\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"value\":\"12px\"},\"text_font_style\":\"bold\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"8c80ac74-34fc-4790-a100-c62b151ded39\",\"type\":\"Text\"},{\"attributes\":{},\"id\":\"a0929048-0a2d-4f8f-8d58-e4a6d0106ad1\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"7d459b4a-fdec-48b5-bc95-828fb94c3c34\",\"type\":\"SaveTool\"},{\"attributes\":{\"text\":{\"field\":\"hypo_i_text\"},\"text_align\":\"center\",\"text_alpha\":{\"value\":0.1},\"text_baseline\":\"middle\",\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"value\":\"12px\"},\"text_font_style\":\"bold\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1aaad8cb-1c94-4061-b4ce-f70e91e33b07\",\"type\":\"Text\"},{\"attributes\":{},\"id\":\"d8a43b51-3e35-4c6f-bb08-c7be8e8c3ce2\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"cacbfd1a-7e88-471a-a3c1-bc588c6af09c\",\"type\":\"ResetTool\"},{\"attributes\":{\"data_source\":{\"id\":\"53346d3c-7b75-4689-95d1-c395b23fa5b8\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"8c80ac74-34fc-4790-a100-c62b151ded39\",\"type\":\"Text\"},\"hover_glyph\":null,\"muted_glyph\":null,\"name\":\"hypo_i\",\"nonselection_glyph\":{\"id\":\"1aaad8cb-1c94-4061-b4ce-f70e91e33b07\",\"type\":\"Text\"},\"selection_glyph\":null,\"view\":{\"id\":\"a931ac6d-24c6-489e-873f-3785a4f5cec4\",\"type\":\"CDSView\"}},\"id\":\"5a71f9c4-4314-4eda-a1d8-25124c2b23d2\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"callback\":null,\"names\":[\"vertices\"],\"tooltips\":[[\"token\",\"@_on_hover_token\"],[\"token_id\",\"@_on_hover_token_id\"],[\"score\",\"@_on_hover_score\"]]},\"id\":\"7a2caa64-375a-466a-8caf-c9f17555b42d\",\"type\":\"HoverTool\"},{\"attributes\":{\"interval\":1},\"id\":\"21b07e0f-be09-4617-8f50-6b55adf59566\",\"type\":\"SingleIntervalTicker\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"59657239-6604-4439-8bf1-83a5ab659d4a\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"axis_label\":\"decoding step (aka output length)\",\"formatter\":{\"id\":\"e26ba807-3a3a-43be-8db2-601d280799bd\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"91387928-8f01-4237-9a5d-24f1d6f93c23\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"c0496269-9e06-46af-ab94-5438bfbff629\",\"type\":\"SingleIntervalTicker\"}},\"id\":\"13a5308f-70ad-452e-af22-43861fd0cb71\",\"type\":\"LinearAxis\"},{\"attributes\":{\"interval\":1},\"id\":\"c0496269-9e06-46af-ab94-5438bfbff629\",\"type\":\"SingleIntervalTicker\"},{\"attributes\":{\"args\":{\"source\":{\"id\":\"49a8987d-d307-4797-9fae-8ec771f76b48\",\"type\":\"ColumnDataSource\"}},\"code\":\"\\n            var x_range = cb_obj;\\n            var font_size = Math.round(14 * 13.0 / (x_range.end - x_range.start));\\n\\n            font_size = Math.min(24, Math.max(font_size, 0));\\n            \\n            var data = source.data;\\n            var fs = data['token_font_size']\\n            \\n            for (var i = 0; i < fs.length; i++)\\n                fs[i] = font_size.toString() + \\\"px\\\";\\n            \\n            source.change.emit();\\n        \"},\"id\":\"eab13fb4-9405-4d14-8bf6-46c6f698b4bb\",\"type\":\"CustomJS\"},{\"attributes\":{},\"id\":\"cab35414-b6a5-47d8-a8cd-55b26f7a7b54\",\"type\":\"PanTool\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"line_width\":{\"field\":\"line_width\"},\"size\":{\"units\":\"screen\",\"value\":24},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"d47ea423-d422-49a0-b24d-798a01f47236\",\"type\":\"Circle\"},{\"attributes\":{\"line_color\":{\"value\":\"#1f77b4\"},\"line_width\":{\"field\":\"line_width\"},\"xs\":{\"field\":\"edge_xx\"},\"ys\":{\"field\":\"edge_yy\"}},\"id\":\"d4677ed5-e7ac-4a9f-960d-0463a72b4947\",\"type\":\"MultiLine\"},{\"attributes\":{\"callback\":null,\"names\":[\"vertices\"],\"tooltips\":[[\"token\",\"@_on_hover_token\"],[\"token_id\",\"@_on_hover_token_id\"],[\"score\",\"@_on_hover_score\"]]},\"id\":\"ce962e54-6eaa-4a63-b602-ab2387d43716\",\"type\":\"HoverTool\"},{\"attributes\":{\"overlay\":{\"id\":\"59657239-6604-4439-8bf1-83a5ab659d4a\",\"type\":\"BoxAnnotation\"}},\"id\":\"64b6f55d-2417-4701-8845-e39dd839273e\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"line_width\":{\"field\":\"line_width\"},\"xs\":{\"field\":\"edge_xx\"},\"ys\":{\"field\":\"edge_yy\"}},\"id\":\"72b704fc-ca92-4d86-89ae-f33aeb3c23a8\",\"type\":\"MultiLine\"},{\"attributes\":{\"dimensions\":\"width\"},\"id\":\"fbfb1ccf-b06f-43d0-93b8-b7a2feb2e51e\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"line_color\":{\"value\":\"#1f77b4\"},\"line_width\":{\"field\":\"line_width\"},\"xs\":{\"field\":\"edge_xx\"},\"ys\":{\"field\":\"edge_yy\"}},\"id\":\"5574258e-e46b-409c-af99-02931bf42c33\",\"type\":\"MultiLine\"},{\"attributes\":{\"fill_color\":{\"field\":\"circle_fill_color\"},\"line_color\":{\"field\":\"line_color\"},\"line_width\":{\"field\":\"line_width\"},\"size\":{\"units\":\"screen\",\"value\":24},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"dd1456df-a3c2-4dcc-a8aa-769ac1a8a3b2\",\"type\":\"Circle\"},{\"attributes\":{\"dimensions\":\"height\"},\"id\":\"7a97466a-a0c3-4c9a-93bc-f7d1909c09da\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"fill_color\":{\"field\":\"circle_fill_color\"},\"line_color\":{\"field\":\"line_color\"},\"line_width\":{\"field\":\"line_width\"},\"size\":{\"units\":\"screen\",\"value\":24},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"14c2f957-8e96-40e9-8d5a-bbe84ee0131a\",\"type\":\"Circle\"},{\"attributes\":{\"args\":{\"source\":{\"id\":\"53346d3c-7b75-4689-95d1-c395b23fa5b8\",\"type\":\"ColumnDataSource\"}},\"code\":\"\\n            var x_range = cb_obj;\\n            var font_size = Math.round(14 * 13.0 / (x_range.end - x_range.start));\\n\\n            font_size = Math.min(24, Math.max(font_size, 0));\\n            \\n            var data = source.data;\\n            var fs = data['token_font_size']\\n            \\n            for (var i = 0; i < fs.length; i++)\\n                fs[i] = font_size.toString() + \\\"px\\\";\\n            \\n            source.change.emit();\\n        \"},\"id\":\"b56f32a3-d365-46b7-9f4f-323382959b2d\",\"type\":\"CustomJS\"},{\"attributes\":{},\"id\":\"a8cad25e-8062-4326-b4df-8b1d1db279c7\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"237d0e16-242f-45d0-81da-35c5b34ecdb4\",\"type\":\"ResetTool\"},{\"attributes\":{\"data_source\":{\"id\":\"49a8987d-d307-4797-9fae-8ec771f76b48\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"5574258e-e46b-409c-af99-02931bf42c33\",\"type\":\"MultiLine\"},\"hover_glyph\":null,\"muted_glyph\":null,\"name\":\"edges\",\"nonselection_glyph\":{\"id\":\"72b704fc-ca92-4d86-89ae-f33aeb3c23a8\",\"type\":\"MultiLine\"},\"selection_glyph\":null,\"view\":{\"id\":\"95e7b794-7dda-4997-b7c6-a664964f6b5a\",\"type\":\"CDSView\"}},\"id\":\"978e683c-44ae-4bb3-b7d1-4660f2415803\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"interval\":1},\"id\":\"aaf5ec9a-6f51-45f4-a1b5-d2a8ac2cb5e0\",\"type\":\"SingleIntervalTicker\"},{\"attributes\":{\"data_source\":{\"id\":\"49a8987d-d307-4797-9fae-8ec771f76b48\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"14c2f957-8e96-40e9-8d5a-bbe84ee0131a\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"name\":\"vertices\",\"nonselection_glyph\":{\"id\":\"d47ea423-d422-49a0-b24d-798a01f47236\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"dd940c41-87e6-4094-8be6-3dc7f22f0921\",\"type\":\"CDSView\"}},\"id\":\"cdf3b0e4-fe4f-450f-86d4-c51260c5d8c0\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"plot\":{\"id\":\"91387928-8f01-4237-9a5d-24f1d6f93c23\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"aaf5ec9a-6f51-45f4-a1b5-d2a8ac2cb5e0\",\"type\":\"SingleIntervalTicker\"}},\"id\":\"004b26cb-f235-4bd3-9464-2eed25a63945\",\"type\":\"Grid\"},{\"attributes\":{\"source\":{\"id\":\"53346d3c-7b75-4689-95d1-c395b23fa5b8\",\"type\":\"ColumnDataSource\"}},\"id\":\"b553c853-cc22-4ae3-aea6-95ad360ad0fc\",\"type\":\"CDSView\"},{\"attributes\":{\"source\":{\"id\":\"49a8987d-d307-4797-9fae-8ec771f76b48\",\"type\":\"ColumnDataSource\"}},\"id\":\"95e7b794-7dda-4997-b7c6-a664964f6b5a\",\"type\":\"CDSView\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"id\",\"parent_id\",\"children_ids\",\"is_best\",\"depth\",\"hypo_i\",\"token\",\"token_id\",\"x\",\"y\",\"circle_fill_color\",\"line_color\",\"line_width\",\"edge_xx\",\"edge_yy\",\"token_text\",\"token_font_size\",\"hypo_i_text\",\"hypo_i_offset\",\"_on_hover_token\",\"_on_hover_token_id\",\"_on_hover_score\"],\"data\":{\"_on_hover_score\":[\"-4.7282\",\"-4.2071\",\"-4.8782\",\"-1.3577\",\"-4.0420\",\"-4.2097\",\"-4.5624\",\"-3.2410\",\"-6.9225\",\"-4.6717\",\"-2.8497\",\"-3.8023\",\"-4.1460\",\"-3.6443\",\"-4.3135\",\"-4.7070\",\"-3.1373\",\"-4.5105\",\"-6.1526\",\"-3.0974\",\"-3.7421\",\"-3.4956\",\"0.0000\",\"-5.2694\",\"-4.4104\",\"-6.1752\",\"-3.7617\",\"-4.6281\",\"-6.5626\",\"-3.7175\",\"-0.7384\",\"-4.5439\",\"-5.2967\",\"-3.6831\",\"-5.2153\",\"-3.7830\",\"-3.1335\",\"-0.8716\",\"-3.7312\",\"-3.8895\",\"-6.5984\"],\"_on_hover_token\":[\"_EOS_\",\"\\u043f\\u0440\\u0430\\u0432\\u0438\\u0442\\u0435\\u043b\\u044c\\u0441\\u0442\\u0432\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0439\",\",\",\"\\u0443\\u043f\\u043b\\u0430\\u0442\\u044b\",\"\\u043e\\u0442\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u0432\",\"_EOS_\",\"\\u0432\\u044b\\u0441\\u0442\\u0443\\u043f\\u0430\\u0435\\u0442\",\"_EOS_\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\\u0435\",\"\\u043d\\u0430\\u043b\\u043e\\u0433\\u043e\\u0432\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043a\\u0438\",\"\\u043d\\u0435\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\",\"<empty>\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\".\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u043d\\u0435\",\"\\u043e\\u0442\\u0432\\u0435\\u0442\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u0438\",\"\\u0437\\u0430\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\",\",\",\"\\u043e\\u0442\",\"`\\u0430\\u044e\\u0449\\u0438\\u0435\",\"_EOS_\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0430\\u043b\",\"\\u043d\\u0435\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0435\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u043f\\u0440\\u0435\\u0442\\u0435\\u043d\\u0437\\u0438\\u0439\"],\"_on_hover_token_id\":[1,780,25,2482,15356,2482,1879,3,17084,25,13862,5,1,7691,1,15356,25,15328,4255,11685,11,3487,-1,21342,15356,4,21342,11,1781,30,3622,3,25,1078,1,5093,26478,11,2451,13862,22624],\"children_ids\":[[],[[2,3]],[[10,3],[10,2],[10,1],[10,0]],[[4,3],[4,1],[4,0]],[[9,3],[9,1],[9,0]],[],[],[[6,1]],[],[[8,2]],[[5,0]],[[2,2]],[],[[4,2]],[],[[7,2]],[[6,2],[6,0]],[],[],[[5,1]],[[2,1]],[],[[1,3],[1,2],[1,1],[1,0]],[[9,2]],[[8,1]],[],[[7,3],[7,1]],[],[],[[5,2]],[[2,0]],[],[],[[8,3],[8,0]],[],[[7,0]],[],[[3,3],[3,2],[3,1],[3,0]],[[5,3]],[[6,3]],[]],\"circle_fill_color\":[\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\",\"#87CEEB\"],\"depth\":[7,1,9,3,8,2,6,5,10,7,4,1,9,3,8,6,5,2,10,4,1,3,0,8,7,9,6,2,10,4,1,5,8,7,9,6,3,2,4,5,10],\"edge_xx\":[[6.0,7.0],[0.0,1.0],[8.0,9.0],[2.0,3.0],[7.0,8.0],[1.0,2.0],[5.0,6.0],[4.0,5.0],[9.0,10.0],[6.0,7.0],[3.0,4.0],[0.0,1.0],[8.0,9.0],[2.0,3.0],[7.0,8.0],[5.0,6.0],[4.0,5.0],[1.0,2.0],[9.0,10.0],[3.0,4.0],[0.0,1.0],[2.0,3.0],[0.0,0.0],[7.0,8.0],[6.0,7.0],[8.0,9.0],[5.0,6.0],[1.0,2.0],[9.0,10.0],[3.0,4.0],[0.0,1.0],[4.0,5.0],[7.0,8.0],[6.0,7.0],[8.0,9.0],[5.0,6.0],[2.0,3.0],[1.0,2.0],[3.0,4.0],[4.0,5.0],[9.0,10.0]],\"edge_yy\":[[4.833333333333333,4.333333333333333],[0.0,-1.5],[3.333333333333333,3.333333333333333],[1.5,3.0],[2.833333333333333,3.333333333333333],[0.5,0.5],[4.333333333333333,3.833333333333333],[2.833333333333333,2.833333333333333],[3.333333333333333,1.833333333333333],[0.0,0.0],[3.0,4.333333333333333],[0.0,-0.5],[3.333333333333333,4.333333333333333],[1.5,0.0],[5.333333333333333,5.333333333333333],[0.0,0.0],[4.333333333333333,4.333333333333333],[-0.5,-0.5],[3.333333333333333,4.833333333333333],[3.0,2.833333333333333],[0.0,0.5],[1.5,1.0],[0.0,0.0],[0.0,0.0],[4.833333333333333,5.333333333333333],[3.333333333333333,2.333333333333333],[4.333333333333333,4.833333333333333],[-1.5,-1.5],[3.333333333333333,3.833333333333333],[0.0,0.0],[0.0,1.5],[1.8333333333333333,1.8333333333333333],[2.833333333333333,2.333333333333333],[2.833333333333333,2.833333333333333],[0.0,0.0],[2.833333333333333,2.833333333333333],[1.5,2.0],[1.5,1.5],[3.0,1.8333333333333333],[0.0,0.0],[3.333333333333333,2.833333333333333]],\"hypo_i\":[3,3,1,0,0,1,2,1,3,2,0,2,0,3,1,3,0,2,0,1,1,2,0,2,1,3,0,3,1,2,0,3,3,0,2,1,1,0,3,2,2],\"hypo_i_offset\":[-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8,-8],\"hypo_i_text\":[\"#3\",\"#3\",\"#1\",\"#0\",\"#0\",\"#1\",\"#2\",\"#1\",\"#3\",\"#2\",\"#0\",\"#2\",\"#0\",\"#3\",\"#1\",\"#3\",\"#0\",\"#2\",\"#0\",\"#1\",\"#1\",\"#2\",\"#0\",\"#2\",\"#1\",\"#3\",\"#0\",\"#3\",\"#1\",\"#2\",\"#0\",\"#3\",\"#3\",\"#0\",\"#2\",\"#1\",\"#1\",\"#0\",\"#3\",\"#2\",\"#2\"],\"id\":[[7,3],[1,3],[9,1],[3,0],[8,0],[2,1],[6,2],[5,1],[10,3],[7,2],[4,0],[1,2],[9,0],[3,3],[8,1],[6,3],[5,0],[2,2],[10,0],[4,1],[1,1],[3,2],[0,0],[8,2],[7,1],[9,3],[6,0],[2,3],[10,1],[4,2],[1,0],[5,3],[8,3],[7,0],[9,2],[6,1],[3,1],[2,0],[4,3],[5,2],[10,2]],\"is_best\":[false,false,false,true,true,false,false,true,false,false,false,false,true,false,false,false,false,false,false,true,false,false,true,false,false,false,false,false,false,false,true,false,false,true,false,true,false,true,false,false,false],\"line_color\":[\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\",\"navy\"],\"line_width\":[1,1,1,3,3,1,1,3,1,1,1,1,3,1,1,1,1,1,1,3,1,1,3,1,1,1,1,1,1,1,3,1,1,3,1,3,1,3,1,1,1],\"parent_id\":[[6,0],[0,0],[8,0],[2,0],[7,0],[1,1],[5,0],[4,1],[9,1],[6,3],[3,0],[0,0],[8,0],[2,0],[7,1],[5,2],[4,0],[1,2],[9,1],[3,0],[0,0],[2,0],[0,0],[7,2],[6,0],[8,0],[5,0],[1,3],[9,1],[3,3],[0,0],[4,3],[7,0],[6,1],[8,2],[5,1],[2,0],[1,0],[3,0],[4,2],[9,1]],\"token\":[\"_EOS_\",\"\\u043f\\u0440\\u0430\\u0432\\u0438\\u0442\\u0435\\u043b\\u044c\\u0441\\u0442\\u0432\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0439\",\",\",\"\\u0443\\u043f\\u043b\\u0430\\u0442\\u044b\",\"\\u043e\\u0442\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u0432\",\"_EOS_\",\"\\u0432\\u044b\\u0441\\u0442\\u0443\\u043f\\u0430\\u0435\\u0442\",\"_EOS_\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\\u0435\",\"\\u043d\\u0430\\u043b\\u043e\\u0433\\u043e\\u0432\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043a\\u0438\",\"\\u043d\\u0435\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\",\"<empty>\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\".\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u043d\\u0435\",\"\\u043e\\u0442\\u0432\\u0435\\u0442\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u0438\",\"\\u0437\\u0430\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\",\",\",\"\\u043e\\u0442\",\"`\\u0430\\u044e\\u0449\\u0438\\u0435\",\"_EOS_\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0430\\u043b\",\"\\u043d\\u0435\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0435\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u043f\\u0440\\u0435\\u0442\\u0435\\u043d\\u0437\\u0438\\u0439\"],\"token_font_size\":[\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\",\"14px\"],\"token_id\":[1,780,25,2482,15356,2482,1879,3,17084,25,13862,5,1,7691,1,15356,25,15328,4255,11685,11,3487,-1,21342,15356,4,21342,11,1781,30,3622,3,25,1078,1,5093,26478,11,2451,13862,22624],\"token_text\":[\"_EOS_\",\"\\u043f\\u0440\\u0430\\u0432\\u0438\\u0442\\u0435\\u043b\\u044c\\u0441\\u0442\\u0432\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0438\\u0432\\u0430\\u0435\\u0442\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0439\",\",\",\"\\u0443\\u043f\\u043b\\u0430\\u0442\\u044b\",\"\\u043e\\u0442\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u0432\",\"_EOS_\",\"\\u0432\\u044b\\u0441\\u0442\\u0443\\u043f\\u0430\\u0435\\u0442\",\"_EOS_\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\"\\u043e\\u0442\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\\u0435\",\"\\u043d\\u0430\\u043b\\u043e\\u0433\\u043e\\u0432\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043a\\u0438\",\"\\u043d\\u0435\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\",\"<empty>\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\",\".\",\"\\u043f\\u043e\\u043f\\u0440\\u0430\\u0432\\u043e\\u043a\",\"\\u043d\\u0435\",\"\\u043e\\u0442\\u0432\\u0435\\u0442\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u0438\",\"\\u0437\\u0430\",\"\\u043f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\",\",\",\"\\u043e\\u0442\",\"`\\u0430\\u044e\\u0449\\u0438\\u0435\",\"_EOS_\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\",\"\\u043f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u0430\\u043b\",\"\\u043d\\u0435\",\"\\u0438\\u0437\\u043c\\u0435\\u043d\\u0435\\u043d\\u0438\\u0435\",\"\\u043e\\u0441\\u0432\\u043e\\u0431\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435\",\"\\u043f\\u0440\\u0435\\u0442\\u0435\\u043d\\u0437\\u0438\\u0439\"],\"x\":[7.0,1.0,9.0,3.0,8.0,2.0,6.0,5.0,10.0,7.0,4.0,1.0,9.0,3.0,8.0,6.0,5.0,2.0,10.0,4.0,1.0,3.0,0.0,8.0,7.0,9.0,6.0,2.0,10.0,4.0,1.0,5.0,8.0,7.0,9.0,6.0,3.0,2.0,4.0,5.0,10.0],\"y\":[4.333333333333333,-1.5,3.333333333333333,3.0,3.333333333333333,0.5,3.833333333333333,2.833333333333333,1.833333333333333,0.0,4.333333333333333,-0.5,4.333333333333333,0.0,5.333333333333333,0.0,4.333333333333333,-0.5,4.833333333333333,2.833333333333333,0.5,1.0,0.0,0.0,5.333333333333333,2.333333333333333,4.833333333333333,-1.5,3.833333333333333,0.0,1.5,1.8333333333333333,2.333333333333333,2.833333333333333,0.0,2.833333333333333,2.0,1.5,1.8333333333333333,0.0,2.833333333333333]}},\"id\":\"49a8987d-d307-4797-9fae-8ec771f76b48\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"e26ba807-3a3a-43be-8db2-601d280799bd\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"line_width\":{\"field\":\"line_width\"},\"xs\":{\"field\":\"edge_xx\"},\"ys\":{\"field\":\"edge_yy\"}},\"id\":\"d41b7979-8903-4ee5-a861-1bf88bfb14ea\",\"type\":\"MultiLine\"},{\"attributes\":{\"source\":{\"id\":\"49a8987d-d307-4797-9fae-8ec771f76b48\",\"type\":\"ColumnDataSource\"}},\"id\":\"dd940c41-87e6-4094-8be6-3dc7f22f0921\",\"type\":\"CDSView\"},{\"attributes\":{\"text\":{\"field\":\"token_text\"},\"text_align\":\"center\",\"text_alpha\":{\"value\":0.1},\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"field\":\"token_font_size\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"},\"y_offset\":{\"value\":-10}},\"id\":\"17a29716-8304-471c-9f48-6dd74ade100a\",\"type\":\"Text\"},{\"attributes\":{},\"id\":\"f3857a7d-2e4f-4ce0-b897-cec5490c8436\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"data_source\":{\"id\":\"53346d3c-7b75-4689-95d1-c395b23fa5b8\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"d4677ed5-e7ac-4a9f-960d-0463a72b4947\",\"type\":\"MultiLine\"},\"hover_glyph\":null,\"muted_glyph\":null,\"name\":\"edges\",\"nonselection_glyph\":{\"id\":\"d41b7979-8903-4ee5-a861-1bf88bfb14ea\",\"type\":\"MultiLine\"},\"selection_glyph\":null,\"view\":{\"id\":\"b553c853-cc22-4ae3-aea6-95ad360ad0fc\",\"type\":\"CDSView\"}},\"id\":\"0657e055-fa58-43fe-a3b8-b2b719174d8a\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"text\":{\"field\":\"token_text\"},\"text_align\":\"center\",\"text_alpha\":{\"value\":0.1},\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"field\":\"token_font_size\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"},\"y_offset\":{\"value\":-10}},\"id\":\"3621fd59-3cb3-4305-8802-6f7f75a23fd6\",\"type\":\"Text\"},{\"attributes\":{\"bounds\":[-11.5,15.333333333333332],\"callback\":null,\"range_padding\":1.0,\"range_padding_units\":\"absolute\"},\"id\":\"3868f6e3-8193-418f-af89-ee5749e490a1\",\"type\":\"DataRange1d\"},{\"attributes\":{\"text\":{\"field\":\"token_text\"},\"text_align\":\"center\",\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"field\":\"token_font_size\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"},\"y_offset\":{\"value\":-10}},\"id\":\"08bbcb52-00b5-4803-9e7a-9252db09708d\",\"type\":\"Text\"},{\"attributes\":{\"below\":[{\"id\":\"84db9569-9f28-4a47-82c8-bfa01402aeb6\",\"type\":\"LinearAxis\"}],\"plot_width\":900,\"renderers\":[{\"id\":\"8033ebc0-a196-49a8-acdc-dbc0ae3fcf54\",\"type\":\"BoxAnnotation\"},{\"id\":\"84db9569-9f28-4a47-82c8-bfa01402aeb6\",\"type\":\"LinearAxis\"},{\"id\":\"d3dfd7db-0295-4068-89b6-6a86f9141772\",\"type\":\"Grid\"},{\"id\":\"978e683c-44ae-4bb3-b7d1-4660f2415803\",\"type\":\"GlyphRenderer\"},{\"id\":\"cdf3b0e4-fe4f-450f-86d4-c51260c5d8c0\",\"type\":\"GlyphRenderer\"},{\"id\":\"a2ab1853-0966-4519-874f-956f801f1c72\",\"type\":\"GlyphRenderer\"},{\"id\":\"d6b0d875-955c-493f-8f95-7f155de46db0\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"5af81591-5793-4721-a459-e0a4ca700855\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"58f26099-07c1-4f46-ba5f-eec6d745dd12\",\"type\":\"Toolbar\"},\"toolbar_location\":\"above\",\"x_range\":{\"id\":\"14e9976c-9458-4bce-be96-da2f3c304cec\",\"type\":\"Range1d\"},\"x_scale\":{\"id\":\"eabb5e31-0740-4852-bcac-887155d4c0fc\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"3868f6e3-8193-418f-af89-ee5749e490a1\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"a3e71058-805b-4461-9009-2e76107d0dad\",\"type\":\"LinearScale\"}},\"id\":\"327205fd-12df-449f-9614-e6816136cb23\",\"subtype\":\"Figure\",\"type\":\"Plot\"}],\"root_ids\":[\"327205fd-12df-449f-9614-e6816136cb23\",\"91387928-8f01-4237-9a5d-24f1d6f93c23\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.7\"}};\n",
              "              var render_items = [{\"docid\":\"ba84f797-d201-498d-a731-5adafa5447b7\",\"elementid\":\"ff8c3f31-952d-4c2f-8b58-13e7cec51b58\",\"modelid\":\"91387928-8f01-4237-9a5d-24f1d6f93c23\"}];\n",
              "              \n",
              "              Bokeh.embed.embed_items(docs_json, render_items);\n",
              "            });\n",
              "          };\n",
              "          if (document.readyState != \"loading\") fn();\n",
              "          else document.addEventListener(\"DOMContentLoaded\", fn);\n",
              "        })();\n",
              "        \n",
              "        </script>\n",
              "    </body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ],
      "source": [
        "from IPython.display import HTML\n",
        "# Here's what it looks like:\n",
        "!wget -q https://raw.githubusercontent.com/yandexdataschool/nlp_course/2020/resources/beam_search.html\n",
        "HTML(\"beam_search.html\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "PgYYCrN9IoU-"
      },
      "outputs": [],
      "source": [
        "def generate_beamsearch(model, prefix=BOS, nucleus=0.9, beam_size=3, max_len=10):\n",
        "    \"\"\"\n",
        "    Generate a sequence with nucleous sampling\n",
        "    :param prefix: a string containing space-separated previous tokens\n",
        "    :param nucleus: N from the formulae above, N \\in [0, 1]\n",
        "    :param beam_size: numebr of branches to keep\n",
        "    :param max_len: generate sequences with at most this many tokens, NOT INCLUDING PREFIX\n",
        "    :returns: beam_size most likely candidates\n",
        "    :note: make sure that nucleous always contains at least one word, even if p(w*) > nucleus\n",
        "    \"\"\"\n",
        "\n",
        "    beam_probs = np.zeros(shape=(beam_size,)) # to accumulate log-probs for each beam \n",
        "    #beam_probs = np.ones(shape=(beam_size,)) # to accumulate log-probs for each beam \n",
        "    outputs = [prefix] # [_BOS_] or [prefix] at the beginning\n",
        "    for _ in range(max_len):\n",
        "        next_beams = []\n",
        "        for i in range(len(outputs)):\n",
        "            # generate 3 samples for each branch\n",
        "            prefix_i = outputs[i]\n",
        "\n",
        "            token_probs = model.get_possible_next_tokens(prefix_i)\n",
        "            tokens, probs = zip(*token_probs.items())\n",
        "            probs = np.array(probs)\n",
        "\n",
        "            sorted_probs_indices = np.argsort(probs)[::-1]\n",
        "            sorted_probs = probs[sorted_probs_indices]\n",
        "            # choose only N % highest probs, discard rest\n",
        "        \n",
        "            cumulative_probs = np.cumsum(sorted_probs)\n",
        "\n",
        "            remove_sorted_indices = cumulative_probs >= nucleus\n",
        "            # add one extra word to make sure \n",
        "            # we cover at least one word even if p(w*) > nucleus\n",
        "            remove_sorted_indices[..., 1:] = np.copy(remove_sorted_indices[..., :-1]) # last True -> False\n",
        "            remove_sorted_indices[..., 0] = 0 # if all of them were True, make at least first one word to False \n",
        "\n",
        "            indices_to_remove = sorted_probs_indices[remove_sorted_indices]\n",
        "            probs[indices_to_remove] = -float('Inf')\n",
        "\n",
        "            # compute softmax over changed array of probabilities\n",
        "            probs = np.exp(probs)\n",
        "            probs /= np.sum(probs)\n",
        "\n",
        "            # choose beam_size = 3 tokens with max probabilities\n",
        "            if outputs[i][-1] == EOS:\n",
        "                next_beams.append([outputs[i], beam_probs[i], i])\n",
        "            else:\n",
        "                beam_size_best_idx = sorted_probs_indices[:beam_size]\n",
        "                for j in range(beam_size):                  \n",
        "                    #next_beams.append([outputs[i] + tokens[beam_size_best_idx[j]], beam_probs[i] * probs[beam_size_best_idx[j]], i])                 \n",
        "                    next_beams.append([outputs[i] + tokens[beam_size_best_idx[j]], beam_probs[i] + np.log(probs[beam_size_best_idx[j]]), i])                 \n",
        "        #print('next_beams = ', next_beams) \n",
        "        # now we sort next_beams to prune branches with low probabilities (leave only beam_size branches)\n",
        "        outputs = [None] * beam_size\n",
        "        \n",
        "        next_beams.sort(key=lambda x: x[1], reverse=True)\n",
        "        #print('next_beams sorted = ', next_beams)\n",
        "        for i in range(beam_size):\n",
        "            outputs[i], beam_probs[i], beam_idx = next_beams[i]\n",
        "            #states[j][0][i] = states_history[beam_idx][0][i]\n",
        "        #print('outputs = \\n', outputs)\n",
        "    \n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Not quite that was planned\n",
        "generate_beamsearch(rnn_model, beam_size=5, max_len=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ro-A-xqbNMWK",
        "outputId": "5b11c018-bc1b-4a47-a7fa-4280486cc514"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:51: RuntimeWarning: divide by zero encountered in log\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' Theoretical Modellings for Multilayerial Neural Networks without Combinatorial Neural Networks ; In ',\n",
              " ' Theoretical Modellings for Multilayerial Neural Networks without Combinatorial Neural Networks ; We ',\n",
              " ' Theoretical Modellings for Multilayerial Neural Networks without Combinatorial Neural Networks for  ',\n",
              " ' Theoretical Modellings for Multilayerial Neural Networks without Combinatorial Neural Networks for S',\n",
              " ' Theoretical Modellings for Multilayerial Neural Networks without Combinatorial Neural Networks for C']"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "5fxmm931IoU_"
      },
      "outputs": [],
      "source": [
        "# check it out: which beam size works best?\n",
        "# find at least 5 prefixes where beam_size=1 and 8 generates different sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J35GFFXLIoVA"
      },
      "source": [
        "### Bonus quest II: Ultimate Language Model (2+ pts)\n",
        "\n",
        "So you've learned the building blocks of neural language models, you can now build the ultimate monster:  \n",
        "* Make it char-level, word level or maybe use sub-word units like [bpe](https://github.com/rsennrich/subword-nmt);\n",
        "* Combine convolutions, recurrent cells, pre-trained embeddings and all the black magic deep learning has to offer;\n",
        "  * Use strides to get larger window size quickly. Here's a [scheme](https://storage.googleapis.com/deepmind-live-cms/documents/BlogPost-Fig2-Anim-160908-r01.gif) from google wavenet.\n",
        "* Train on large data. Like... really large. Try [1 Billion Words](http://www.statmt.org/lm-benchmark/1-billion-word-language-modeling-benchmark-r13output.tar.gz) benchmark;\n",
        "* Use training schedules to speed up training. Start with small length and increase over time; Take a look at [one cycle](https://medium.com/@nachiket.tanksale/finding-good-learning-rate-and-the-one-cycle-policy-7159fe1db5d6) for learning rate;\n",
        "\n",
        "_You are NOT required to submit this assignment. Please make sure you don't miss your deadline because of it :)_"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py38",
      "language": "python",
      "name": "py38"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "J35GFFXLIoVA"
      ],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}