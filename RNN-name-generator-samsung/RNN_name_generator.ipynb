{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерация текста с помощью RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(по мотивам [семинара](https://github.com/neychev/harbour_dlia2019/blob/master/day02_Simple_RNN/Day_2_Simple_RNN_pytorch.ipynb)\n",
    " [курса \"Deep Learning in Applications\"](https://in.harbour.space/data-science/deep-learning-in-applications-radoslav-neychev-anastasia-ianina/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle,\n",
    "# выполните следующие строчки, чтобы подгрузить библиотеку dlnlputils:\n",
    "\n",
    "# !git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\n",
    "# import sys; sys.path.append('./stepik-dl-nlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:20:34.854793Z",
     "start_time": "2019-11-05T18:20:34.372865Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Данные\n",
    "Датасет содержит ~9k имен, все написаны латиницей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:21:03.509714Z",
     "start_time": "2019-11-05T18:21:03.491489Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "with open('russian_names.txt') as input_file:\n",
    "    names = input_file.read()[:-1].split('\\n')\n",
    "    names = [' ' + line for line in names]  # add whitespace to each name as <BOS> symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:21:03.946758Z",
     "start_time": "2019-11-05T18:21:03.938432Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Ababko', ' Abaev', ' Abagyan', ' Abaidulin', ' Abaidullin']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на распределение длин имен:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9408"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:21:05.420060Z",
     "start_time": "2019-11-05T18:21:05.179513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Name length distribution')\n",
    "plt.hist(list(map(len, names)), bins=25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Препроцессинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:21:07.335188Z",
     "start_time": "2019-11-05T18:21:07.320148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_tokens =  53\n"
     ]
    }
   ],
   "source": [
    "#all unique characters go here\n",
    "tokens = list(set(''.join(names)))\n",
    "\n",
    "num_tokens = len(tokens)\n",
    "print ('num_tokens = ', num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Символы -> id\n",
    "\n",
    "Создадим словарь < символ > -> < id >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:21:07.674548Z",
     "start_time": "2019-11-05T18:21:07.671129Z"
    }
   },
   "outputs": [],
   "source": [
    "token_to_id = {token: idx for idx, token in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:21:07.838814Z",
     "start_time": "2019-11-05T18:21:07.833611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems alright!\n"
     ]
    }
   ],
   "source": [
    "assert len(tokens) == len(token_to_id), \"dictionaries must have same size\"\n",
    "\n",
    "for i in range(num_tokens):\n",
    "    assert token_to_id[tokens[i]] == i, \"token identifier must be it's position in tokens list\"\n",
    "\n",
    "print(\"Seems alright!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:21:07.988093Z",
     "start_time": "2019-11-05T18:21:07.977722Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_matrix(data, token_to_id, max_len=None, dtype='int32', batch_first = True):\n",
    "    \"\"\"Casts a list of names into rnn-digestable matrix\"\"\"\n",
    "    \n",
    "    max_len = max_len or max(map(len, data))\n",
    "    data_ix = np.zeros([len(data), max_len], dtype) + token_to_id[' ']\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        line_ix = [token_to_id[c] for c in data[i]]\n",
    "        data_ix[i, :len(line_ix)] = line_ix\n",
    "        \n",
    "    if not batch_first: # convert [batch, time] into [time, batch]\n",
    "        data_ix = np.transpose(data_ix)\n",
    "\n",
    "    return data_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:21:08.136936Z",
     "start_time": "2019-11-05T18:21:08.131609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ababko\n",
      " Chihachev\n",
      " Isaikov\n",
      " Nakhamkin\n",
      " Ustenko\n",
      "[[18 14 34 47 34  0 26 18 18 18]\n",
      " [18 33 31 24 31 47 51 31 49 11]\n",
      " [18  7 45 47 24  0 26 11 18 18]\n",
      " [18 17 47  0 31 47  5  0 24 15]\n",
      " [18 10 45 41 49 15  0 26 18 18]]\n"
     ]
    }
   ],
   "source": [
    "#Example: cast 4 names to matrices, pad with zeros\n",
    "print('\\n'.join(names[::2000]))\n",
    "print(to_matrix(names[::2000], token_to_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Рекуррентные нейронные сети\n",
    "\n",
    "<img src=\"img/rnn.png\" width=480>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'stepik-dl-nlp/requirements.txt'\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:21:10.739438Z",
     "start_time": "2019-11-05T18:21:09.661222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Visual C++ Redistributable is not installed, this may lead to the DLL load failure.\n",
      "                 It can be downloaded at https://aka.ms/vs/16/release/vc_redist.x64.exe\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found. Error loading \"C:\\Users\\oobur\\anaconda3\\lib\\site-packages\\torch\\lib\\asmjit.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-f11b09da06d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    138\u001b[0m                 \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWinError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_last_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrerror\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34mf' Error loading \"{dll}\" or one of its dependencies.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[0mkernel32\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSetErrorMode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprev_error_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found. Error loading \"C:\\Users\\oobur\\anaconda3\\lib\\site-packages\\torch\\lib\\asmjit.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:21:10.751862Z",
     "start_time": "2019-11-05T18:21:10.741772Z"
    }
   },
   "outputs": [],
   "source": [
    "class CharRNNCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Implement the scheme above as torch module\n",
    "    \"\"\"\n",
    "    def __init__(self, num_tokens=len(tokens), embedding_size=16, rnn_num_units=64):\n",
    "        super(self.__class__,self).__init__()\n",
    "        self.num_units = rnn_num_units\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_tokens, embedding_size)\n",
    "        self.rnn_update = nn.Linear(embedding_size + rnn_num_units, rnn_num_units)\n",
    "        self.rnn_to_logits = nn.Linear(rnn_num_units, num_tokens)\n",
    "        \n",
    "    def forward(self, x, h_prev):\n",
    "        \"\"\"\n",
    "        This method computes h_next(x, h_prev) and log P(x_next | h_next)\n",
    "        We'll call it repeatedly to produce the whole sequence.\n",
    "        \n",
    "        :param x: batch of character ids, variable containing vector of int64\n",
    "        :param h_prev: previous rnn hidden states, variable containing matrix [batch, rnn_num_units] of float32\n",
    "        \"\"\"\n",
    "        # get vector embedding of x\n",
    "        x_emb = self.embedding(x)\n",
    "        \n",
    "        # compute next hidden state using self.rnn_update\n",
    "        x_and_h = torch.cat([x_emb, h_prev], dim=1) #YOUR CODE HERE\n",
    "        h_next = self.rnn_update(x_and_h) #YOUR CODE HERE\n",
    "        \n",
    "        h_next = F.tanh(h_next)\n",
    "        \n",
    "        assert h_next.size() == h_prev.size()\n",
    "        \n",
    "        #compute logits for next character probs\n",
    "        logits = self.rnn_to_logits(h_next)\n",
    "        \n",
    "        return h_next, F.log_softmax(logits, -1)\n",
    "    \n",
    "    def initial_state(self, batch_size):\n",
    "        \"\"\" return rnn state before it processes first input (aka h0) \"\"\"\n",
    "        return Variable(torch.zeros(batch_size, self.num_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:21:11.071002Z",
     "start_time": "2019-11-05T18:21:11.052377Z"
    }
   },
   "outputs": [],
   "source": [
    "char_rnn = CharRNNCell()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тренировка сети, RNN loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:21:11.521078Z",
     "start_time": "2019-11-05T18:21:11.510175Z"
    }
   },
   "outputs": [],
   "source": [
    "def rnn_loop(rnn, batch_index):\n",
    "    \"\"\"\n",
    "    Computes log P(next_character) for all time-steps in names_ix\n",
    "    :param names_ix: an int32 matrix of shape [batch, time], output of to_matrix(names)\n",
    "    \"\"\"\n",
    "    batch_size, max_length = batch_index.size()\n",
    "    hid_state = rnn.initial_state(batch_size)\n",
    "    logprobs = []\n",
    "\n",
    "    for x_t in batch_index.transpose(0,1):\n",
    "        hid_state, logp_next = rnn(x_t, hid_state)  \n",
    "        logprobs.append(logp_next)\n",
    "        \n",
    "    return torch.stack(logprobs, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тренировка сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:21:12.120106Z",
     "start_time": "2019-11-05T18:21:12.109585Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from random import sample\n",
    "\n",
    "char_rnn = CharRNNCell()\n",
    "opt = torch.optim.Adam(char_rnn.parameters())\n",
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:21:23.521061Z",
     "start_time": "2019-11-05T18:21:12.302892Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = max(map(len, names))\n",
    "\n",
    "for i in range(1000):\n",
    "    batch_ix = to_matrix(sample(names, 32), token_to_id, max_len=MAX_LENGTH)\n",
    "    batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
    "    \n",
    "    logp_seq = rnn_loop(char_rnn, batch_ix)\n",
    "    \n",
    "    # compute loss\n",
    "    predictions_logp = logp_seq[:, :-1]\n",
    "    actual_next_tokens = batch_ix[:, 1:]\n",
    "\n",
    "    loss = -torch.mean(torch.gather(predictions_logp, dim=2, index=actual_next_tokens[:,:,None]))###YOUR CODE\n",
    "    \n",
    "    # train with backprop\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    # visualizing training process\n",
    "    history.append(loss.data.numpy())\n",
    "    if (i + 1) % 100 == 0:\n",
    "        clear_output(True)\n",
    "        plt.plot(history,label='loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN: генерация имен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:21:23.540765Z",
     "start_time": "2019-11-05T18:21:23.524503Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_sample(char_rnn, seed_phrase=' ', max_length=MAX_LENGTH, temperature=1.0):\n",
    "    '''\n",
    "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
    "    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n",
    "    :param max_length: maximum output length, including seed_phrase\n",
    "    :param temperature: coefficient for sampling.  higher temperature produces more chaotic outputs,\n",
    "                        smaller temperature converges to the single most likely output\n",
    "    '''\n",
    "    \n",
    "    x_sequence = [token_to_id[token] for token in seed_phrase]\n",
    "    x_sequence = torch.tensor([x_sequence], dtype=torch.int64)\n",
    "    hid_state = char_rnn.initial_state(batch_size=1)\n",
    "    \n",
    "    #feed the seed phrase, if any\n",
    "    for i in range(len(seed_phrase) - 1):\n",
    "        hid_state, _ = char_rnn(x_sequence[:, i], hid_state)\n",
    "    \n",
    "    #start generating\n",
    "    for _ in range(max_length - len(seed_phrase)):\n",
    "        hid_state, logp_next = char_rnn(x_sequence[:, -1], hid_state)\n",
    "        p_next = F.softmax(logp_next / temperature, dim=-1).data.numpy()[0]\n",
    "        \n",
    "        # sample next token and push it back into x_sequence\n",
    "        next_ix = np.random.choice(len(tokens), p=p_next)\n",
    "        next_ix = torch.tensor([[next_ix]], dtype=torch.int64)\n",
    "        x_sequence = torch.cat([x_sequence, next_ix], dim=1)\n",
    "        \n",
    "    return ''.join([tokens[ix] for ix in x_sequence.data.numpy()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:21:23.625562Z",
     "start_time": "2019-11-05T18:21:23.544968Z"
    }
   },
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    print(generate_sample(char_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:21:23.702249Z",
     "start_time": "2019-11-05T18:21:23.629226Z"
    }
   },
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    print(generate_sample(char_rnn, seed_phrase=' Ar'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Более простое решение\n",
    "\n",
    "* `nn.RNNCell(emb_size, rnn_num_units)` - шаг RNN. Алгоритм: concat-linear-tanh\n",
    "* `nn.RNN(emb_size, rnn_num_units` - весь rnn_loop.\n",
    "\n",
    "Кроме того, в PyTorch есть `nn.LSTMCell`, `nn.LSTM`, `nn.GRUCell`, `nn.GRU`, etc. etc.\n",
    "\n",
    "Перепишем наш пример с генерацией имен с помощью средств PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:21:23.713285Z",
     "start_time": "2019-11-05T18:21:23.704755Z"
    }
   },
   "outputs": [],
   "source": [
    "class CharRNNLoop(nn.Module):\n",
    "    def __init__(self, num_tokens=num_tokens, emb_size=16, rnn_num_units=64):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.emb = nn.Embedding(num_tokens, emb_size)\n",
    "        self.rnn = nn.RNN(emb_size, rnn_num_units, batch_first=True)\n",
    "        self.hid_to_logits = nn.Linear(rnn_num_units, num_tokens)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        assert isinstance(x, Variable) and isinstance(x.data, torch.LongTensor)\n",
    "        h_seq, _ = self.rnn(self.emb(x))\n",
    "        next_logits = self.hid_to_logits(h_seq)\n",
    "        next_logp = F.log_softmax(next_logits, dim=-1)\n",
    "        return next_logp\n",
    "    \n",
    "model = CharRNNLoop()\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:21:23.790047Z",
     "start_time": "2019-11-05T18:21:23.715167Z"
    }
   },
   "outputs": [],
   "source": [
    "# the model applies over the whole sequence\n",
    "batch_ix = to_matrix(sample(names, 32), token_to_id, max_len=MAX_LENGTH)\n",
    "batch_ix = Variable(torch.LongTensor(batch_ix))\n",
    "\n",
    "logp_seq = model(batch_ix)\n",
    "\n",
    "# compute loss\n",
    "loss = F.nll_loss(logp_seq[:, 1:].contiguous().view(-1, num_tokens), \n",
    "                  batch_ix[:, :-1].contiguous().view(-1))\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:21:31.468107Z",
     "start_time": "2019-11-05T18:21:23.792092Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = max(map(len, names))\n",
    "\n",
    "for i in range(1000):\n",
    "    batch_ix = to_matrix(sample(names, 32), token_to_id, max_len=MAX_LENGTH)\n",
    "    batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
    "    \n",
    "    logp_seq = model(batch_ix)\n",
    "    \n",
    "    # compute loss\n",
    "    predictions_logp = logp_seq[:, :-1]\n",
    "    actual_next_tokens = batch_ix[:, 1:]\n",
    "\n",
    "    loss = -torch.mean(torch.gather(predictions_logp, dim=2, index=actual_next_tokens[:,:,None]))\n",
    "    \n",
    "    # train with backprop\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    history.append(loss.data.numpy())\n",
    "    if (i + 1) % 100 == 0:\n",
    "        clear_output(True)\n",
    "        plt.plot(history, label='loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "assert np.mean(history[:25]) > np.mean(history[-25:]), \"RNN didn't converge.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:21:31.526436Z",
     "start_time": "2019-11-05T18:21:31.469965Z"
    }
   },
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    print(generate_sample(char_rnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашнее задание: мотивационные лозунги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:21:31.570320Z",
     "start_time": "2019-11-05T18:21:31.528673Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "with open('datasets/author_quotes.txt') as input_file:\n",
    "    quotes = input_file.read()[:-1].split('\\n')\n",
    "    quotes = [' ' + line for line in quotes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:21:31.575286Z",
     "start_time": "2019-11-05T18:21:31.571798Z"
    }
   },
   "outputs": [],
   "source": [
    "quotes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:21:31.653673Z",
     "start_time": "2019-11-05T18:21:31.578424Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = list(set(''.join(quotes)))\n",
    "token_to_id = {token: idx for idx, token in enumerate(tokens)}\n",
    "num_tokens = len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что еще можно генерировать?\n",
    "С помощью кода из этого семинара можно генерировать не только имена, но и:\n",
    "\n",
    "* Повести/романы/поэзию/песни любимого автора\n",
    "* Новостные заголовки\n",
    "* Программный код\n",
    "* Молекулы в формате [smiles](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system)\n",
    "* Музыку\n",
    "* Названия мебели из ИКЕА\n",
    "* Мотивационные лозунги\n",
    "* etc.\n",
    "\n",
    "__Удачи!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
